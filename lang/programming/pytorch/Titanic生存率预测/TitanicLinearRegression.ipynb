{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch: Examining the Titanic Sinking with Ridge Regression\n",
    "## Examining the data of more than 800 titanic passengers and training a model on it\n",
    "\n",
    "In this notebook we shall use [this](https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html) dataset containing data about passengers from the titanic. Based on this data we will use a Ridge Regression model which just means a Logistic Regression model that uses L2 Regularization for predicting whether a person survived the sinking based on their passenger class, sex, the number of their siblings/spouses aboard, the number of their parents/children aboard and the fare they payed.\n",
    "\n",
    "First we import everything we need for plotting data and creating a great model to make predictions on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import jovian\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Here we can see what the data actually looks like. The first column indicates whether the person survived with a 1 or a 0 where the 1 stands for survival and the 0 for death. The rest of the columns are all our input columns used to predict the survival. We will however forget about as it does not hold important information needed to predict survival. You can also see below that we have 887 persons with their data and 8 total columns where 6 of them will be the input values and 1 of them (the Survived column) the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"./data/titanic.csv\")\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get a little bit more familiar with the data we can do some computations with it and plot it. First we print how big the part of the survivors is which can also be described as the total probability of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total probability of survival for all passengers -->  {:.4f}\".format(dataframe.Survived.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at how likely different people from different classes and of different sex are to survive we can see a clear trend that the higher the class the higher the survival probability and also that women are much more likely to survive. But why is this the case. The answer is quite simple. When the Titanic began to sink women and children should go offboard in the lifeboats first before the men and the lower classes were not treated equally during the sinking as there were so many people in those classes that they could not be informed as well by the stewardesses so that it took mutch longer for them to get to the deck for rescue while first and second class passengers were already boarding the lifeboats and then the sailors fastened down the hatchways leading to the third-class section. They said they wanted to keep the air down there so the vessel could stay up longer. It meant all hope was gone for those still down there. Another reason why there where so many people dying was the missing safety measures onboard the Titanic. For example there where not enough boats for the passengers to escape the ship. The lifeboats would have been only sufficient for half the people onboard and due to bad organization not all of them were completely filled so that even more than half of the passengers was left behind. One good aspect, however, is that the laws for a ships safety have become much more strict after this disaster. If you want to read about the sinking in detail have a look at this: <https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==1)].index) / \\\n",
    "    len(dataframe[(dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How likely is survival??\n",
    "print(\"Survival probability for female in first class -->  {:.4f}\".format(len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==1)].index) / len(dataframe[(dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==1)].index)))\n",
    "print(\"Survival probability for female in second class -->  {:.4f}\".format(len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==2)].index) / len(dataframe[(dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==2)].index)))\n",
    "print(\"Survival probability for female in third class -->  {:.4f}\".format(len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==3)].index) / len(dataframe[(dataframe[\"Sex\"]==\"female\") & (dataframe[\"Pclass\"]==3)].index)))\n",
    "print(\"\")\n",
    "print(\"Survival probability for male in first class -->  {:.4f}\".format(len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"male\") & (dataframe[\"Pclass\"]==1)].index) / len(dataframe[(dataframe[\"Sex\"]==\"male\") & (dataframe[\"Pclass\"]==1)].index)))\n",
    "print(\"Survival probability for male in second class -->  {:.4f}\".format(len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"male\") & (dataframe[\"Pclass\"]==2)].index) / len(dataframe[(dataframe[\"Sex\"]==\"male\") & (dataframe[\"Pclass\"]==2)].index)))\n",
    "print(\"Survival probability for male in third class -->  {:.4f}\".format(len(dataframe[(dataframe[\"Survived\"]==1) & (dataframe[\"Sex\"]==\"male\") & (dataframe[\"Pclass\"]==3)].index) / len(dataframe[(dataframe[\"Sex\"]==\"male\") & (dataframe[\"Pclass\"]==3)].index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the prices which are all in measured in pounds we can see the total average fare and then the different ones from the different classes. Note that due to inflation these numbers measured in pounds today would be quite a bit higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average fare of all passengers -->   {:.4f}\".format(dataframe.Fare.mean()))\n",
    "# How much you have to pay for the different classes in average?? (price unit is Â£/pounds)\n",
    "print(\"First class mean cost -->  {:.4f}\".format(dataframe[dataframe[\"Pclass\"]==1].Fare.mean()))\n",
    "print(\"Second class mean cost -->  {:.4f}\".format(dataframe[dataframe[\"Pclass\"]==2].Fare.mean()))\n",
    "print(\"Third class mean cost -->  {:.4f}\".format(dataframe[dataframe[\"Pclass\"]==3].Fare.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also people from all ages on board while the average age is 30 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average age of all passengers\n",
    "print(\"Average age of all passengers -->  {0:.1f}\".format(dataframe.Age.mean()))\n",
    "print(\"Age of oldest passenger -->  {:.1f}\".format(dataframe.Age.max()))\n",
    "print(\"Age of youngest passenger -->  {:.1f}\".format(dataframe.Age.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the difference of survival probalilty already mentioned an explained above more visually we can plot them like the following plots show. Here you can see the difference between the different classes and sex very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(dataframe.Survived)\n",
    "plt.title(\"No. of survivors and dead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Survived\", hue=\"Sex\", data=dataframe)\n",
    "plt.title(\"No. of people survived and died separated into women and men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Survived\", hue=\"Pclass\", data=dataframe)\n",
    "plt.title(\"No. of people survived from different classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the fare distribution and the costs from the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(dataframe.Fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average pricing for different classes\n",
    "sns.lmplot(x=\"Pclass\", y=\"Fare\", data=dataframe, y_jitter=.03, x_estimator=np.mean, robust=True)\n",
    "plt.title(\"Average pricing for different classes in Â£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we can see very well that most passengers did not have any siblings/spouses or parents/children aboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dataframe[\"Siblings/Spouses Aboard\"], kde=False)\n",
    "plt.title(\"Number of siblings and spouses on board of each passenger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(dataframe[\"Parents/Children Aboard\"], kde=False)\n",
    "plt.title(\"Number of parents and children on board of each passenger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we look at the distribution of the ages of the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(dataframe.Age)\n",
    "plt.title(\"Ages of passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Now to be able to train our model we want to convert our pandas dataframe into pytorch tensors. To do this we define the `dataframe_to_arrays` method which does the conversion to numpy arrays. To use the function we need to specify 3 kinds of columns namely input columns, categorical columns (columns that do not contain numbers but rather a string standing for a category) and output columns so that it can properly create a numpy array for input data and labels with all input data from the input columns (by first converting the categorical columns to numerical ones) and the labels from the output columns. Then we can easily convert them to pytorch tensors and specify the desired data types so that we are ready to define the model to be ready for the training on the data. Note also that the `normalize` parameter is set to `True` which makes the function normalize the input data by sqishing all values in a range between 0 and 1 with Min Max Normalization for the model to be able to better learn from the data as it is more uniform now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [title for title in dataframe.columns[1:] if title != 'Name']\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"Sex\"]\n",
    "output_cols = [\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe, normalize=False, labeled=True, cols=input_cols):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    # Convert categorical columns to numbers\n",
    "    for col in categorical_cols:\n",
    "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
    "    \n",
    "    # normalize the input data with min max normalization\n",
    "    if normalize:\n",
    "        for col in cols:\n",
    "            dataframe1[col] = (dataframe1[col] - dataframe1[col].min()) / (dataframe1[col].max() - dataframe1[col].min())\n",
    "    # Extract input & outputs as numpy arrays\n",
    "    inputs_array = dataframe1[cols].to_numpy()\n",
    "    if labeled:\n",
    "        targets_array = dataframe1[output_cols].to_numpy().reshape(-1,1)\n",
    "        return inputs_array, targets_array\n",
    "    else:\n",
    "        return inputs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr, target_arr = dataframe_to_arrays(dataframe, normalize=True)\n",
    "input_arr, target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(input_arr, dtype=torch.float32)\n",
    "targets = torch.tensor(target_arr, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the pytorch tensors for the input data and the labels we put them into a pytorch `TensorDataset` which contains pairs of inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we have to do is that we split the original dataset into 1 for training the model and another one for validating that the model is learning something which means that the validation dataset contains data that the model has never seen before and by making predictions on it we can see how well the model can perform on unknown data. This accuracy from the validation data will be used as a metric for all training epochs as well as the loss on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_part = 0.1\n",
    "validation_size = int(validation_part*len(dataset))\n",
    "train_size = len(dataset) - validation_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, validation_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing we do with our data is to put it into a `DataLoader` (one for the validation data and one for the training data) which will be used to train the model with shuffled and batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = len(input_cols)\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    return torch.tensor(torch.sum(outputs.round() == labels).item() / len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Structure\n",
    "\n",
    "Now we can create our model which is just a simple Logistic Regression model with a linear layer that accepts 6 inputs and outputs 1 value between 0 and 1 which basically makes the model's `forward` method return the probability of survival it predicts by using the sigmoid activation function. This is necessary as then we can train the model to output a 1 when it thinks that the person would survive and a 0 when it does think that the person will not survive even though the model will probably never return a 1 or a 0 but it will predict a probability closer to 1 or 0 after some time of training. Moreover we define some other methods in our model for training and computing accuracies or printing them. One more thing to note is that as a loss function in `training_step` we use the Binary Cross Entropy loss. Lastly we create an instance of the `TitanicModel` called `model` which we will train on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = torch.sigmoid(self.linear(xb))\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, labels = batch \n",
    "        out = self(inputs)                  # Generate predictions\n",
    "        loss = F.binary_cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, labels = batch\n",
    "        out = self(inputs)                    # Generate predictions\n",
    "        loss = F.binary_cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = TitanicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "After that now comes the cool part which is the actual training. For this we need to make a `fit_one_cycle` function to do the training for our model. Like any usual fit functions this one uses an optimizer to adjust the models parameters according to the gradients by backpopagating the loss backwards through the model with a certain learning rate. Here however there are some things about the fit function I want to point out that are not just the usual computing loss and then adjusting weights and biases thing.\n",
    "\n",
    "* **Learning rate scheduling**: This is a technique replacing the fixed learning rate usually used by changing the learning rate after every batch of training. This can be done several ways but the way we will do it is with the **\"One Cycle Learning Policy\"** which starts with a smaller learning rate and then starts to gradually increase for the first 30% of epochs and then decreasing it again for optimal learning. For this scheduler we just need to set the maximum learning rate to which it will increase over time. If you want to go deaper into this topic I suggest you read this: <https://sgugger.github.io/the-1cycle-policy.html>\n",
    "* **Weight decay / L2 Regularization**: Another thing we use is weight decay which adds the sum of the weights squared to the loss function so that bigger weights will be punished as bigger weights are usually a sign of overfitting. Thereby we make the model able to generalize better and achieve better results on unknown data as the weights are being lowered. See this for more information about weight decay: <https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab>\n",
    "* **Gradient clipping**: Lastly there is gradient clipping. This is actually quite simple but still very useful. The way gradient clipping works is that it just limits the gradient to a certain value so that if the gradient would take the model in the wrong direction it is limited to a certain size which means that the model can't be hurt due to large gradient values. Here is also an interesting post about it: <https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    \n",
    "    # Set up cutom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be all set to start the training. First we compute the accuracy and loss on the validation data to see how good the model initially performed vs how it performs after the training. For the training itself we define the hyperparameters like maximum learning rate, epochs to train for, weigh decay value, gradient clipping and the optimizer which will be the adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "max_lr = 0.5\n",
    "grad_clip = 0.6\n",
    "weight_decay = 1e-2\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history += fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this short training our model should be very good at predicting survival for titanic passengers. You can also see this when you look at the accuracy and the loss on the validation data and compare it to the one computed before the training. To verify this further let's plot the accuracies on the validation data over the course of the training phase as well as the loss on both training data and validation data. This last thing can also show us how much the model is overfitting since as soon as the training data loss still decreases but the validation data loss increases or stays the same we are overfitting since we are getting better and better on the training data but worse on the validation data which is definitely not what we want. However the model does not seem to be overfitting which is great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "\n",
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "def plot_lrs(history):\n",
    "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "    plt.plot(lrs)\n",
    "    plt.xlabel('Batch no.')\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.title('Learning Rate vs. Batch no.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Now we need to save our model by writing its state which means all of its parameters to a file and log our hyperparameters, final accuracy and final loss so we can later easily compare different model architectures and different choices of hyperparameters to see how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(model, val_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'titanic-logistic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when looking at the weights from the `model.state_dict()` output we can see how important each of the input values is. For example we can see that the class is associated with a negative value which is good since people from a class described with a higher number like class 3 were much less likely to survive. The next weight shows the extreme importance of the sex for the prediction as it is associated with the largest negative value which can be understood if you know that a man is represented with 1 and a woman with 0. What we can also deduce from the last weight is that the larger the fare paid the higher the survival probability which I think makes sense, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.reset()\n",
    "jovian.log_hyperparams(arch='Ridge Logistic Regression model', \n",
    "                       epochs=epochs, \n",
    "                       lr=max_lr,\n",
    "                       batch_size=batch_size,\n",
    "                       scheduler='one-cycle', \n",
    "                       weight_decay=weight_decay, \n",
    "                       grad_clip=grad_clip,\n",
    "                       opt=opt_func.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.log_metrics(val_loss=history[-1]['val_loss'], \n",
    "                   val_acc=history[-1]['val_acc'],\n",
    "                   train_loss=history[-1]['train_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model on Samples\n",
    "\n",
    "Having the training phase behind us we can do some testing on various single examples from the validation data to get a feeling for how well the model performs. Therefore we need to make a function which will return the models prediction for a given dataset element as well as what the person's data is and whether the person actually survived or not. As you can see in order to display the data it is important that we denormalize our data again by putting all values from the range between 0 and 1 back to their initial range and converting the categorical column Sex back to the strings female and male from the numbers 0 and 1 as which they were represented in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, with_label=True, verbose=True):\n",
    "    if verbose:\n",
    "        for i in range(len(input_cols)):\n",
    "            val = input_data[0][i].item()\n",
    "            if input_cols[i] == \"Sex\":\n",
    "                val = \"female\" if input_data[0][i] == 0.0 else \"male\"\n",
    "            else:\n",
    "                val = round(val * (dataframe[input_cols[i]].max() - dataframe[input_cols[i]].min()) + dataframe[input_cols[i]].min(), 4)\n",
    "            print(input_cols[i] + \":\", val)\n",
    "        print(\"\")\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_data[0]).item()\n",
    "    print(\"Predicted by model..\")\n",
    "    if pred < 0.5:\n",
    "        print(\"Didn't make it (Probability for survival: {:.4f})\".format(pred))\n",
    "    else:   \n",
    "        print(\"A survivor!!! (Probability for survival: {:.4f})\".format(pred))\n",
    "    \n",
    "    if with_label:\n",
    "        print(\"\")\n",
    "        print(\"Actual result...\")\n",
    "        if input_data[1].item() == 0:\n",
    "            print(\"Didn't make it\")\n",
    "        else:\n",
    "            print(\"A survivor!!!\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict(val_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(val_ds[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(val_ds[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(val_ds[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't you want to find out as well whether you would have survived the titanic disaster. To do this we have this nice function which asks you to input your data and then returns its prediction after converting the categorical values to numericals and normalizing the input data. Just think of a fare reasonable for your chosen class (or not and try to break the predictions). You can of course completely also make up data to test the model with that and see which people would survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine whether you would have survived the titanic sinking\n",
    "# it will ask you for the passenger class(classes from 1 to 3), sex (either \"male\" or \"female\") and some other information\n",
    "# note that the price unit is Â£\n",
    "female = 0\n",
    "male = 1\n",
    "def will_I_survive():\n",
    "    data = list()\n",
    "    for i in range(len(input_cols)):\n",
    "        data.append(input(\"Your \" + input_cols[i] + \"?: \"))\n",
    "    data = [float(item) if (item != \"female\" and item != \"male\") else float(eval(item)) for item in data]\n",
    "    for col in range(len(input_cols)):\n",
    "        if input_cols[col] != \"Sex\":\n",
    "            data[col] = (data[col] - dataframe[input_cols[col]].min()) / (dataframe[input_cols[col]].max() - dataframe[input_cols[col]].min())\n",
    "    print(\"\")\n",
    "    predict(torch.tensor([data], dtype=torch.float32), with_label=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this function to see whether you would have survived\n",
    "will_I_survive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can make a submission .csv file for the titanic competition on kaggle to become first place p ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_cols = \"?\".join(input_cols).replace(\"Siblings/Spouses Aboard\", \"SibSp\").replace(\"Parents/Children Aboard\", \"Parch\").split(\"?\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "# eleminate all nan values in the age column\n",
    "test_df.loc[test_df['Age'].isnull()] = test_df['Age'].median()\n",
    "# eleminate all nan values in the fare column\n",
    "test_df.loc[test_df['Fare'].isnull()] = test_df['Fare'].median()\n",
    "test_input_arr = dataframe_to_arrays(test_df, normalize=True, labeled=False, cols=test_input_cols)\n",
    "test_inputs = torch.tensor(test_input_arr, dtype=torch.float32)\n",
    "test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(inputs, out_file=\"./data/logistic_submission.csv\"):\n",
    "    submission_df = pd.read_csv(\"./data/gender_submission.csv\")\n",
    "    result = list()\n",
    "    for person in inputs:\n",
    "        result.append(int(round(model(person).item(), 0)))\n",
    "    submission_df[\"Survived\"] = result\n",
    "    submission_df.to_csv(out_file, index=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(test_inputs, out_file=\"./data/logistic_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=\"titanic-logistic-regression\", outputs=['titanic-logistic.pth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Opportunities for future Work\n",
    "\n",
    "Lastly I want to summarize the amazing things I learned from this nice project. The first major takeaway from this project was how to deal with pandas dataframes and data in general which was usually done for me when I was provided a starter notebook. Now that I did this project from scratch I read about the pandas library and various of its functions so I was able to use this data for my project very well. I also learned quite a bit about data normalization.\n",
    "\n",
    "Another thing I took away from this was a lot of knowledge about Logistic Regression as I read quite a lot on the various approaches. For example I read about why you would use 1 output neuron vs 2 output neurons for binary classification and came to the result that the usage of 1 output neuron is less prone to overfitting as it has less parameters which makes totally sense. This is also why I used this for my model with the Binary Cross Entropy. Morover I learned the math behind regularization to be able to better understand it and implement it which helped a lot when implementing regularization and choosing the weight decay hyperparameter.\n",
    "\n",
    "Not to forget are also the things I learned about the disaster by examining the data and also from additional research which was very interesting.\n",
    "\n",
    "Summing the learning up I can just stress how great projects are for learning as by doing everything yourself you can learn it much better. I feel a lot more comfortable with the pytorch library and Machine Learning in general now.\n",
    "\n",
    "For the future I can't wait to work on different more challenging projects with other datasets and compete in various interesting kaggle challenges with all the newly learned things to deepen my knowledge in the world of AI and have fun. I am really looking forward to doing all of the thousands of projects I have in my mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}