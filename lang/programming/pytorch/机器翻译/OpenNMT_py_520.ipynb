{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenNMT-py_520",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "// 原始语料在 doc\\lang\\programming\\pytorch\\机器翻译\\中英平行语料520万_translation2019zh  格式是：每行一个JSON 文本， 有两个字段 english 和 chinese \n",
        "// 转换为 OpenNMT-py 格式，英文一个文件，中文一个文件"
      ],
      "metadata": {
        "id": "PRQShiytjwEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "! gdown --id '1vzW8NXWmJ9LHp9hYs4u6SsgWtTF4sd7z'\n",
        "! unzip ./en_chs.zip\n",
        "#! unzip ./chinese_roberta_wwm_ext_L-12_H-768_A-12.zip -d ./chinese_roberta_wwm_ext_L-12_H-768_A-12\n"
      ],
      "metadata": {
        "id": "koFGLvfZhHen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHvEf7ZMW_oj"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RLOWbG_RDgD"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install OpenNMT-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaazqR27cDa8"
      },
      "source": [
        "%%bash\n",
        "cat <<EOF > en_chs.yaml\n",
        "# toy_en_de.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: en_chs/run/example\n",
        "## Where the vocab(s) will be written\n",
        "src_vocab: en_chs/run/example.vocab.src\n",
        "tgt_vocab: en_chs/run/example.vocab.tgt\n",
        "# Prevent overwriting existing files in the folder\n",
        "overwrite: True\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: en_chs/src-train.txt\n",
        "        path_tgt: en_chs/tgt-train.txt\n",
        "    valid:\n",
        "        path_src: en_chs/src-val.txt\n",
        "        path_tgt: en_chs/tgt-val.txt\n",
        "\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAbZHee4coKb"
      },
      "source": [
        "From this configuration, we can build the vocab(s), that will be necessary to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMCXdJT2co93"
      },
      "source": [
        "!onmt_build_vocab -config en_chs.yaml -n_sample 20000\n",
        "#!onmt_build_vocab -config en_chs.yaml -n_sample 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4wLeBKBctEs"
      },
      "source": [
        "Notes:\n",
        "\n",
        "-n_sample is required here – it represents the number of lines sampled from each corpus to build the vocab.\n",
        "\n",
        "This configuration is the simplest possible, without any tokenization or other transforms. See other example configurations for more complex pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfiYDE2cvOl"
      },
      "source": [
        "**Step 2: Train the model**\n",
        "\n",
        "To train a model, we need to add the following to the YAML configuration file:\n",
        "\n",
        "the vocabulary path(s) that will be used: can be that generated by onmt_build_vocab;\n",
        "\n",
        "training specific parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_k1CJ0WdLlm"
      },
      "source": [
        "%%bash \n",
        "cat <<EOF >> en_chs.yaml\n",
        "# Vocabulary files that were just created\n",
        "src_vocab: en_chs/run/example.vocab.src\n",
        "tgt_vocab: en_chs/run/example.vocab.tgt\n",
        "\n",
        "# Train on a single GPU\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Where to save the checkpoints\n",
        "save_model: en_chs/run/model\n",
        "save_checkpoint_steps: 10000\n",
        "train_steps: 10000\n",
        "valid_steps: 5000\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgjyeEyVdcAR"
      },
      "source": [
        "Then you can simply run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnWQpabuddc9"
      },
      "source": [
        "!onmt_train -config en_chs.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y4lltv_dfQk"
      },
      "source": [
        "This configuration will run the default model, which consists of a 2-layer LSTM with 500 hidden units on both the encoder and decoder. It will run on a single GPU (world_size 1 & gpu_ranks [0]).\n",
        "\n",
        "Before the training process actually starts, the *.vocab.pt together with *.transforms.pt can be dumped to -save_data with configurations specified in -config yaml file by enabling the -dump_fields and -dump_transforms flags. It is also possible to generate transformed samples to simplify any potentially required visual inspection. The number of sample lines to dump per corpus is set with the -n_sample flag.\n",
        "\n",
        "For more advanded models and parameters, see other example configurations or the FAQ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_iIzy_-dqsR"
      },
      "source": [
        "**Step 3: Translate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kWMdbILdvTP"
      },
      "source": [
        "!onmt_translate -model en_chs/run/model_step_10000.pt -src en_chs/src-test.txt -output en_chs/pred_10000.txt -gpu 0 -verbose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLz4AJ74erb9"
      },
      "source": [
        "Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into toy-ende/pred_1000.txt.\n",
        "\n",
        "Note:\n",
        "\n",
        "The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for translation or summarization."
      ]
    }
  ]
}