{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_torch_version2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFQM12Qy2scn"
      },
      "source": [
        "!unzip MNIST.zip_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLwb4iUY3uNH",
        "outputId": "df268e64-070d-44d2-9253-c46b44d008ef"
      },
      "source": [
        "import cv2\n",
        "\n",
        "im = cv2.imread(\"./MNIST/0_0_10.jpg\")\n",
        "print(im.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "sHbfOT2E4My7",
        "outputId": "fdf7eaf7-b6da-4334-aa71-765720919b3d"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "#import numpy as np\n",
        "\n",
        "rootdir = './MNIST'\n",
        "names = os.listdir(rootdir)\n",
        "\n",
        "for name in names:\n",
        "  print(\"name: \", name)\n",
        "  path = os.path.join(rootdir, name)\n",
        "  im = cv2.imread(path)\n",
        "  im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  print(type(im))\n",
        "  print(im.shape)\n",
        "\n",
        "  # cv2.imshow('image', im) # BUG: crash the colab kernel\n",
        "  cv2_imshow(im)\n",
        "  plt.imshow(im.squeeze(), cmap='gray_r'); \n",
        "  \n",
        "  break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name:  7_10_6.jpg\n",
            "<class 'numpy.ndarray'>\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABMUlEQVR4nOWQvS5EURSFv7XPvUaEjISESjQkag9AFF5HL6KSaPx0ngCJTi2hkKBQzAOoJkKCSFBM5M49Zyvmjplcj+Ar91lrn7U2/CsEoTE/Nrfx7s9HmVA1lVsKp6MqmyuX6x5l2t5x4QASxkInxuhepphi4V8gAzA8S1obEXS/31/e9qbOnRA8AWROmZVPj6Ov13ftG0vyorCIlOh5yZldbEIQoBP/IPwmtSpYAMMmWuWVMP2+BQOBgbEUfaOnr7c1OE7tSYb2VmQA6pRnqL+1T4AGEp++TKiUQxgizHw/TKq6wUAhJdziVnj5ctzlNWsOq4UfDPoNh+7CXJ7ulBFItR7Cxu/Lm2nAal0MjN0ibgoY3KdHIk9M5LpwM/mfP7s53XTYwpNDLaww9m8bw54fGV5sUMOyEqgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F6E23605890>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v30zot4W7BRU"
      },
      "source": [
        "\"\"\"\n",
        "手写数字识别 pytorch 实现\n",
        "\n",
        "reference:\n",
        "    https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
        "    doc\\lang\\programming\\pytorch\\李宏毅2020机器翻译\\iAttention.py\n",
        "    doc\\lang\\programming\\pytorch\\数字识别\\ihandwritten_digit_recognition_GPU.ipynb\n",
        "    https://gist.github.com/user01/68514db1127eb007f24d28bfd11dd60e\n",
        "\n",
        "\n",
        "MNIST 图片的导出（）\n",
        "# import cv2\n",
        "\n",
        "# print( images.shape[0] )\n",
        "\n",
        "# for j in range(30):\n",
        "  \n",
        "#   images, labels = next(dataiter)\n",
        "\n",
        "#   for i in range(images.shape[0]):\n",
        "#     a = ((images[i].numpy().squeeze() + 1) / 2) * 255  # 原来的数值范围是 -1.0 ~ +1.0 ，规范为 0 ~ 255\n",
        "#     b = np.rint(a)  # Round elements of the array to the nearest integer.\n",
        "#     #plt.imshow(b, cmap='gray_r')\n",
        "#     #b\n",
        "#     label = labels.numpy()[i]\n",
        "#     #print(label)\n",
        "#     #cv2.imshow('Binary Threshold', b)\n",
        "#     cv2.imwrite(f'./out/{label}_{j}_{i}.jpg',b)\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.types import Number\n",
        "import torch.utils.data as torch_data\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from time import time\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def Data(type='training'):\n",
        "\n",
        "    #currDir = os.getcwd() # jupyter is ok\n",
        "    #currDir = os.path.dirname(os.path.abspath(__file__)) # jupyter not ok\n",
        "\n",
        "    data = []\n",
        "\n",
        "\n",
        "    currDir = \"/content\" # jupyter not ok\n",
        "\n",
        "    root = os.path.join(currDir, \"MNIST\")\n",
        "\n",
        "    names = os.listdir(root)\n",
        "\n",
        "    for name in names:\n",
        "\n",
        "        #print(\"name: \", name)\n",
        "\n",
        "        num = name.split('_')[0]  # number in fname already \n",
        "        num = int(num)\n",
        "\n",
        "        #print( \"label: \", num )\n",
        "\n",
        "        #num = num / 10  # normalize number to 0.0 ~ 1.0\n",
        "\n",
        "        path = os.path.join(root, name)\n",
        "        im = cv2.imread(path)\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        \"\"\"\n",
        "        输入：im.shape，  可以看到维度是 (28, 28)\n",
        "              im，       可以看到数值范围是 0 ~ 255\n",
        "              im/255     可以看到数值范围变成了 0.0 ~ 1.0\n",
        "        \"\"\"\n",
        "        # import pdb; pdb.set_trace() # 调试， exit 退出\n",
        "\n",
        "        im = im / 255\n",
        "\n",
        "        im_sq = im.reshape(28*28) # (28*28) 的图片降维成一维数组\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        输入：im_sq.shape 可以看到维度是 (784,)\n",
        "        \"\"\"\n",
        "\n",
        "        #import pdb; pdb.set_trace()\n",
        "\n",
        "\n",
        "        #print(im.shape)\n",
        "\n",
        "        #print( type(im) )\n",
        "        #print(im.shape)\n",
        "        #print(im)\n",
        "\n",
        "        # cv2.imshow('image', im) # BUG: crash the colab kernel\n",
        "        #cv2_imshow(im)\n",
        "        #plt.imshow(im.squeeze(), cmap='gray_r');\n",
        "\n",
        "        data.append( [ torch.Tensor(im_sq), torch.Tensor([num]) ] )\n",
        "\n",
        "        #break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "class TorchDataset(torch_data.Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, Index):\n",
        "    item = self.data[Index]\n",
        "\n",
        "    return item[0], item[1]\n",
        "\n",
        "def infinite_iter(data_loader):\n",
        "  it = iter(data_loader)\n",
        "  while True:\n",
        "    try:\n",
        "      item_in , item_out = next(it)\n",
        "      yield item_in, item_out\n",
        "    except StopIteration:\n",
        "      it = iter(data_loader)\n",
        "\n",
        "#Data();\n",
        "\n",
        "\n",
        "train_dataset = TorchDataset(Data(type='training'))\n",
        "train_loader = torch_data.DataLoader(train_dataset, batch_size = 64, shuffle=True)  # 每个输入是维度是(2) 的一维数组，每一批总共输入2 组，维度既是：(2, 2)\n",
        "train_iter = infinite_iter(train_loader)\n",
        "\n",
        "sources, targets = next(train_iter)\n",
        "\n",
        "print( sources, targets )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn0Uxx9hgQx-",
        "outputId": "fdfc7297-d997-4635-d189-9243af53b69e"
      },
      "source": [
        "# 训练\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "# Layer details for the neural network\n",
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "time0 = time()\n",
        "epochs = 3\n",
        "for e in range(epochs):\n",
        "\n",
        "    for i in range(10000):\n",
        "\n",
        "        running_loss = 0\n",
        "    \n",
        "        images, labels = next(train_iter)\n",
        "    \n",
        "        labels = labels.reshape(64)\n",
        "        labels = torch.tensor(labels, dtype=torch.long) \n",
        "    \n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(  images.cuda() if torch.cuda.is_available() else images.cpu() )\n",
        "\n",
        "        #import pdb; pdb.set_trace() # 调试， exit 退出\n",
        "\n",
        "        loss = criterion(output, labels.cuda() if torch.cuda.is_available() else labels.cpu())\n",
        "        \n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "        \n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(\"Epoch {} - Training loss: {}\".format(e, running_loss/64))\n",
        "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ")\n",
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.00011264593922533095\n",
            "\n",
            "Training Time (in minutes) = 0.25551716883977255\n",
            "Epoch 1 - Training loss: 1.376107320538722e-05\n",
            "\n",
            "Training Time (in minutes) = 0.5136989275614421\n",
            "Epoch 2 - Training loss: 8.917024388210848e-06\n",
            "\n",
            "Training Time (in minutes) = 0.769320539633433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "BYlOAl-zGLYC",
        "outputId": "5febf2df-8c69-42ff-a14b-d127e81f0827"
      },
      "source": [
        "correct_count, all_count = 0, 0\n",
        "\n",
        "images, labels = next(train_iter)\n",
        "labels = labels.reshape(64)\n",
        "labels = torch.tensor(labels, dtype=torch.long) \n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(1):\n",
        "    img = images[i].reshape(1, 784)\n",
        "    logps = model(img.cuda())\n",
        "\n",
        "    # Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "    ps = torch.exp(logps)\n",
        "    probab = list(ps.cpu().numpy()[0])\n",
        "    pred_label = probab.index(max(probab))\n",
        "\n",
        "\n",
        "    img_to_show = images[i].reshape(28,28)\n",
        "    img_to_show = img_to_show * 255\n",
        "\n",
        "    plt.imshow(img_to_show, cmap='gray_r'); \n",
        "\n",
        "    print(pred_label)  # 预测的数字\n",
        "\n",
        "    true_label = labels.numpy()[i]\n",
        "\n",
        "    print(true_label)  # 真实的数字\n",
        "\n",
        "    if(true_label == pred_label):\n",
        "      correct_count += 1\n",
        "    all_count += 1\n",
        "\n",
        "\n",
        "print(\"Number Of Images Tested =\", all_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count))  # 预测的正确率"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "3\n",
            "Number Of Images Tested = 1\n",
            "\n",
            "Model Accuracy = 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 251,
              "height": 248
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVLao9GxqErS"
      },
      "source": [
        "# 准备数据\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('./mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
        "valset = datasets.MNIST('./mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPVS6m1wXsu5"
      },
      "source": [
        "# 观察数据\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)  # images [64, 1, 28, 28]，64 张，1 通道，28 宽，28 高 的图片  # labels [64] 64个标签\n",
        "  # 28*28 = 784 个像素点，每张图片作为输入是[768] 的一维数组，每一批总共输入64 张，既是：[64, 784]\n",
        "  # so, 输入层共784 个神经元结点，每个结点得到图片的其中一个像素作为输入\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');  # squeeze 将 [1, 28, 28] 降维成 [28, 28]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "输入： images[0].numpy().squeeze()\n",
        "可以看到数值范围是：-1.0 ~ +1.0\n",
        "\"\"\"\n",
        "# import pdb; pdb.set_trace() # 调试， exit 退出\n",
        "\n",
        "\n",
        "figure = plt.figure()\n",
        "num_of_images = 60\n",
        "for index in range(1, num_of_images + 1):\n",
        "    plt.subplot(6, 10, index)  # 绘制  6 行，10 列 个对象\n",
        "    plt.axis('off')            # 关闭坐标轴显示\n",
        "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iov7ufvQqPhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "5506360c-8c35-4280-8979-c1f89dacdff4"
      },
      "source": [
        "# 训练\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "# Layer details for the neural network\n",
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "time0 = time()\n",
        "epochs = 15\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)  # 元素的总数不变，对维度重新进行解释，相当于 reshape  # 64 行，列数自适应(-1 的作用)  \n",
        "          # 28*28 = 784 个像素点，每张图片作为输入是[784] 的一维数组，每一批总共输入64 张，既是：[64, 784]\n",
        "    \n",
        "        \"\"\"\n",
        "        输入： \n",
        "            images[0] 可以看到是张量 tensor，数值范围是：-1.0 ~ +1.0\n",
        "            images[0].shape，可以看到维度是 784，是一维数组\n",
        "            ((images[0]+1)/2)*255 ，数值范围规范为：0 ~ 255\n",
        "        \"\"\"\n",
        "        #import pdb; pdb.set_trace() # 调试， exit 退出\n",
        "\n",
        "\n",
        "        for i in range(len(images)):\n",
        "          images[i] = ( (images[i]+1) / 2 ) # 原来的数值范围是 -1.0 ~ +1.0 ，规范为 0 ~ +1.0\n",
        "        \n",
        "\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "\n",
        "\n",
        "        output = model(  images.cuda() if torch.cuda.is_available() else images.cpu() )\n",
        "        loss = criterion(output, labels.cuda() if torch.cuda.is_available() else  labels.cpu())\n",
        "\n",
        "        import pdb; pdb.set_trace() # 调试， exit 退出\n",
        "        \n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "        \n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
        "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ")\n",
            "cpu\n",
            "> <ipython-input-10-32d940273697>(76)<module>()\n",
            "-> loss.backward()\n",
            "(Pdb) label.shape\n",
            "*** NameError: name 'label' is not defined\n",
            "(Pdb) labels.shape\n",
            "torch.Size([64])\n",
            "(Pdb) exit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "BdbQuit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-32d940273697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#This is where the model learns by backpropagating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#And optimizes its weights here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-32d940273697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#This is where the model learns by backpropagating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#And optimizes its weights here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBdbQuit\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7NcRWeYambN"
      },
      "source": [
        "# 验证模型精度\n",
        "correct_count, all_count = 0, 0\n",
        "for images,labels in valloader:\n",
        "  for i in range(len(labels)):\n",
        "    img = images[i].view(1, 784)\n",
        "    img = ( (img+1) / 2 ) # 原来的数值范围是 -1.0 ~ +1.0 ，规范为 0 ~ +1.0\n",
        "    # Turn off gradients to speed up this part\n",
        "    with torch.no_grad():\n",
        "        logps = model(img.cuda())\n",
        "\n",
        "    # Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "    ps = torch.exp(logps)\n",
        "    probab = list(ps.cpu().numpy()[0])\n",
        "    pred_label = probab.index(max(probab))\n",
        "    true_label = labels.numpy()[i]\n",
        "    if(true_label == pred_label):\n",
        "      correct_count += 1\n",
        "    all_count += 1\n",
        "\n",
        "print(\"Number Of Images Tested =\", all_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bdAmw1v7DHq"
      },
      "source": [
        "# 预测\n",
        "def view_classify(img, ps):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.cpu().data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    ax2.set_yticklabels(np.arange(10))\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "\n",
        "images, labels = next(iter(valloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "img = ( (img+1) / 2 ) # 原来的数值范围是 -1.0 ~ +1.0 ，规范为 0 ~ +1.0\n",
        "\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = model(img.cuda())\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "probab = list(ps.cpu().numpy()[0])\n",
        "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}