{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1080ti~3090_python3.8_cuda11.1_DBNet_All_In_One.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZHM14c8QU2mJ",
        "BoRrrbQoXgPa",
        "wMnQ3eueVpPq",
        "okDL0LPgV1tJ",
        "hfnbUS8BXd0G",
        "ly0UmMEdkNVn",
        "KC38E3Vxb4rK",
        "PIOXgffuHa8K",
        "hmqwD5BbnlHx",
        "gTR68GOzAcAA",
        "NNZ7Q0GUB7MO",
        "sh4_vWqMopXX",
        "BdTpHw3Ao1JY",
        "ZfTr8HuWSUp_"
      ],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- https://blog.csdn.net/qq_46521210/article/details/122211173\n",
        "\n",
        "colab 的包全部装在这里： /usr/local/lib/python3.7/dist-packages  \n",
        "conda 的包全部装在这里： /root/miniforge3/envs/DB/lib/python3.7/site-packages  "
      ],
      "metadata": {
        "id": "V0jxnJUcB-rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Info"
      ],
      "metadata": {
        "id": "ZHM14c8QU2mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /etc/os-release"
      ],
      "metadata": {
        "id": "beNe0VNGU53A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /proc/version"
      ],
      "metadata": {
        "id": "Pi1PXXt8VD6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Info"
      ],
      "metadata": {
        "id": "BoRrrbQoXgPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "3WOLVdj8Xjna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install CUDA11.1"
      ],
      "metadata": {
        "id": "V589mH1wyTmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove old cuda"
      ],
      "metadata": {
        "id": "wMnQ3eueVpPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#Uninstall the current CUDA version\n",
        "apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "apt-get remove cuda-*\n",
        "apt autoremove\n",
        "apt-get update"
      ],
      "metadata": {
        "id": "tAXzr9Pr-h5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get --purge -y remove 'cuda*'\n",
        "apt-get --purge -y remove 'nvidia*'\n",
        "apt autoremove -y\n",
        "apt-get clean\n",
        "apt update -qq;"
      ],
      "metadata": {
        "id": "8V8XUdH6_CsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add CUDA GPG key"
      ],
      "metadata": {
        "id": "okDL0LPgV1tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb"
      ],
      "metadata": {
        "id": "F9QUlIFJWOvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! dpkg -i cuda-keyring_1.0-1_all.deb"
      ],
      "metadata": {
        "id": "1SnRSFlBWioW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install cuda"
      ],
      "metadata": {
        "id": "hfnbUS8BXd0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# wget --no-clobber https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcudnn8_8.0.5.39-1+cuda11.1_amd64.deb"
      ],
      "metadata": {
        "id": "dl75daphTkHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dpkg -i libcudnn8_8.0.5.39-1+cuda11.1_amd64.deb"
      ],
      "metadata": {
        "id": "6CfevjQJWy7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get -y install cuda-11-1"
      ],
      "metadata": {
        "id": "kX4wCZk6X_bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "metadata": {
        "id": "27IC33YJr3Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ldconfig -p | grep cuda"
      ],
      "metadata": {
        "id": "S3AQ-6qeGy1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install conda"
      ],
      "metadata": {
        "id": "ly0UmMEdkNVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\n",
        "bash Miniforge3-Linux-x86_64.sh -b"
      ],
      "metadata": {
        "id": "ph_8Yoy2kVNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "~/miniforge3/bin/conda init"
      ],
      "metadata": {
        "id": "osfM28aLkpCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ln -s ~/miniforge3/bin/conda /usr/local/bin"
      ],
      "metadata": {
        "id": "w2RO_CIKna2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ln -s ~/miniforge3/bin/activate /usr/local/bin"
      ],
      "metadata": {
        "id": "3eamE-aon178"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ln -s ~/miniforge3/bin/deactivate /usr/local/bin"
      ],
      "metadata": {
        "id": "N4yEOtKOn-Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source ~/miniforge3/etc/profile.d/conda.sh"
      ],
      "metadata": {
        "id": "F8eOv23RlgFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install DB environment"
      ],
      "metadata": {
        "id": "KC38E3Vxb4rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "conda update -y conda -n base && \\\n",
        "conda install ipython pip --yes && \\\n",
        "conda create -n DB python=3.7 --yes && \\\n",
        "source activate DB && \\\n",
        "conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.1 -c pytorch --yes"
      ],
      "metadata": {
        "id": "Pe43sOarYwhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB\n",
        "git clone https://github.com/MhLiao/DB.git\n",
        "cd DB\n",
        "pip install -r requirement.txt"
      ],
      "metadata": {
        "id": "MUh1UwWXGmCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB\n",
        "pip3 install --upgrade protobuf==3.20.0"
      ],
      "metadata": {
        "id": "gNRv64p9Gogj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB && \\\n",
        "apt install build-essential  && \\\n",
        "export CUDA_HOME=/usr/local/cuda && \\\n",
        "echo $CUDA_HOME && \\\n",
        "cd /content/DB/assets/ops/dcn/ && \\\n",
        "sed -i 's/AT_CHECK/TORCH_CHECK/1' /content/DB/assets/ops/dcn/src/deform_conv_cuda.cpp && \\\n",
        "sed -i 's/AT_CHECK/TORCH_CHECK/1' /content/DB/assets/ops/dcn/src/deform_pool_cuda.cpp && \\\n",
        "python setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "uGsp5wmdLwCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify batch_size & num_workers"
      ],
      "metadata": {
        "id": "PIOXgffuHa8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sed -i 's/batch_size\\:\\ 16/batch_size\\:\\ 10/1' DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml\n",
        "sed -i 's/num_workers\\:\\ 16/num_workers\\:\\ 10/1' DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml"
      ],
      "metadata": {
        "id": "5AFZxr_FKKsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify epochs "
      ],
      "metadata": {
        "id": "hmqwD5BbnlHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sed -i 's/save_interval\\:\\ 18000/save_interval\\:\\ 450/1' DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml\n",
        "sed -i 's/epochs\\:\\ 1200/epochs\\:\\ 30/1' DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml"
      ],
      "metadata": {
        "id": "7FApnwr0m980"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "gTR68GOzAcAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! gdown --id '1Q2WXxV7aADurPF1ElQJUypnETSOJH7ZO'\n",
        "! gdown --id '1U9RDgco8-YDWbvjO_gKuNiHK684CoQ8f'\n",
        "\n"
      ],
      "metadata": {
        "id": "7eNje9wLAlhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip ./TD500.zip\n",
        "! unzip ./TD_TR.zip -d DB/datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "pROMHLdbAoP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainning"
      ],
      "metadata": {
        "id": "0S8OkzGhKoYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB\n",
        "echo $CUDA_HOME\n",
        "cd DB\n",
        "CUDA_VISIBLE_DEVICES=0 python train.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --num_gpus 1"
      ],
      "metadata": {
        "id": "bYUq9oLNKtqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evel"
      ],
      "metadata": {
        "id": "NNZ7Q0GUB7MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown --id '1Lud-q-as3O4LOzz58hihKtCTNadFcgga' "
      ],
      "metadata": {
        "id": "fmKYF0joF0TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB\n",
        "echo $CUDA_HOME\n",
        "cd DB\n",
        "CUDA_VISIBLE_DEVICES=0 python demo.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --image_path /content/DB/datasets/TD_TR/TD500/test_images/IMG_0059.JPG --resume /content/td500_resnet18 --polygon --box_thresh 0.7 --visualize"
      ],
      "metadata": {
        "id": "TbLQ5qEmCBTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB\n",
        "echo $CUDA_HOME\n",
        "cd DB\n",
        "CUDA_VISIBLE_DEVICES=0 python demo.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --image_path /content/DB/datasets/TD_TR/TD500/train_images/IMG_1783.JPG --resume /content/td500_resnet18 --polygon --box_thresh 0.7 --visualize"
      ],
      "metadata": {
        "id": "rfjimd2bG0Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate DB\n",
        "echo $CUDA_HOME\n",
        "cd DB\n",
        "CUDA_VISIBLE_DEVICES=0 python demo.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --image_path /content/01.jpg --resume /content/td500_resnet18 --polygon --box_thresh 0.7 --visualize"
      ],
      "metadata": {
        "id": "OW-U_lH-1163"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All in one"
      ],
      "metadata": {
        "id": "8eMM9xSI8Ekm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy Conda Packages "
      ],
      "metadata": {
        "id": "sh4_vWqMopXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "把前面在虚拟环境 DB 中安装所有 package 复制到 colab 的真实环境中"
      ],
      "metadata": {
        "id": "7-OOy0PspBB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -rf /root/miniforge3/envs/DB/lib/python3.7/site-packages/* /usr/local/lib/python3.7/dist-packages"
      ],
      "metadata": {
        "id": "PISL3-9WopXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python -c 'import anyconfig;print(anyconfig.__file__)' # 注意这里没有激活 conda 的虚拟环境 DB ，所以这里是 colab 的原生 python 环境 "
      ],
      "metadata": {
        "id": "CAL2YMYxopXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! touch DB/__init__.py"
      ],
      "metadata": {
        "id": "ed92AC0Kp93G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r DB/assets/ops/dcn /usr/local/lib/python3.7/dist-packages  # 上面自已编译的包，复制到 python3.7 的系统包目录，方便后面导入"
      ],
      "metadata": {
        "id": "gFwTX5T4r1Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dcn import DeformConv"
      ],
      "metadata": {
        "id": "ZNJG5aOzqX9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traning"
      ],
      "metadata": {
        "id": "BdTpHw3Ao1JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DB/concern/config.py\n",
        "import importlib\n",
        "from collections import OrderedDict\n",
        "\n",
        "import anyconfig\n",
        "import munch\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def load(self, conf):\n",
        "        import pdb; pdb.set_trace()\n",
        "        conf = anyconfig.load(conf)\n",
        "        return munch.munchify(conf)\n",
        "\n",
        "    def compile(self, conf, return_packages=False):\n",
        "        packages = conf.get('package', [])\n",
        "        defines = {}\n",
        "\n",
        "        for path in conf.get('import', []):\n",
        "            parent_conf = self.load(path)\n",
        "            parent_packages, parent_defines = self.compile(\n",
        "                parent_conf, return_packages=True)\n",
        "            packages.extend(parent_packages)\n",
        "            defines.update(parent_defines)\n",
        "\n",
        "        modules = []\n",
        "        for package in packages:\n",
        "            module = importlib.import_module(package)\n",
        "            modules.append(module)\n",
        "\n",
        "        if isinstance(conf['define'], dict):\n",
        "            conf['define'] = [conf['define']]\n",
        "\n",
        "        for define in conf['define']:\n",
        "            name = define.copy().pop('name')\n",
        "\n",
        "            if not isinstance(name, str):\n",
        "                raise RuntimeError('name must be str')\n",
        "\n",
        "            defines[name] = self.compile_conf(define, defines, modules)\n",
        "\n",
        "        if return_packages:\n",
        "            return packages, defines\n",
        "        else:\n",
        "            return defines\n",
        "\n",
        "    def compile_conf(self, conf, defines, modules):\n",
        "        if isinstance(conf, (int, float)):\n",
        "            return conf\n",
        "        elif isinstance(conf, str):\n",
        "            if conf.startswith('^'):\n",
        "                return defines[conf[1:]]\n",
        "            if conf.startswith('$'):\n",
        "                return {'class': self.find_class_in_modules(conf[1:], modules)}\n",
        "            return conf\n",
        "        elif isinstance(conf, dict):\n",
        "            if 'class' in conf:\n",
        "                conf['class'] = self.find_class_in_modules(\n",
        "                    conf['class'], modules)\n",
        "            if 'base' in conf:\n",
        "                base = conf.copy().pop('base')\n",
        "\n",
        "                if not isinstance(base, str):\n",
        "                    raise RuntimeError('base must be str')\n",
        "\n",
        "                conf = {\n",
        "                    **defines[base],\n",
        "                    **conf,\n",
        "                }\n",
        "            return {key: self.compile_conf(value, defines, modules) for key, value in conf.items()}\n",
        "        elif isinstance(conf, (list, tuple)):\n",
        "            return [self.compile_conf(value, defines, modules) for value in conf]\n",
        "        else:\n",
        "            return conf\n",
        "\n",
        "    def find_class_in_modules(self, cls, modules):\n",
        "        if not isinstance(cls, str):\n",
        "            raise RuntimeError('class name must be str')\n",
        "\n",
        "        if cls.find('.') != -1:\n",
        "            package, cls = cls.rsplit('.', 1)\n",
        "            module = importlib.import_module(package)\n",
        "            if hasattr(module, cls):\n",
        "                return module.__name__ + '.' + cls\n",
        "\n",
        "        for module in modules:\n",
        "            if hasattr(module, cls):\n",
        "                return module.__name__ + '.' + cls\n",
        "        raise RuntimeError('class not found ' + cls)\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self, autoload=True, default=None):\n",
        "        self.autoload = autoload\n",
        "        self.default = default\n",
        "\n",
        "\n",
        "class StateMeta(type):\n",
        "    def __new__(mcs, name, bases, attrs):\n",
        "        current_states = []\n",
        "        for key, value in attrs.items():\n",
        "            if isinstance(value, State):\n",
        "                current_states.append((key, value))\n",
        "\n",
        "        current_states.sort(key=lambda x: x[0])\n",
        "        attrs['states'] = OrderedDict(current_states)\n",
        "        new_class = super(StateMeta, mcs).__new__(mcs, name, bases, attrs)\n",
        "\n",
        "        # Walk through the MRO\n",
        "        states = OrderedDict()\n",
        "        for base in reversed(new_class.__mro__):\n",
        "            if hasattr(base, 'states'):\n",
        "                states.update(base.states)\n",
        "        new_class.states = states\n",
        "\n",
        "        for key, value in states.items():\n",
        "            setattr(new_class, key, value.default)\n",
        "\n",
        "        return new_class\n",
        "\n",
        "\n",
        "class Configurable(metaclass=StateMeta):\n",
        "    def __init__(self, *args, cmd={}, **kwargs):\n",
        "        self.load_all(cmd=cmd, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def construct_class_from_config(args):\n",
        "        cls = Configurable.extract_class_from_args(args)\n",
        "        return cls(**args)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_class_from_args(args):\n",
        "        cls = args.copy().pop('class')\n",
        "        package, cls = cls.rsplit('.', 1)\n",
        "        module = importlib.import_module(package)\n",
        "        cls = getattr(module, cls)\n",
        "        return cls\n",
        "\n",
        "    def load_all(self, **kwargs):\n",
        "        for name, state in self.states.items():\n",
        "            if state.autoload:\n",
        "                self.load(name, **kwargs)\n",
        "\n",
        "    def load(self, state_name, **kwargs):\n",
        "        # FIXME: kwargs should be filtered\n",
        "        # Args passed from command line\n",
        "        cmd = kwargs.pop('cmd', dict())\n",
        "        if state_name in kwargs:\n",
        "            setattr(self, state_name, self.create_member_from_config(\n",
        "                (kwargs[state_name], cmd)))\n",
        "        else:\n",
        "            setattr(self, state_name, self.states[state_name].default)\n",
        "\n",
        "    def create_member_from_config(self, conf):\n",
        "        args, cmd = conf\n",
        "        if args is None or isinstance(args, (int, float, str)):\n",
        "            return args\n",
        "        elif isinstance(args, (list, tuple)):\n",
        "            return [self.create_member_from_config((subargs, cmd)) for subargs in args]\n",
        "        elif isinstance(args, dict):\n",
        "            if 'class' in args:\n",
        "                cls = self.extract_class_from_args(args)\n",
        "                return cls(**args, cmd=cmd)\n",
        "            return {key: self.create_member_from_config((subargs, cmd)) for key, subargs in args.items()}\n",
        "        else:\n",
        "            return args\n",
        "\n",
        "    def dump(self):\n",
        "        state = {}\n",
        "        state['class'] = self.__class__.__module__ + \\\n",
        "            '.' + self.__class__.__name__\n",
        "        for name, value in self.states.items():\n",
        "            obj = getattr(self, name)\n",
        "            state[name] = self.dump_obj(obj)\n",
        "        return state\n",
        "\n",
        "    def dump_obj(self, obj):\n",
        "        if obj is None:\n",
        "            return None\n",
        "        elif hasattr(obj, 'dump'):\n",
        "            return obj.dump()\n",
        "        elif isinstance(obj, (int, float, str)):\n",
        "            return obj\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return [self.dump_obj(value) for value in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: self.dump_obj(value) for key, value in obj.items()}\n",
        "        else:\n",
        "            return str(obj)\n",
        "\n",
        "\n",
        "# DB/concern/log.py\n",
        "import os\n",
        "import logging\n",
        "import functools\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# from concern.config import Configurable, State\n",
        "\n",
        "\n",
        "class Logger(Configurable):\n",
        "    SUMMARY_DIR_NAME = 'summaries'\n",
        "    VISUALIZE_NAME = 'visualize'\n",
        "    LOG_FILE_NAME = 'output.log'\n",
        "    ARGS_FILE_NAME = 'args.log'\n",
        "    METRICS_FILE_NAME = 'metrics.log'\n",
        "\n",
        "    database_dir = State(default='./outputs/')\n",
        "    log_dir = State(default='workspace')\n",
        "    verbose = State(default=False)\n",
        "    level = State(default='info')\n",
        "    log_interval = State(default=100)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "        self._make_storage()\n",
        "\n",
        "        cmd = kwargs['cmd']\n",
        "        self.name = cmd['name']\n",
        "        self.log_dir = os.path.join(self.log_dir, self.name)\n",
        "        try:\n",
        "            self.verbose = cmd['verbose']\n",
        "        except:\n",
        "            print('verbose:', self.verbose)\n",
        "        if self.verbose:\n",
        "            print('Initializing log dir for', self.log_dir)\n",
        "\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "\n",
        "        self.message_logger = self._init_message_logger()\n",
        "\n",
        "        summary_path = os.path.join(self.log_dir, self.SUMMARY_DIR_NAME)\n",
        "        self.tf_board_logger = SummaryWriter(summary_path)\n",
        "\n",
        "        self.metrics_writer = open(os.path.join(\n",
        "            self.log_dir, self.METRICS_FILE_NAME), 'at')\n",
        "\n",
        "        self.timestamp = time.time()\n",
        "        self.logged = -1\n",
        "        self.speed = None\n",
        "        self.eta_time = None\n",
        "\n",
        "    def _make_storage(self):\n",
        "        application = os.path.basename(os.getcwd())\n",
        "        storage_dir = os.path.join(\n",
        "            self.database_dir, self.log_dir, application)\n",
        "        if not os.path.exists(storage_dir):\n",
        "            os.makedirs(storage_dir)\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.symlink(storage_dir, self.log_dir)\n",
        "\n",
        "    def save_dir(self, dir_name):\n",
        "        return os.path.join(self.log_dir, dir_name)\n",
        "\n",
        "    def _init_message_logger(self):\n",
        "        message_logger = logging.getLogger('messages')\n",
        "        message_logger.setLevel(\n",
        "            logging.DEBUG if self.verbose else logging.INFO)\n",
        "        formatter = logging.Formatter(\n",
        "            '[%(levelname)s] [%(asctime)s] %(message)s')\n",
        "        std_handler = logging.StreamHandler()\n",
        "        std_handler.setLevel(message_logger.level)\n",
        "        std_handler.setFormatter(formatter)\n",
        "\n",
        "        file_handler = logging.FileHandler(\n",
        "            os.path.join(self.log_dir, self.LOG_FILE_NAME))\n",
        "        file_handler.setLevel(message_logger.level)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        message_logger.addHandler(std_handler)\n",
        "        message_logger.addHandler(file_handler)\n",
        "        return message_logger\n",
        "\n",
        "    def report_time(self, name: str):\n",
        "        if self.verbose:\n",
        "            self.info(name + \" time :\" + str(time.time() - self.timestamp))\n",
        "            self.timestamp = time.time()\n",
        "\n",
        "    def report_eta(self, steps, total, epoch):\n",
        "        self.logged = self.logged % total + 1\n",
        "        steps = steps % total\n",
        "        if self.eta_time is None:\n",
        "            self.eta_time = time.time()\n",
        "            speed = -1\n",
        "        else:\n",
        "            eta_time = time.time()\n",
        "            speed = eta_time - self.eta_time\n",
        "            if self.speed is not None:\n",
        "                speed = ((self.logged - 1) * self.speed + speed) / self.logged\n",
        "            self.speed = speed\n",
        "            self.eta_time = eta_time\n",
        "\n",
        "        seconds = (total - steps) * speed\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds - (hours * 3600)) // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        print('%d/%d batches processed in epoch %d, ETA: %2d:%2d:%2d' %\n",
        "              (steps, total, epoch,\n",
        "               hours, minutes, seconds), end='\\r')\n",
        "\n",
        "    def args(self, parameters=None):\n",
        "        if parameters is None:\n",
        "            with open(os.path.join(self.log_dir, self.ARGS_FILE_NAME), 'rt') as reader:\n",
        "                return yaml.load(reader.read())\n",
        "        with open(os.path.join(self.log_dir, self.ARGS_FILE_NAME), 'wt') as writer:\n",
        "            yaml.dump(parameters.dump(), writer)\n",
        "\n",
        "    def metrics(self, epoch, steps, metrics_dict):\n",
        "        results = {}\n",
        "        for name, a in metrics_dict.items():\n",
        "            results[name] = {'count': a.count, 'value': float(a.avg)}\n",
        "            self.add_scalar('metrics/' + name, a.avg, steps)\n",
        "        result_dict = {\n",
        "            str(datetime.now()): {\n",
        "                'epoch': epoch,\n",
        "                'steps': steps,\n",
        "                **results\n",
        "            }\n",
        "        }\n",
        "        string_result = yaml.dump(result_dict)\n",
        "        self.info(string_result)\n",
        "        self.metrics_writer.write(string_result)\n",
        "        self.metrics_writer.flush()\n",
        "\n",
        "    def named_number(self, name, num=None, default=0):\n",
        "        if num is None:\n",
        "            return int(self.has_signal(name)) or default\n",
        "        else:\n",
        "            with open(os.path.join(self.log_dir, name), 'w') as writer:\n",
        "                writer.write(str(num))\n",
        "            return num\n",
        "\n",
        "    epoch = functools.partialmethod(named_number, 'epoch')\n",
        "    iter = functools.partialmethod(named_number, 'iter')\n",
        "\n",
        "    def message(self, level, content):\n",
        "        self.message_logger.__getattribute__(level)(content)\n",
        "\n",
        "    def images(self, prefix, image_dict, step):\n",
        "        for name, image in image_dict.items():\n",
        "            self.add_image(prefix + '/' + name, image, step, dataformats='HWC')\n",
        "\n",
        "    def merge_save_images(self, name, images):\n",
        "        for i, image in enumerate(images):\n",
        "            if i == 0:\n",
        "                result = image\n",
        "            else:\n",
        "                result = np.concatenate([result, image], 0)\n",
        "        cv2.imwrite(os.path.join(self.vis_dir(), name+'.jpg'), result)\n",
        "\n",
        "    def vis_dir(self):\n",
        "        vis_dir = os.path.join(self.log_dir, self.VISUALIZE_NAME)\n",
        "        if not os.path.exists(vis_dir):\n",
        "            os.mkdir(vis_dir)\n",
        "        return vis_dir\n",
        "\n",
        "    def save_image_dict(self, images, max_size=1024):\n",
        "        for file_name, image in images.items():\n",
        "            height, width = image.shape[:2]\n",
        "            if height > width:\n",
        "                actual_height = min(height, max_size)\n",
        "                actual_width = int(round(actual_height * width / height))\n",
        "            else:\n",
        "                actual_width = min(width, max_size)\n",
        "                actual_height = int(round(actual_width * height / width))\n",
        "                image = cv2.resize(image, (actual_width, actual_height))\n",
        "            cv2.imwrite(os.path.join(self.vis_dir(), file_name+'.jpg'), image)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        message_levels = set(['debug', 'info', 'warning', 'error', 'critical'])\n",
        "        if name == '__setstate__':\n",
        "            raise AttributeError('haha')\n",
        "        if name in message_levels:\n",
        "            return functools.partial(self.message, name)\n",
        "        elif hasattr(self.__dict__.get('tf_board_logger'), name):\n",
        "            return self.tf_board_logger.__getattribute__(name)\n",
        "        else:\n",
        "            super()\n",
        "\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def constant_init(module, constant, bias=0):\n",
        "    nn.init.constant_(module.weight, constant)\n",
        "    if hasattr(module, 'bias'):\n",
        "        nn.init.constant_(module.bias, bias)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.with_dcn = dcn is not None\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.with_modulated_dcn = False\n",
        "        if self.with_dcn:\n",
        "            fallback_on_stride = dcn.get('fallback_on_stride', False)\n",
        "            self.with_modulated_dcn = dcn.get('modulated', False)\n",
        "        # self.conv2 = conv3x3(planes, planes)\n",
        "        if not self.with_dcn or fallback_on_stride:\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                   padding=1, bias=False)\n",
        "        else:\n",
        "            deformable_groups = dcn.get('deformable_groups', 1)\n",
        "            if not self.with_modulated_dcn:\n",
        "                # from assets.ops.dcn import DeformConv\n",
        "                from dcn import DeformConv\n",
        "                conv_op = DeformConv\n",
        "                offset_channels = 18\n",
        "            else:\n",
        "                # from assets.ops.dcn import ModulatedDeformConv\n",
        "                from dcn import ModulatedDeformConv\n",
        "                conv_op = ModulatedDeformConv\n",
        "                offset_channels = 27\n",
        "            self.conv2_offset = nn.Conv2d(\n",
        "                planes,\n",
        "                deformable_groups * offset_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1)\n",
        "            self.conv2 = conv_op(\n",
        "                planes,\n",
        "                planes,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                deformable_groups=deformable_groups,\n",
        "                bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # out = self.conv2(out)\n",
        "        if not self.with_dcn:\n",
        "            out = self.conv2(out)\n",
        "        elif self.with_modulated_dcn:\n",
        "            offset_mask = self.conv2_offset(out)\n",
        "            offset = offset_mask[:, :18, :, :]\n",
        "            mask = offset_mask[:, -9:, :, :].sigmoid()\n",
        "            out = self.conv2(out, offset, mask)\n",
        "        else:\n",
        "            offset = self.conv2_offset(out)\n",
        "            out = self.conv2(out, offset)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.with_dcn = dcn is not None\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes)\n",
        "        fallback_on_stride = False\n",
        "        self.with_modulated_dcn = False\n",
        "        if self.with_dcn:\n",
        "            fallback_on_stride = dcn.get('fallback_on_stride', False)\n",
        "            self.with_modulated_dcn = dcn.get('modulated', False)\n",
        "        if not self.with_dcn or fallback_on_stride:\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                   stride=stride, padding=1, bias=False)\n",
        "        else:\n",
        "            deformable_groups = dcn.get('deformable_groups', 1)\n",
        "            if not self.with_modulated_dcn:\n",
        "                from assets.ops.dcn import DeformConv\n",
        "                conv_op = DeformConv\n",
        "                offset_channels = 18\n",
        "            else:\n",
        "                from assets.ops.dcn import ModulatedDeformConv\n",
        "                conv_op = ModulatedDeformConv\n",
        "                offset_channels = 27\n",
        "            self.conv2_offset = nn.Conv2d(\n",
        "                planes, deformable_groups * offset_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1)\n",
        "            self.conv2 = conv_op(\n",
        "                planes, planes, kernel_size=3, padding=1, stride=stride,\n",
        "                deformable_groups=deformable_groups, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.dcn = dcn\n",
        "        self.with_dcn = dcn is not None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # out = self.conv2(out)\n",
        "        if not self.with_dcn:\n",
        "            out = self.conv2(out)\n",
        "        elif self.with_modulated_dcn:\n",
        "            offset_mask = self.conv2_offset(out)\n",
        "            offset = offset_mask[:, :18, :, :]\n",
        "            mask = offset_mask[:, -9:, :, :].sigmoid()\n",
        "            out = self.conv2(out, offset, mask)\n",
        "        else:\n",
        "            offset = self.conv2_offset(out)\n",
        "            out = self.conv2(out, offset)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000, \n",
        "                 dcn=None, stage_with_dcn=(False, False, False, False)):\n",
        "        self.dcn = dcn\n",
        "        self.stage_with_dcn = stage_with_dcn\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], stride=2, dcn=dcn)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], stride=2, dcn=dcn)\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], stride=2, dcn=dcn)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "    \n",
        "        self.smooth = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=1)    \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        if self.dcn is not None:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):\n",
        "                    if hasattr(m, 'conv2_offset'):\n",
        "                        constant_init(m.conv2_offset, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dcn=None):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes,\n",
        "                            stride, downsample, dcn=dcn))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dcn=dcn))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x2 = self.layer1(x)\n",
        "        x3 = self.layer2(x2)\n",
        "        x4 = self.layer3(x3)\n",
        "        x5 = self.layer4(x4)\n",
        "\n",
        "        return x2, x3, x4, x5\n",
        "\n",
        "\n",
        "def resnet18(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet18']), strict=False)\n",
        "    return model\n",
        "\n",
        "def deformable_resnet18(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2],\n",
        "                    dcn=dict(modulated=True,\n",
        "                            deformable_groups=1,\n",
        "                            fallback_on_stride=False),\n",
        "                    stage_with_dcn=[False, True, True, True], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet18']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet34']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet50']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deformable_resnet50(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model with deformable conv.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   dcn=dict(modulated=True,\n",
        "                            deformable_groups=1,\n",
        "                            fallback_on_stride=False),\n",
        "                   stage_with_dcn=[False, True, True, True],\n",
        "                   **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet50']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet101']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet152']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "# DB/structure/model.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#import backbones\n",
        "#import decoders\n",
        "\n",
        "\n",
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        #self.backbone = getattr(backbones, args['backbone'])(**args.get('backbone_args', {}))\n",
        "        backboneName = 'deformable_resnet18' # args['backbone']\n",
        "        backboneFunc = deformable_resnet18 #getattr(backbones, backboneName)\n",
        "        backboneInstance = backboneFunc(**args.get('backbone_args', {}))\n",
        "        self.backbone = backboneInstance\n",
        "\n",
        "        self.decoder = getattr(decoders, args['decoder'])(**args.get('decoder_args', {}))\n",
        "\n",
        "    def forward(self, data, *args, **kwargs):\n",
        "        return self.decoder(self.backbone(data), *args, **kwargs)\n",
        "\n",
        "\n",
        "def parallelize(model, distributed, local_rank):\n",
        "    if distributed:\n",
        "        return nn.parallel.DistributedDataParallel(\n",
        "            model,\n",
        "            device_ids=[local_rank],\n",
        "            output_device=[local_rank],\n",
        "            find_unused_parameters=True)\n",
        "    else:\n",
        "        return nn.DataParallel(model)\n",
        "\n",
        "class SegDetectorModel(nn.Module):\n",
        "    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n",
        "        super(SegDetectorModel, self).__init__()\n",
        "        from decoders.seg_detector_loss import SegDetectorLossBuilder\n",
        "\n",
        "        self.model = BasicModel(args)\n",
        "        # for loading models\n",
        "        self.model = parallelize(self.model, distributed, local_rank)\n",
        "        self.criterion = SegDetectorLossBuilder(\n",
        "            args['loss_class'], *args.get('loss_args', []), **args.get('loss_kwargs', {})).build()\n",
        "        self.criterion = parallelize(self.criterion, distributed, local_rank)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def model_name(args):\n",
        "        return os.path.join('seg_detector', args['backbone'], args['loss_class'])\n",
        "\n",
        "    def forward(self, batch, training=True):\n",
        "        if isinstance(batch, dict):\n",
        "            data = batch['image'].to(self.device)\n",
        "        else:\n",
        "            data = batch.to(self.device)\n",
        "        data = data.float()\n",
        "        pred = self.model(data, training=self.training)\n",
        "\n",
        "        if self.training:\n",
        "            for key, value in batch.items():\n",
        "                if value is not None:\n",
        "                    if hasattr(value, 'to'):\n",
        "                        batch[key] = value.to(self.device)\n",
        "            loss_with_metrics = self.criterion(pred, batch)\n",
        "            loss, metrics = loss_with_metrics\n",
        "            return loss, pred, metrics\n",
        "        return pred\n",
        "\n",
        "\n",
        "# DB/experiment.py\n",
        "#from concern.config import Configurable, State\n",
        "# from concern.log import Logger\n",
        "# from structure.builder import Builder\n",
        "# from structure.representers import *\n",
        "# from structure.measurers import *\n",
        "# from structure.visualizers import *\n",
        "# from data.data_loader import *\n",
        "# from data import *\n",
        "# from training.model_saver import ModelSaver\n",
        "# from training.checkpoint import Checkpoint\n",
        "# from training.optimizer_scheduler import OptimizerScheduler\n",
        "\n",
        "\n",
        "class Structure(Configurable):\n",
        "    builder = State()\n",
        "    representer = State()\n",
        "    measurer = State()\n",
        "    visualizer = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def model_name(self):\n",
        "        return self.builder.model_name\n",
        "\n",
        "\n",
        "class TrainSettings(Configurable):\n",
        "    data_loader = State()\n",
        "    model_saver = State()\n",
        "    checkpoint = State()\n",
        "    scheduler = State()\n",
        "    epochs = State(default=10)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        kwargs['cmd'].update(is_train=True)\n",
        "        self.load_all(**kwargs)\n",
        "        if 'epochs' in kwargs['cmd']:\n",
        "            self.epochs = kwargs['cmd']['epochs']\n",
        "\n",
        "\n",
        "class ValidationSettings(Configurable):\n",
        "    data_loaders = State()\n",
        "    visualize = State()\n",
        "    interval = State(default=100)\n",
        "    exempt = State(default=-1)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        kwargs['cmd'].update(is_train=False)\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "        cmd = kwargs['cmd']\n",
        "        self.visualize = cmd['visualize']\n",
        "\n",
        "\n",
        "class EvaluationSettings(Configurable):\n",
        "    data_loaders = State()\n",
        "    visualize = State(default=True)\n",
        "    resume = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "\n",
        "class EvaluationSettings2(Configurable):\n",
        "    structure = State()\n",
        "    data_loaders = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "\n",
        "class ShowSettings(Configurable):\n",
        "    data_loader = State()\n",
        "    representer = State()\n",
        "    visualizer = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "\n",
        "class Experiment(Configurable):\n",
        "    structure = State(autoload=False)\n",
        "    train = State()\n",
        "    validation = State(autoload=False)\n",
        "    evaluation = State(autoload=False)\n",
        "    logger = State(autoload=True)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load('structure', **kwargs)\n",
        "\n",
        "        cmd = kwargs.get('cmd', {})\n",
        "        if 'name' not in cmd:\n",
        "            cmd['name'] = self.structure.model_name\n",
        "\n",
        "        self.load_all(**kwargs)\n",
        "        self.distributed = cmd.get('distributed', False)\n",
        "        self.local_rank = cmd.get('local_rank', 0)\n",
        "\n",
        "        if cmd.get('validate', False):\n",
        "            self.load('validation', **kwargs)\n",
        "        else:\n",
        "            self.validation = None\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from experiment import Experiment\n",
        "# from data.data_loader import DistributedSampler\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, experiment: Experiment):\n",
        "        self.init_device()\n",
        "\n",
        "        self.experiment = experiment\n",
        "        self.structure = experiment.structure\n",
        "        self.logger = experiment.logger\n",
        "        self.model_saver = experiment.train.model_saver\n",
        "\n",
        "        # FIXME: Hack the save model path into logger path\n",
        "        self.model_saver.dir_path = self.logger.save_dir(\n",
        "            self.model_saver.dir_path)\n",
        "        self.current_lr = 0\n",
        "\n",
        "        self.total = 0\n",
        "\n",
        "    def init_device(self):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "    def init_model(self):\n",
        "        model = self.structure.builder.build(\n",
        "            self.device, self.experiment.distributed, self.experiment.local_rank)\n",
        "        return model\n",
        "\n",
        "    def update_learning_rate(self, optimizer, epoch, step):\n",
        "        lr = self.experiment.train.scheduler.learning_rate.get_learning_rate(\n",
        "            epoch, step)\n",
        "\n",
        "        for group in optimizer.param_groups:\n",
        "            group['lr'] = lr\n",
        "        self.current_lr = lr\n",
        "\n",
        "    def train(self):\n",
        "        self.logger.report_time('Start')\n",
        "        self.logger.args(self.experiment)\n",
        "        model = self.init_model()\n",
        "        train_data_loader = self.experiment.train.data_loader\n",
        "        if self.experiment.validation:\n",
        "            validation_loaders = self.experiment.validation.data_loaders\n",
        "\n",
        "        self.steps = 0\n",
        "        if self.experiment.train.checkpoint:\n",
        "            self.experiment.train.checkpoint.restore_model(\n",
        "                model, self.device, self.logger)\n",
        "            epoch, iter_delta = self.experiment.train.checkpoint.restore_counter()\n",
        "            self.steps = epoch * self.total + iter_delta\n",
        "\n",
        "        # Init start epoch and iter\n",
        "        optimizer = self.experiment.train.scheduler.create_optimizer(\n",
        "            model.parameters())\n",
        "\n",
        "        self.logger.report_time('Init')\n",
        "\n",
        "        model.train()\n",
        "        while True:\n",
        "            self.logger.info('Training epoch ' + str(epoch))\n",
        "            self.logger.epoch(epoch)\n",
        "            self.total = len(train_data_loader)\n",
        "\n",
        "            for batch in train_data_loader:\n",
        "                self.update_learning_rate(optimizer, epoch, self.steps)\n",
        "\n",
        "                self.logger.report_time(\"Data loading\")\n",
        "\n",
        "                if self.experiment.validation and\\\n",
        "                        self.steps % self.experiment.validation.interval == 0 and\\\n",
        "                        self.steps > self.experiment.validation.exempt:\n",
        "                    self.validate(validation_loaders, model, epoch, self.steps)\n",
        "                self.logger.report_time('Validating ')\n",
        "                if self.logger.verbose:\n",
        "                    torch.cuda.synchronize()\n",
        "\n",
        "                self.train_step(model, optimizer, batch,\n",
        "                                epoch=epoch, step=self.steps)\n",
        "                if self.logger.verbose:\n",
        "                    torch.cuda.synchronize()\n",
        "                self.logger.report_time('Forwarding ')\n",
        "\n",
        "                self.model_saver.maybe_save_model(\n",
        "                    model, epoch, self.steps, self.logger)\n",
        "\n",
        "                self.steps += 1\n",
        "                self.logger.report_eta(self.steps, self.total, epoch)\n",
        "\n",
        "            epoch += 1\n",
        "            if epoch > self.experiment.train.epochs:\n",
        "                self.model_saver.save_checkpoint(model, 'final')\n",
        "                if self.experiment.validation:\n",
        "                    self.validate(validation_loaders, model, epoch, self.steps)\n",
        "                self.logger.info('Training done')\n",
        "                break\n",
        "            iter_delta = 0\n",
        "\n",
        "    def train_step(self, model, optimizer, batch, epoch, step, **kwards):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        results = model.forward(batch, training=True)\n",
        "        if len(results) == 2:\n",
        "            l, pred = results\n",
        "            metrics = {}\n",
        "        elif len(results) == 3:\n",
        "            l, pred, metrics = results\n",
        "\n",
        "        if isinstance(l, dict):\n",
        "            line = []\n",
        "            loss = torch.tensor(0.).cuda()\n",
        "            for key, l_val in l.items():\n",
        "                loss += l_val.mean()\n",
        "                line.append('loss_{0}:{1:.4f}'.format(key, l_val.mean()))\n",
        "        else:\n",
        "            loss = l.mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % self.experiment.logger.log_interval == 0:\n",
        "            if isinstance(l, dict):\n",
        "                line = '\\t'.join(line)\n",
        "                log_info = '\\t'.join(['step:{:6d}', 'epoch:{:3d}', '{}', 'lr:{:.4f}']).format(step, epoch, line, self.current_lr)\n",
        "                self.logger.info(log_info)\n",
        "            else:\n",
        "                self.logger.info('step: %6d, epoch: %3d, loss: %.6f, lr: %f' % (\n",
        "                    step, epoch, loss.item(), self.current_lr))\n",
        "            self.logger.add_scalar('loss', loss, step)\n",
        "            self.logger.add_scalar('learning_rate', self.current_lr, step)\n",
        "            for name, metric in metrics.items():\n",
        "                self.logger.add_scalar(name, metric.mean(), step)\n",
        "                self.logger.info('%s: %6f' % (name, metric.mean()))\n",
        "\n",
        "            self.logger.report_time('Logging')\n",
        "\n",
        "    def validate(self, validation_loaders, model, epoch, step):\n",
        "        all_matircs = {}\n",
        "        model.eval()\n",
        "        for name, loader in validation_loaders.items():\n",
        "            if self.experiment.validation.visualize:\n",
        "                metrics, vis_images = self.validate_step(\n",
        "                    loader, model, True)\n",
        "                self.logger.images(\n",
        "                    os.path.join('vis', name), vis_images, step)\n",
        "            else:\n",
        "                metrics, vis_images = self.validate_step(loader, model, False)\n",
        "            for _key, metric in metrics.items():\n",
        "                key = name + '/' + _key\n",
        "                if key in all_matircs:\n",
        "                    all_matircs[key].update(metric.val, metric.count)\n",
        "                else:\n",
        "                    all_matircs[key] = metric\n",
        "\n",
        "        for key, metric in all_matircs.items():\n",
        "            self.logger.info('%s : %f (%d)' % (key, metric.avg, metric.count))\n",
        "        self.logger.metrics(epoch, self.steps, all_matircs)\n",
        "        model.train()\n",
        "        return all_matircs\n",
        "\n",
        "    def validate_step(self, data_loader, model, visualize=False):\n",
        "        raw_metrics = []\n",
        "        vis_images = dict()\n",
        "        for i, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            pred = model.forward(batch, training=False)\n",
        "            output = self.structure.representer.represent(batch, pred)\n",
        "            raw_metric, interested = self.structure.measurer.validate_measure(\n",
        "                batch, output)\n",
        "            raw_metrics.append(raw_metric)\n",
        "\n",
        "            if visualize and self.structure.visualizer:\n",
        "                vis_image = self.structure.visualizer.visualize(\n",
        "                    batch, output, interested)\n",
        "                vis_images.update(vis_image)\n",
        "        metrics = self.structure.measurer.gather_measure(\n",
        "            raw_metrics, self.logger)\n",
        "        return metrics, vis_images\n",
        "\n",
        "    def to_np(self, x):\n",
        "        return x.cpu().data.numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #!python3\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "# # from trainer import Trainer\n",
        "# # # tagged yaml objects\n",
        "# # from experiment import Structure, TrainSettings, ValidationSettings, Experiment\n",
        "# # from concern.log import Logger\n",
        "# # from data.data_loader import DataLoader\n",
        "# # from data.image_dataset import ImageDataset\n",
        "# # from training.checkpoint import Checkpoint\n",
        "# # from training.model_saver import ModelSaver\n",
        "# # from training.optimizer_scheduler import OptimizerScheduler\n",
        "# # from concern.config import Configurable, Config\n",
        "\n",
        "import sys\n",
        "\n",
        "def main():\n",
        "    # \"\"\"\n",
        "    # CUDA_VISIBLE_DEVICES=0 python train.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --num_gpus 1\n",
        "    # \"\"\"\n",
        "    # #sys.argv.append( 'experiments/seg_detector/td500_resnet18_deform_thre.yaml' )\n",
        "    # # sys.argv.append( '/content/DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml' )\n",
        "\n",
        "    # sys.argv.append( '--num_gpus' )\n",
        "    # sys.argv.append( '1' )\n",
        "\n",
        "    # parser = argparse.ArgumentParser(description='Text Recognition Training')\n",
        "    # parser.add_argument('exp', default='/content/DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml', type=str)\n",
        "    # parser.add_argument('--name', type=str)\n",
        "    # parser.add_argument('--batch_size', type=int, help='Batch size for training')\n",
        "    # parser.add_argument('--resume', type=str, help='Resume from checkpoint')\n",
        "    # parser.add_argument('--epochs', type=int, help='Number of training epochs')\n",
        "    # parser.add_argument('--num_workers', type=int, help='Number of dataloader workers')\n",
        "    # parser.add_argument('--start_iter', type=int, help='Begin counting iterations starting from this value (should be used with resume)')\n",
        "    # parser.add_argument('--start_epoch', type=int, help='Begin counting epoch starting from this value (should be used with resume)')\n",
        "    # parser.add_argument('--max_size', type=int, help='max length of label')\n",
        "    # parser.add_argument('--lr', type=float, help='initial learning rate')\n",
        "    # parser.add_argument('--optimizer', type=str, help='The optimizer want to use')\n",
        "    # parser.add_argument('--thresh', type=float, help='The threshold to replace it in the representers')\n",
        "    # parser.add_argument('--verbose', action='store_true', help='show verbose info')\n",
        "    # parser.add_argument('--visualize', action='store_true', help='visualize maps in tensorboard')\n",
        "    # parser.add_argument('--force_reload', action='store_true', dest='force_reload', help='Force reload data meta')\n",
        "    # parser.add_argument('--no-force_reload', action='store_false', dest='force_reload', help='Force reload data meta')\n",
        "    # parser.add_argument('--validate', action='store_true', dest='validate', help='Validate during training')\n",
        "    # parser.add_argument('--no-validate', action='store_false', dest='validate', help='Validate during training')\n",
        "    # parser.add_argument('--print-config-only', action='store_true', help='print config without actual training')\n",
        "    # parser.add_argument('--debug', action='store_true', dest='debug', help='Run with debug mode, which hacks dataset num_samples to toy number')\n",
        "    # parser.add_argument('--no-debug', action='store_false', dest='debug', help='Run without debug mode')\n",
        "    # parser.add_argument('--benchmark', action='store_true', dest='benchmark', help='Open cudnn benchmark mode')\n",
        "    # parser.add_argument('--no-benchmark', action='store_false', dest='benchmark', help='Turn cudnn benchmark mode off')\n",
        "    # parser.add_argument('-d', '--distributed', action='store_true', dest='distributed', help='Use distributed training')\n",
        "    # parser.add_argument('--local_rank', dest='local_rank', default=0, type=int, help='Use distributed training')\n",
        "    # parser.add_argument('-g', '--num_gpus', dest='num_gpus', default=4, type=int, help='The number of accessible gpus')\n",
        "    # parser.set_defaults(debug=False)\n",
        "    # parser.set_defaults(benchmark=True)\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    # args = vars(args)\n",
        "    # args = {k: v for k, v in args.items() if v is not None}\n",
        "\n",
        "#     if args['distributed']:\n",
        "#         torch.cuda.set_device(args['local_rank'])\n",
        "#         torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
        "\n",
        "    args = {\n",
        "        'exp': '/content/DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml',\n",
        "        'verbose': False,\n",
        "        'visualize': False,\n",
        "        'force_reload': False,\n",
        "        'validate': False,\n",
        "        'print_config_only': False,\n",
        "        'debug': False,\n",
        "        'benchmark': True,\n",
        "        'distributed': False,\n",
        "        'local_rank': 0,\n",
        "        'num_gpus': 1,\n",
        "    }\n",
        "\n",
        "    conf = Config()\n",
        "    experiment_args = conf.compile(conf.load(args['exp']))['Experiment']\n",
        "    experiment_args.update(cmd=args)\n",
        "    experiment = Configurable.construct_class_from_config(experiment_args)\n",
        "\n",
        "    if not args['print_config_only']:\n",
        "        torch.backends.cudnn.benchmark = args['benchmark']\n",
        "        trainer = Trainer(experiment)\n",
        "        trainer.train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# CUDA_VISIBLE_DEVICES=0 python train.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --num_gpus 1\n",
        "\n",
        "# \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "vbb86WL98JTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test OpenCV"
      ],
      "metadata": {
        "id": "ZfTr8HuWSUp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "显示人工标记的区域\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    im = '/content/DB/datasets/TD_TR/TD500/train_images/IMG_0855.JPG'\n",
        "    gt = '/content/DB/datasets/TD_TR/TD500/train_gts/IMG_0855.JPG.txt'\n",
        "    \n",
        "    items = []\n",
        "    reader = open(gt, 'r').readlines()\n",
        "    for line in reader:\n",
        "        item = {}\n",
        "        parts = line.strip().split(',')\n",
        "        label = parts[-1]\n",
        "        if 'TD' in gt and label == '1':\n",
        "            label = '###'\n",
        "        line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in parts]\n",
        "        if 'icdar' in gt:\n",
        "            poly = np.array(list(map(float, line[:8]))).reshape(\n",
        "                (-1, 2)).tolist()\n",
        "        else:\n",
        "            num_points = math.floor((len(line) - 1) / 2) * 2\n",
        "            poly = np.array(list(map(float, line[:num_points]))).reshape(\n",
        "                (-1, 2)).tolist()\n",
        "        item['poly'] = poly\n",
        "        item['text'] = label\n",
        "        items.append( item )\n",
        "\n",
        "    img = cv2.imdecode(np.fromfile(im, dtype=np.uint8), -1)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    for i in range( len(items) ):\n",
        "        poly = items[i]['poly']\n",
        "        poly = np.array(poly)\n",
        "        poly = poly.astype(np.int32)\n",
        "\n",
        "        #cv2.fillPoly(img, pts=[ poly ], color=(0, 0, 255))  # 就是画线，从起点连到第二个点 ... 最后一个点连到第一个点\n",
        "        cv2.polylines(img, [ poly ], isClosed = True, color = (0, 0, 255), thickness = 1) # 只画线，不填充\n",
        "\n",
        "    #cv2.imwrite(\"poly.jpg\", img)\n",
        "\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey()"
      ],
      "metadata": {
        "id": "LZ7LdQ5eSYq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "QqTzM8JVQ2ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DB/concern/config.py\n",
        "import importlib\n",
        "from collections import OrderedDict\n",
        "\n",
        "import anyconfig\n",
        "import munch\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def load(self, conf):\n",
        "        import pdb; pdb.set_trace()\n",
        "        conf = anyconfig.load(conf)\n",
        "        return munch.munchify(conf)\n",
        "\n",
        "    def compile(self, conf, return_packages=False):\n",
        "        packages = conf.get('package', [])\n",
        "        defines = {}\n",
        "\n",
        "        for path in conf.get('import', []):\n",
        "            parent_conf = self.load(path)\n",
        "            parent_packages, parent_defines = self.compile(\n",
        "                parent_conf, return_packages=True)\n",
        "            packages.extend(parent_packages)\n",
        "            defines.update(parent_defines)\n",
        "\n",
        "        modules = []\n",
        "        for package in packages:\n",
        "            module = importlib.import_module(package)\n",
        "            modules.append(module)\n",
        "\n",
        "        if isinstance(conf['define'], dict):\n",
        "            conf['define'] = [conf['define']]\n",
        "\n",
        "        for define in conf['define']:\n",
        "            name = define.copy().pop('name')\n",
        "\n",
        "            if not isinstance(name, str):\n",
        "                raise RuntimeError('name must be str')\n",
        "\n",
        "            defines[name] = self.compile_conf(define, defines, modules)\n",
        "\n",
        "        if return_packages:\n",
        "            return packages, defines\n",
        "        else:\n",
        "            return defines\n",
        "\n",
        "    def compile_conf(self, conf, defines, modules):\n",
        "        if isinstance(conf, (int, float)):\n",
        "            return conf\n",
        "        elif isinstance(conf, str):\n",
        "            if conf.startswith('^'):\n",
        "                return defines[conf[1:]]\n",
        "            if conf.startswith('$'):\n",
        "                return {'class': self.find_class_in_modules(conf[1:], modules)}\n",
        "            return conf\n",
        "        elif isinstance(conf, dict):\n",
        "            if 'class' in conf:\n",
        "                conf['class'] = self.find_class_in_modules(\n",
        "                    conf['class'], modules)\n",
        "            if 'base' in conf:\n",
        "                base = conf.copy().pop('base')\n",
        "\n",
        "                if not isinstance(base, str):\n",
        "                    raise RuntimeError('base must be str')\n",
        "\n",
        "                conf = {\n",
        "                    **defines[base],\n",
        "                    **conf,\n",
        "                }\n",
        "            return {key: self.compile_conf(value, defines, modules) for key, value in conf.items()}\n",
        "        elif isinstance(conf, (list, tuple)):\n",
        "            return [self.compile_conf(value, defines, modules) for value in conf]\n",
        "        else:\n",
        "            return conf\n",
        "\n",
        "    def find_class_in_modules(self, cls, modules):\n",
        "        if not isinstance(cls, str):\n",
        "            raise RuntimeError('class name must be str')\n",
        "\n",
        "        if cls.find('.') != -1:\n",
        "            package, cls = cls.rsplit('.', 1)\n",
        "            module = importlib.import_module(package)\n",
        "            if hasattr(module, cls):\n",
        "                return module.__name__ + '.' + cls\n",
        "\n",
        "        for module in modules:\n",
        "            if hasattr(module, cls):\n",
        "                return module.__name__ + '.' + cls\n",
        "        raise RuntimeError('class not found ' + cls)\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self, autoload=True, default=None):\n",
        "        self.autoload = autoload\n",
        "        self.default = default\n",
        "\n",
        "\n",
        "class StateMeta(type):\n",
        "    def __new__(mcs, name, bases, attrs):\n",
        "        current_states = []\n",
        "        for key, value in attrs.items():\n",
        "            if isinstance(value, State):\n",
        "                current_states.append((key, value))\n",
        "\n",
        "        current_states.sort(key=lambda x: x[0])\n",
        "        attrs['states'] = OrderedDict(current_states)\n",
        "        new_class = super(StateMeta, mcs).__new__(mcs, name, bases, attrs)\n",
        "\n",
        "        # Walk through the MRO\n",
        "        states = OrderedDict()\n",
        "        for base in reversed(new_class.__mro__):\n",
        "            if hasattr(base, 'states'):\n",
        "                states.update(base.states)\n",
        "        new_class.states = states\n",
        "\n",
        "        for key, value in states.items():\n",
        "            setattr(new_class, key, value.default)\n",
        "\n",
        "        return new_class\n",
        "\n",
        "\n",
        "class Configurable(metaclass=StateMeta):\n",
        "    def __init__(self, *args, cmd={}, **kwargs):\n",
        "        self.load_all(cmd=cmd, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def construct_class_from_config(args):\n",
        "        cls = Configurable.extract_class_from_args(args)\n",
        "        return cls(**args)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_class_from_args(args):\n",
        "        cls = args.copy().pop('class')\n",
        "        package, cls = cls.rsplit('.', 1)\n",
        "        module = importlib.import_module(package)\n",
        "        cls = getattr(module, cls)\n",
        "        return cls\n",
        "\n",
        "    def load_all(self, **kwargs):\n",
        "        for name, state in self.states.items():\n",
        "            if state.autoload:\n",
        "                self.load(name, **kwargs)\n",
        "\n",
        "    def load(self, state_name, **kwargs):\n",
        "        # FIXME: kwargs should be filtered\n",
        "        # Args passed from command line\n",
        "        cmd = kwargs.pop('cmd', dict())\n",
        "        if state_name in kwargs:\n",
        "            setattr(self, state_name, self.create_member_from_config(\n",
        "                (kwargs[state_name], cmd)))\n",
        "        else:\n",
        "            setattr(self, state_name, self.states[state_name].default)\n",
        "\n",
        "    def create_member_from_config(self, conf):\n",
        "        args, cmd = conf\n",
        "        if args is None or isinstance(args, (int, float, str)):\n",
        "            return args\n",
        "        elif isinstance(args, (list, tuple)):\n",
        "            return [self.create_member_from_config((subargs, cmd)) for subargs in args]\n",
        "        elif isinstance(args, dict):\n",
        "            if 'class' in args:\n",
        "                cls = self.extract_class_from_args(args)\n",
        "                return cls(**args, cmd=cmd)\n",
        "            return {key: self.create_member_from_config((subargs, cmd)) for key, subargs in args.items()}\n",
        "        else:\n",
        "            return args\n",
        "\n",
        "    def dump(self):\n",
        "        state = {}\n",
        "        state['class'] = self.__class__.__module__ + \\\n",
        "            '.' + self.__class__.__name__\n",
        "        for name, value in self.states.items():\n",
        "            obj = getattr(self, name)\n",
        "            state[name] = self.dump_obj(obj)\n",
        "        return state\n",
        "\n",
        "    def dump_obj(self, obj):\n",
        "        if obj is None:\n",
        "            return None\n",
        "        elif hasattr(obj, 'dump'):\n",
        "            return obj.dump()\n",
        "        elif isinstance(obj, (int, float, str)):\n",
        "            return obj\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return [self.dump_obj(value) for value in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: self.dump_obj(value) for key, value in obj.items()}\n",
        "        else:\n",
        "            return str(obj)\n",
        "\n",
        "\n",
        "# DB/concern/log.py\n",
        "import os\n",
        "import logging\n",
        "import functools\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# from concern.config import Configurable, State\n",
        "\n",
        "\n",
        "class Logger(Configurable):\n",
        "    SUMMARY_DIR_NAME = 'summaries'\n",
        "    VISUALIZE_NAME = 'visualize'\n",
        "    LOG_FILE_NAME = 'output.log'\n",
        "    ARGS_FILE_NAME = 'args.log'\n",
        "    METRICS_FILE_NAME = 'metrics.log'\n",
        "\n",
        "    database_dir = State(default='./outputs/')\n",
        "    log_dir = State(default='workspace')\n",
        "    verbose = State(default=False)\n",
        "    level = State(default='info')\n",
        "    log_interval = State(default=100)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "        self._make_storage()\n",
        "\n",
        "        cmd = kwargs['cmd']\n",
        "        self.name = cmd['name']\n",
        "        self.log_dir = os.path.join(self.log_dir, self.name)\n",
        "        try:\n",
        "            self.verbose = cmd['verbose']\n",
        "        except:\n",
        "            print('verbose:', self.verbose)\n",
        "        if self.verbose:\n",
        "            print('Initializing log dir for', self.log_dir)\n",
        "\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "\n",
        "        self.message_logger = self._init_message_logger()\n",
        "\n",
        "        summary_path = os.path.join(self.log_dir, self.SUMMARY_DIR_NAME)\n",
        "        self.tf_board_logger = SummaryWriter(summary_path)\n",
        "\n",
        "        self.metrics_writer = open(os.path.join(\n",
        "            self.log_dir, self.METRICS_FILE_NAME), 'at')\n",
        "\n",
        "        self.timestamp = time.time()\n",
        "        self.logged = -1\n",
        "        self.speed = None\n",
        "        self.eta_time = None\n",
        "\n",
        "    def _make_storage(self):\n",
        "        application = os.path.basename(os.getcwd())\n",
        "        storage_dir = os.path.join(\n",
        "            self.database_dir, self.log_dir, application)\n",
        "        if not os.path.exists(storage_dir):\n",
        "            os.makedirs(storage_dir)\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.symlink(storage_dir, self.log_dir)\n",
        "\n",
        "    def save_dir(self, dir_name):\n",
        "        return os.path.join(self.log_dir, dir_name)\n",
        "\n",
        "    def _init_message_logger(self):\n",
        "        message_logger = logging.getLogger('messages')\n",
        "        message_logger.setLevel(\n",
        "            logging.DEBUG if self.verbose else logging.INFO)\n",
        "        formatter = logging.Formatter(\n",
        "            '[%(levelname)s] [%(asctime)s] %(message)s')\n",
        "        std_handler = logging.StreamHandler()\n",
        "        std_handler.setLevel(message_logger.level)\n",
        "        std_handler.setFormatter(formatter)\n",
        "\n",
        "        file_handler = logging.FileHandler(\n",
        "            os.path.join(self.log_dir, self.LOG_FILE_NAME))\n",
        "        file_handler.setLevel(message_logger.level)\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        message_logger.addHandler(std_handler)\n",
        "        message_logger.addHandler(file_handler)\n",
        "        return message_logger\n",
        "\n",
        "    def report_time(self, name: str):\n",
        "        if self.verbose:\n",
        "            self.info(name + \" time :\" + str(time.time() - self.timestamp))\n",
        "            self.timestamp = time.time()\n",
        "\n",
        "    def report_eta(self, steps, total, epoch):\n",
        "        self.logged = self.logged % total + 1\n",
        "        steps = steps % total\n",
        "        if self.eta_time is None:\n",
        "            self.eta_time = time.time()\n",
        "            speed = -1\n",
        "        else:\n",
        "            eta_time = time.time()\n",
        "            speed = eta_time - self.eta_time\n",
        "            if self.speed is not None:\n",
        "                speed = ((self.logged - 1) * self.speed + speed) / self.logged\n",
        "            self.speed = speed\n",
        "            self.eta_time = eta_time\n",
        "\n",
        "        seconds = (total - steps) * speed\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds - (hours * 3600)) // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        print('%d/%d batches processed in epoch %d, ETA: %2d:%2d:%2d' %\n",
        "              (steps, total, epoch,\n",
        "               hours, minutes, seconds), end='\\r')\n",
        "\n",
        "    def args(self, parameters=None):\n",
        "        if parameters is None:\n",
        "            with open(os.path.join(self.log_dir, self.ARGS_FILE_NAME), 'rt') as reader:\n",
        "                return yaml.load(reader.read())\n",
        "        with open(os.path.join(self.log_dir, self.ARGS_FILE_NAME), 'wt') as writer:\n",
        "            yaml.dump(parameters.dump(), writer)\n",
        "\n",
        "    def metrics(self, epoch, steps, metrics_dict):\n",
        "        results = {}\n",
        "        for name, a in metrics_dict.items():\n",
        "            results[name] = {'count': a.count, 'value': float(a.avg)}\n",
        "            self.add_scalar('metrics/' + name, a.avg, steps)\n",
        "        result_dict = {\n",
        "            str(datetime.now()): {\n",
        "                'epoch': epoch,\n",
        "                'steps': steps,\n",
        "                **results\n",
        "            }\n",
        "        }\n",
        "        string_result = yaml.dump(result_dict)\n",
        "        self.info(string_result)\n",
        "        self.metrics_writer.write(string_result)\n",
        "        self.metrics_writer.flush()\n",
        "\n",
        "    def named_number(self, name, num=None, default=0):\n",
        "        if num is None:\n",
        "            return int(self.has_signal(name)) or default\n",
        "        else:\n",
        "            with open(os.path.join(self.log_dir, name), 'w') as writer:\n",
        "                writer.write(str(num))\n",
        "            return num\n",
        "\n",
        "    epoch = functools.partialmethod(named_number, 'epoch')\n",
        "    iter = functools.partialmethod(named_number, 'iter')\n",
        "\n",
        "    def message(self, level, content):\n",
        "        self.message_logger.__getattribute__(level)(content)\n",
        "\n",
        "    def images(self, prefix, image_dict, step):\n",
        "        for name, image in image_dict.items():\n",
        "            self.add_image(prefix + '/' + name, image, step, dataformats='HWC')\n",
        "\n",
        "    def merge_save_images(self, name, images):\n",
        "        for i, image in enumerate(images):\n",
        "            if i == 0:\n",
        "                result = image\n",
        "            else:\n",
        "                result = np.concatenate([result, image], 0)\n",
        "        cv2.imwrite(os.path.join(self.vis_dir(), name+'.jpg'), result)\n",
        "\n",
        "    def vis_dir(self):\n",
        "        vis_dir = os.path.join(self.log_dir, self.VISUALIZE_NAME)\n",
        "        if not os.path.exists(vis_dir):\n",
        "            os.mkdir(vis_dir)\n",
        "        return vis_dir\n",
        "\n",
        "    def save_image_dict(self, images, max_size=1024):\n",
        "        for file_name, image in images.items():\n",
        "            height, width = image.shape[:2]\n",
        "            if height > width:\n",
        "                actual_height = min(height, max_size)\n",
        "                actual_width = int(round(actual_height * width / height))\n",
        "            else:\n",
        "                actual_width = min(width, max_size)\n",
        "                actual_height = int(round(actual_width * height / width))\n",
        "                image = cv2.resize(image, (actual_width, actual_height))\n",
        "            cv2.imwrite(os.path.join(self.vis_dir(), file_name+'.jpg'), image)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        message_levels = set(['debug', 'info', 'warning', 'error', 'critical'])\n",
        "        if name == '__setstate__':\n",
        "            raise AttributeError('haha')\n",
        "        if name in message_levels:\n",
        "            return functools.partial(self.message, name)\n",
        "        elif hasattr(self.__dict__.get('tf_board_logger'), name):\n",
        "            return self.tf_board_logger.__getattribute__(name)\n",
        "        else:\n",
        "            super()\n",
        "\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def constant_init(module, constant, bias=0):\n",
        "    nn.init.constant_(module.weight, constant)\n",
        "    if hasattr(module, 'bias'):\n",
        "        nn.init.constant_(module.bias, bias)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.with_dcn = dcn is not None\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.with_modulated_dcn = False\n",
        "        if self.with_dcn:\n",
        "            fallback_on_stride = dcn.get('fallback_on_stride', False)\n",
        "            self.with_modulated_dcn = dcn.get('modulated', False)\n",
        "        # self.conv2 = conv3x3(planes, planes)\n",
        "        if not self.with_dcn or fallback_on_stride:\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                   padding=1, bias=False)\n",
        "        else:\n",
        "            deformable_groups = dcn.get('deformable_groups', 1)\n",
        "            if not self.with_modulated_dcn:\n",
        "                # from assets.ops.dcn import DeformConv\n",
        "                from dcn import DeformConv\n",
        "                conv_op = DeformConv\n",
        "                offset_channels = 18\n",
        "            else:\n",
        "                # from assets.ops.dcn import ModulatedDeformConv\n",
        "                from dcn import ModulatedDeformConv\n",
        "                conv_op = ModulatedDeformConv\n",
        "                offset_channels = 27\n",
        "            self.conv2_offset = nn.Conv2d(\n",
        "                planes,\n",
        "                deformable_groups * offset_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1)\n",
        "            self.conv2 = conv_op(\n",
        "                planes,\n",
        "                planes,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                deformable_groups=deformable_groups,\n",
        "                bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # out = self.conv2(out)\n",
        "        if not self.with_dcn:\n",
        "            out = self.conv2(out)\n",
        "        elif self.with_modulated_dcn:\n",
        "            offset_mask = self.conv2_offset(out)\n",
        "            offset = offset_mask[:, :18, :, :]\n",
        "            mask = offset_mask[:, -9:, :, :].sigmoid()\n",
        "            out = self.conv2(out, offset, mask)\n",
        "        else:\n",
        "            offset = self.conv2_offset(out)\n",
        "            out = self.conv2(out, offset)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.with_dcn = dcn is not None\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes)\n",
        "        fallback_on_stride = False\n",
        "        self.with_modulated_dcn = False\n",
        "        if self.with_dcn:\n",
        "            fallback_on_stride = dcn.get('fallback_on_stride', False)\n",
        "            self.with_modulated_dcn = dcn.get('modulated', False)\n",
        "        if not self.with_dcn or fallback_on_stride:\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                   stride=stride, padding=1, bias=False)\n",
        "        else:\n",
        "            deformable_groups = dcn.get('deformable_groups', 1)\n",
        "            if not self.with_modulated_dcn:\n",
        "                from assets.ops.dcn import DeformConv\n",
        "                conv_op = DeformConv\n",
        "                offset_channels = 18\n",
        "            else:\n",
        "                from assets.ops.dcn import ModulatedDeformConv\n",
        "                conv_op = ModulatedDeformConv\n",
        "                offset_channels = 27\n",
        "            self.conv2_offset = nn.Conv2d(\n",
        "                planes, deformable_groups * offset_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1)\n",
        "            self.conv2 = conv_op(\n",
        "                planes, planes, kernel_size=3, padding=1, stride=stride,\n",
        "                deformable_groups=deformable_groups, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.dcn = dcn\n",
        "        self.with_dcn = dcn is not None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # out = self.conv2(out)\n",
        "        if not self.with_dcn:\n",
        "            out = self.conv2(out)\n",
        "        elif self.with_modulated_dcn:\n",
        "            offset_mask = self.conv2_offset(out)\n",
        "            offset = offset_mask[:, :18, :, :]\n",
        "            mask = offset_mask[:, -9:, :, :].sigmoid()\n",
        "            out = self.conv2(out, offset, mask)\n",
        "        else:\n",
        "            offset = self.conv2_offset(out)\n",
        "            out = self.conv2(out, offset)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000, \n",
        "                 dcn=None, stage_with_dcn=(False, False, False, False)):\n",
        "        self.dcn = dcn\n",
        "        self.stage_with_dcn = stage_with_dcn\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], stride=2, dcn=dcn)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], stride=2, dcn=dcn)\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], stride=2, dcn=dcn)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "    \n",
        "        self.smooth = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=1)    \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        if self.dcn is not None:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):\n",
        "                    if hasattr(m, 'conv2_offset'):\n",
        "                        constant_init(m.conv2_offset, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dcn=None):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes,\n",
        "                            stride, downsample, dcn=dcn))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dcn=dcn))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x2 = self.layer1(x)\n",
        "        x3 = self.layer2(x2)\n",
        "        x4 = self.layer3(x3)\n",
        "        x5 = self.layer4(x4)\n",
        "\n",
        "        return x2, x3, x4, x5\n",
        "\n",
        "\n",
        "def resnet18(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet18']), strict=False)\n",
        "    return model\n",
        "\n",
        "def deformable_resnet18(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2],\n",
        "                    dcn=dict(modulated=True,\n",
        "                            deformable_groups=1,\n",
        "                            fallback_on_stride=False),\n",
        "                    stage_with_dcn=[False, True, True, True], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet18']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet34']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet50']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deformable_resnet50(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model with deformable conv.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   dcn=dict(modulated=True,\n",
        "                            deformable_groups=1,\n",
        "                            fallback_on_stride=False),\n",
        "                   stage_with_dcn=[False, True, True, True],\n",
        "                   **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet50']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet101']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(\n",
        "            model_urls['resnet152']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "# DB/structure/model.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#import backbones\n",
        "#import decoders\n",
        "\n",
        "\n",
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        #self.backbone = getattr(backbones, args['backbone'])(**args.get('backbone_args', {}))\n",
        "        backboneName = 'deformable_resnet18' # args['backbone']\n",
        "        backboneFunc = deformable_resnet18 #getattr(backbones, backboneName)\n",
        "        backboneInstance = backboneFunc(**args.get('backbone_args', {}))\n",
        "        self.backbone = backboneInstance\n",
        "\n",
        "        self.decoder = getattr(decoders, args['decoder'])(**args.get('decoder_args', {}))\n",
        "\n",
        "    def forward(self, data, *args, **kwargs):\n",
        "        return self.decoder(self.backbone(data), *args, **kwargs)\n",
        "\n",
        "\n",
        "def parallelize(model, distributed, local_rank):\n",
        "    if distributed:\n",
        "        return nn.parallel.DistributedDataParallel(\n",
        "            model,\n",
        "            device_ids=[local_rank],\n",
        "            output_device=[local_rank],\n",
        "            find_unused_parameters=True)\n",
        "    else:\n",
        "        return nn.DataParallel(model)\n",
        "\n",
        "class SegDetectorModel(nn.Module):\n",
        "    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n",
        "        super(SegDetectorModel, self).__init__()\n",
        "        from decoders.seg_detector_loss import SegDetectorLossBuilder\n",
        "\n",
        "        self.model = BasicModel(args)\n",
        "        # for loading models\n",
        "        self.model = parallelize(self.model, distributed, local_rank)\n",
        "        self.criterion = SegDetectorLossBuilder(\n",
        "            args['loss_class'], *args.get('loss_args', []), **args.get('loss_kwargs', {})).build()\n",
        "        self.criterion = parallelize(self.criterion, distributed, local_rank)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def model_name(args):\n",
        "        return os.path.join('seg_detector', args['backbone'], args['loss_class'])\n",
        "\n",
        "    def forward(self, batch, training=True):\n",
        "        if isinstance(batch, dict):\n",
        "            data = batch['image'].to(self.device)\n",
        "        else:\n",
        "            data = batch.to(self.device)\n",
        "        data = data.float()\n",
        "        pred = self.model(data, training=self.training)\n",
        "\n",
        "        if self.training:\n",
        "            for key, value in batch.items():\n",
        "                if value is not None:\n",
        "                    if hasattr(value, 'to'):\n",
        "                        batch[key] = value.to(self.device)\n",
        "            loss_with_metrics = self.criterion(pred, batch)\n",
        "            loss, metrics = loss_with_metrics\n",
        "            return loss, pred, metrics\n",
        "        return pred\n",
        "\n",
        "\n",
        "# DB/experiment.py\n",
        "#from concern.config import Configurable, State\n",
        "# from concern.log import Logger\n",
        "# from structure.builder import Builder\n",
        "# from structure.representers import *\n",
        "# from structure.measurers import *\n",
        "# from structure.visualizers import *\n",
        "# from data.data_loader import *\n",
        "# from data import *\n",
        "# from training.model_saver import ModelSaver\n",
        "# from training.checkpoint import Checkpoint\n",
        "# from training.optimizer_scheduler import OptimizerScheduler\n",
        "\n",
        "\n",
        "class Structure(Configurable):\n",
        "    builder = State()\n",
        "    representer = State()\n",
        "    measurer = State()\n",
        "    visualizer = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def model_name(self):\n",
        "        return self.builder.model_name\n",
        "\n",
        "\n",
        "class TrainSettings(Configurable):\n",
        "    data_loader = State()\n",
        "    model_saver = State()\n",
        "    checkpoint = State()\n",
        "    scheduler = State()\n",
        "    epochs = State(default=10)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        kwargs['cmd'].update(is_train=True)\n",
        "        self.load_all(**kwargs)\n",
        "        if 'epochs' in kwargs['cmd']:\n",
        "            self.epochs = kwargs['cmd']['epochs']\n",
        "\n",
        "\n",
        "class ValidationSettings(Configurable):\n",
        "    data_loaders = State()\n",
        "    visualize = State()\n",
        "    interval = State(default=100)\n",
        "    exempt = State(default=-1)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        kwargs['cmd'].update(is_train=False)\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "        cmd = kwargs['cmd']\n",
        "        self.visualize = cmd['visualize']\n",
        "\n",
        "\n",
        "class EvaluationSettings(Configurable):\n",
        "    data_loaders = State()\n",
        "    visualize = State(default=True)\n",
        "    resume = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "\n",
        "class EvaluationSettings2(Configurable):\n",
        "    structure = State()\n",
        "    data_loaders = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "\n",
        "class ShowSettings(Configurable):\n",
        "    data_loader = State()\n",
        "    representer = State()\n",
        "    visualizer = State()\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load_all(**kwargs)\n",
        "\n",
        "\n",
        "class Experiment(Configurable):\n",
        "    structure = State(autoload=False)\n",
        "    train = State()\n",
        "    validation = State(autoload=False)\n",
        "    evaluation = State(autoload=False)\n",
        "    logger = State(autoload=True)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.load('structure', **kwargs)\n",
        "\n",
        "        cmd = kwargs.get('cmd', {})\n",
        "        if 'name' not in cmd:\n",
        "            cmd['name'] = self.structure.model_name\n",
        "\n",
        "        self.load_all(**kwargs)\n",
        "        self.distributed = cmd.get('distributed', False)\n",
        "        self.local_rank = cmd.get('local_rank', 0)\n",
        "\n",
        "        if cmd.get('validate', False):\n",
        "            self.load('validation', **kwargs)\n",
        "        else:\n",
        "            self.validation = None\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from experiment import Experiment\n",
        "# from data.data_loader import DistributedSampler\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, experiment: Experiment):\n",
        "        self.init_device()\n",
        "\n",
        "        self.experiment = experiment\n",
        "        self.structure = experiment.structure\n",
        "        self.logger = experiment.logger\n",
        "        self.model_saver = experiment.train.model_saver\n",
        "\n",
        "        # FIXME: Hack the save model path into logger path\n",
        "        self.model_saver.dir_path = self.logger.save_dir(\n",
        "            self.model_saver.dir_path)\n",
        "        self.current_lr = 0\n",
        "\n",
        "        self.total = 0\n",
        "\n",
        "    def init_device(self):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "    def init_model(self):\n",
        "        model = self.structure.builder.build(\n",
        "            self.device, self.experiment.distributed, self.experiment.local_rank)\n",
        "        return model\n",
        "\n",
        "    def update_learning_rate(self, optimizer, epoch, step):\n",
        "        lr = self.experiment.train.scheduler.learning_rate.get_learning_rate(\n",
        "            epoch, step)\n",
        "\n",
        "        for group in optimizer.param_groups:\n",
        "            group['lr'] = lr\n",
        "        self.current_lr = lr\n",
        "\n",
        "    def train(self):\n",
        "        self.logger.report_time('Start')\n",
        "        self.logger.args(self.experiment)\n",
        "        model = self.init_model()\n",
        "        train_data_loader = self.experiment.train.data_loader\n",
        "        if self.experiment.validation:\n",
        "            validation_loaders = self.experiment.validation.data_loaders\n",
        "\n",
        "        self.steps = 0\n",
        "        if self.experiment.train.checkpoint:\n",
        "            self.experiment.train.checkpoint.restore_model(\n",
        "                model, self.device, self.logger)\n",
        "            epoch, iter_delta = self.experiment.train.checkpoint.restore_counter()\n",
        "            self.steps = epoch * self.total + iter_delta\n",
        "\n",
        "        # Init start epoch and iter\n",
        "        optimizer = self.experiment.train.scheduler.create_optimizer(\n",
        "            model.parameters())\n",
        "\n",
        "        self.logger.report_time('Init')\n",
        "\n",
        "        model.train()\n",
        "        while True:\n",
        "            self.logger.info('Training epoch ' + str(epoch))\n",
        "            self.logger.epoch(epoch)\n",
        "            self.total = len(train_data_loader)\n",
        "\n",
        "            for batch in train_data_loader:\n",
        "                self.update_learning_rate(optimizer, epoch, self.steps)\n",
        "\n",
        "                self.logger.report_time(\"Data loading\")\n",
        "\n",
        "                if self.experiment.validation and\\\n",
        "                        self.steps % self.experiment.validation.interval == 0 and\\\n",
        "                        self.steps > self.experiment.validation.exempt:\n",
        "                    self.validate(validation_loaders, model, epoch, self.steps)\n",
        "                self.logger.report_time('Validating ')\n",
        "                if self.logger.verbose:\n",
        "                    torch.cuda.synchronize()\n",
        "\n",
        "                self.train_step(model, optimizer, batch,\n",
        "                                epoch=epoch, step=self.steps)\n",
        "                if self.logger.verbose:\n",
        "                    torch.cuda.synchronize()\n",
        "                self.logger.report_time('Forwarding ')\n",
        "\n",
        "                self.model_saver.maybe_save_model(\n",
        "                    model, epoch, self.steps, self.logger)\n",
        "\n",
        "                self.steps += 1\n",
        "                self.logger.report_eta(self.steps, self.total, epoch)\n",
        "\n",
        "            epoch += 1\n",
        "            if epoch > self.experiment.train.epochs:\n",
        "                self.model_saver.save_checkpoint(model, 'final')\n",
        "                if self.experiment.validation:\n",
        "                    self.validate(validation_loaders, model, epoch, self.steps)\n",
        "                self.logger.info('Training done')\n",
        "                break\n",
        "            iter_delta = 0\n",
        "\n",
        "    def train_step(self, model, optimizer, batch, epoch, step, **kwards):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        results = model.forward(batch, training=True)\n",
        "        if len(results) == 2:\n",
        "            l, pred = results\n",
        "            metrics = {}\n",
        "        elif len(results) == 3:\n",
        "            l, pred, metrics = results\n",
        "\n",
        "        if isinstance(l, dict):\n",
        "            line = []\n",
        "            loss = torch.tensor(0.).cuda()\n",
        "            for key, l_val in l.items():\n",
        "                loss += l_val.mean()\n",
        "                line.append('loss_{0}:{1:.4f}'.format(key, l_val.mean()))\n",
        "        else:\n",
        "            loss = l.mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % self.experiment.logger.log_interval == 0:\n",
        "            if isinstance(l, dict):\n",
        "                line = '\\t'.join(line)\n",
        "                log_info = '\\t'.join(['step:{:6d}', 'epoch:{:3d}', '{}', 'lr:{:.4f}']).format(step, epoch, line, self.current_lr)\n",
        "                self.logger.info(log_info)\n",
        "            else:\n",
        "                self.logger.info('step: %6d, epoch: %3d, loss: %.6f, lr: %f' % (\n",
        "                    step, epoch, loss.item(), self.current_lr))\n",
        "            self.logger.add_scalar('loss', loss, step)\n",
        "            self.logger.add_scalar('learning_rate', self.current_lr, step)\n",
        "            for name, metric in metrics.items():\n",
        "                self.logger.add_scalar(name, metric.mean(), step)\n",
        "                self.logger.info('%s: %6f' % (name, metric.mean()))\n",
        "\n",
        "            self.logger.report_time('Logging')\n",
        "\n",
        "    def validate(self, validation_loaders, model, epoch, step):\n",
        "        all_matircs = {}\n",
        "        model.eval()\n",
        "        for name, loader in validation_loaders.items():\n",
        "            if self.experiment.validation.visualize:\n",
        "                metrics, vis_images = self.validate_step(\n",
        "                    loader, model, True)\n",
        "                self.logger.images(\n",
        "                    os.path.join('vis', name), vis_images, step)\n",
        "            else:\n",
        "                metrics, vis_images = self.validate_step(loader, model, False)\n",
        "            for _key, metric in metrics.items():\n",
        "                key = name + '/' + _key\n",
        "                if key in all_matircs:\n",
        "                    all_matircs[key].update(metric.val, metric.count)\n",
        "                else:\n",
        "                    all_matircs[key] = metric\n",
        "\n",
        "        for key, metric in all_matircs.items():\n",
        "            self.logger.info('%s : %f (%d)' % (key, metric.avg, metric.count))\n",
        "        self.logger.metrics(epoch, self.steps, all_matircs)\n",
        "        model.train()\n",
        "        return all_matircs\n",
        "\n",
        "    def validate_step(self, data_loader, model, visualize=False):\n",
        "        raw_metrics = []\n",
        "        vis_images = dict()\n",
        "        for i, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            pred = model.forward(batch, training=False)\n",
        "            output = self.structure.representer.represent(batch, pred)\n",
        "            raw_metric, interested = self.structure.measurer.validate_measure(\n",
        "                batch, output)\n",
        "            raw_metrics.append(raw_metric)\n",
        "\n",
        "            if visualize and self.structure.visualizer:\n",
        "                vis_image = self.structure.visualizer.visualize(\n",
        "                    batch, output, interested)\n",
        "                vis_images.update(vis_image)\n",
        "        metrics = self.structure.measurer.gather_measure(\n",
        "            raw_metrics, self.logger)\n",
        "        return metrics, vis_images\n",
        "\n",
        "    def to_np(self, x):\n",
        "        return x.cpu().data.numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #!python3\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "# # from trainer import Trainer\n",
        "# # # tagged yaml objects\n",
        "# # from experiment import Structure, TrainSettings, ValidationSettings, Experiment\n",
        "# # from concern.log import Logger\n",
        "# # from data.data_loader import DataLoader\n",
        "# # from data.image_dataset import ImageDataset\n",
        "# # from training.checkpoint import Checkpoint\n",
        "# # from training.model_saver import ModelSaver\n",
        "# # from training.optimizer_scheduler import OptimizerScheduler\n",
        "# # from concern.config import Configurable, Config\n",
        "\n",
        "import sys\n",
        "\n",
        "def main():\n",
        "    # \"\"\"\n",
        "    # CUDA_VISIBLE_DEVICES=0 python train.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --num_gpus 1\n",
        "    # \"\"\"\n",
        "    # #sys.argv.append( 'experiments/seg_detector/td500_resnet18_deform_thre.yaml' )\n",
        "    # # sys.argv.append( '/content/DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml' )\n",
        "\n",
        "    # sys.argv.append( '--num_gpus' )\n",
        "    # sys.argv.append( '1' )\n",
        "\n",
        "    # parser = argparse.ArgumentParser(description='Text Recognition Training')\n",
        "    # parser.add_argument('exp', default='/content/DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml', type=str)\n",
        "    # parser.add_argument('--name', type=str)\n",
        "    # parser.add_argument('--batch_size', type=int, help='Batch size for training')\n",
        "    # parser.add_argument('--resume', type=str, help='Resume from checkpoint')\n",
        "    # parser.add_argument('--epochs', type=int, help='Number of training epochs')\n",
        "    # parser.add_argument('--num_workers', type=int, help='Number of dataloader workers')\n",
        "    # parser.add_argument('--start_iter', type=int, help='Begin counting iterations starting from this value (should be used with resume)')\n",
        "    # parser.add_argument('--start_epoch', type=int, help='Begin counting epoch starting from this value (should be used with resume)')\n",
        "    # parser.add_argument('--max_size', type=int, help='max length of label')\n",
        "    # parser.add_argument('--lr', type=float, help='initial learning rate')\n",
        "    # parser.add_argument('--optimizer', type=str, help='The optimizer want to use')\n",
        "    # parser.add_argument('--thresh', type=float, help='The threshold to replace it in the representers')\n",
        "    # parser.add_argument('--verbose', action='store_true', help='show verbose info')\n",
        "    # parser.add_argument('--visualize', action='store_true', help='visualize maps in tensorboard')\n",
        "    # parser.add_argument('--force_reload', action='store_true', dest='force_reload', help='Force reload data meta')\n",
        "    # parser.add_argument('--no-force_reload', action='store_false', dest='force_reload', help='Force reload data meta')\n",
        "    # parser.add_argument('--validate', action='store_true', dest='validate', help='Validate during training')\n",
        "    # parser.add_argument('--no-validate', action='store_false', dest='validate', help='Validate during training')\n",
        "    # parser.add_argument('--print-config-only', action='store_true', help='print config without actual training')\n",
        "    # parser.add_argument('--debug', action='store_true', dest='debug', help='Run with debug mode, which hacks dataset num_samples to toy number')\n",
        "    # parser.add_argument('--no-debug', action='store_false', dest='debug', help='Run without debug mode')\n",
        "    # parser.add_argument('--benchmark', action='store_true', dest='benchmark', help='Open cudnn benchmark mode')\n",
        "    # parser.add_argument('--no-benchmark', action='store_false', dest='benchmark', help='Turn cudnn benchmark mode off')\n",
        "    # parser.add_argument('-d', '--distributed', action='store_true', dest='distributed', help='Use distributed training')\n",
        "    # parser.add_argument('--local_rank', dest='local_rank', default=0, type=int, help='Use distributed training')\n",
        "    # parser.add_argument('-g', '--num_gpus', dest='num_gpus', default=4, type=int, help='The number of accessible gpus')\n",
        "    # parser.set_defaults(debug=False)\n",
        "    # parser.set_defaults(benchmark=True)\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    # args = vars(args)\n",
        "    # args = {k: v for k, v in args.items() if v is not None}\n",
        "\n",
        "#     if args['distributed']:\n",
        "#         torch.cuda.set_device(args['local_rank'])\n",
        "#         torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
        "\n",
        "    args = {\n",
        "        'exp': '/content/DB/experiments/seg_detector/td500_resnet18_deform_thre.yaml',\n",
        "        'verbose': False,\n",
        "        'visualize': False,\n",
        "        'force_reload': False,\n",
        "        'validate': False,\n",
        "        'print_config_only': False,\n",
        "        'debug': False,\n",
        "        'benchmark': True,\n",
        "        'distributed': False,\n",
        "        'local_rank': 0,\n",
        "        'num_gpus': 1,\n",
        "    }\n",
        "\n",
        "    conf = Config()\n",
        "    experiment_args = conf.compile(conf.load(args['exp']))['Experiment']\n",
        "    experiment_args.update(cmd=args)\n",
        "    experiment = Configurable.construct_class_from_config(experiment_args)\n",
        "\n",
        "    if not args['print_config_only']:\n",
        "        torch.backends.cudnn.benchmark = args['benchmark']\n",
        "        trainer = Trainer(experiment)\n",
        "        trainer.train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# CUDA_VISIBLE_DEVICES=0 python train.py experiments/seg_detector/td500_resnet18_deform_thre.yaml --num_gpus 1\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "SjzVu-UDQ4Yk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}