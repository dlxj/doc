% Chapter 5 of Concrete Mathematics
% (c) Addison-Wesley, all rights reserved.
\input gkpmac
\refin bib
\refin chap2
\refin chap4
\refin chap6

\pageno=153
\beginchapter 5 Binomial Coefficients

LET'S TAKE A BREATHER. The previous chapters have seen some heavy going,
with sums involving floor, ceiling, mod, phi, and mu functions.
Now we're going to study binomial coefficients, which turn out
to be (a)~more important in applications, and (b)~easier to manipulate,
\g Lucky us!\g
than all those other quantities.

\beginsection 5.1 Basic Identities

The symbol $n \choose k$ is a "binomial coefficient",
so called because of an important property we look at later this section,
the binomial theorem.
But we read the symbol ``$n$~choose~$k$.\qback''
This incantation arises from its "combinatorial interpretation"\dash---%
it is the number of ways to choose a $k$-element subset
from an $n$-element set.
\g Otherwise known as "combinations" of $n$~things, $k$~at a time.\g
For example, from the set $\{1,2,3,4\}$ we can choose two elements in six ways,
\begindisplay
 \{1,2\}\,,\quad \{1,3\}\,,\quad \{1,4\}\,,\quad \{2,3\}\,,\quad
	\{2,4\}\,,\quad \{3,4\}\,;
\enddisplay
so ${4 \choose 2} = 6$.

To express the number~$n \choose k$ in more familiar terms
it's easiest to first determine
the number of $k$-element {\it sequences},
rather than subsets, chosen from an $n$-element set;
for sequences, the order of the elements counts.
We use the same argument we used in Chapter~4 to show that
$n!$~is the number of permutations of $n$~objects.
There are $n$~choices for the first element of the sequence;
for each, there are $n-1$~choices for the second;
and so on, until there are $n-k+1$ choices for the~$k$th.
This gives $n(n-1) \ldots (n-k+1)=n\_k$ choices in all.
And since each $k$-element subset
has exactly $k!$ different orderings,
this number of {\it sequences\/} counts
each {\it subset\/} exactly $k!$~times.
To get our answer, we simply divide by~$k!$:
\begindisplay
 {n \choose k}
	= {n(n-1) \ldots (n-k+1)\over k(k-1) \ldots (1)} \,.
\enddisplay
For example,
\begindisplay
 {4 \choose 2}
	= {4 \cdt 3\over 2 \cdt 1}
	= 6 \,;
\enddisplay
this agrees with our previous enumeration.

We call $n$ the {\it"upper index"\/} and $k$ the {\it"lower index"}.
"!index, of a binomial coefficient"
The indices are
restricted to be nonnegative integers by the combinatorial interpretation,
because sets don't have negative or fractional numbers of elements.
But the binomial coefficient has many uses
besides its combinatorial interpretation,
so we will remove some of the restrictions.
It's most useful, it turns out,
to allow an arbitrary real (or even complex) number to appear in
the upper index, and to allow
an arbitrary integer in the lower.
Our formal definition therefore takes the following form:
\begindisplay
{r \choose k}=\cases{
	\displaystyle {r(r-1) \ldots (r-k+1)\over k(k-1) \ldots (1)}
	 = {r\_k\over k!}\,,&integer $k\ge0$;\cr
\noalign{\medskip}
	0\,,&integer $k < 0$.\cr}
\eqno\eqref|bc-def|
\enddisplay

This definition has several noteworthy features. First, the upper index
is called~$r$, not~$n$; the letter~$r$ emphasizes the fact
that binomial coefficients make sense
when any real number appears in this position.
For instance, we have ${-1 \choose 3} = (-1)(-2)(-3)/
(3 \cdt 2 \cdt 1) = -1$.
There's no combinatorial interpretation here, but $r=-1$ turns out to
be an important special case.
A noninteger index like $r = -1/2$ also turns out to be useful.

Second, we can view $r \choose k$ as a $k$th-degree polynomial in~$r$.
We'll see that this viewpoint is often helpful.

Third, we haven't defined binomial coefficients
for noninteger lower indices.
A reasonable definition can be given, but actual applications are
rare, so we will defer this generalization to
later in the chapter.

Final note:
We've listed the restrictions `integer $k \geq 0@$'
and `integer $k < 0@$' at the right of the definition.
Such restrictions will be listed
in all the identities we will study, so that the range of
applicability will be clear.
In general the fewer restrictions the better,
because an unrestricted identity is most useful;
still, any restrictions that apply are
an important part of the identity.
When we manipulate binomial coefficients,
it's easier to ignore
difficult-to-remember restrictions temporarily
and to check later that nothing has been violated.
But the check needs to be made.

For example, almost every time we encounter $n \choose n$ it equals~$1$,
so we can get lulled into thinking that it's always~$1$.
But a careful look at definition~\eq(|bc-def|) tells us that $n\choose n$ is~$1$
only when $n \geq 0$ (assuming that $n$~is an integer);
when $n < 0$ we have ${n \choose n} = 0$.
"Trap"s like this can (and will) make life adventuresome.

Before getting to the identities that we will use to tame binomial coefficients,
let's take a peek at some "small values".
The numbers in Table~|pascal-triangle|
form the beginning of {\it "Pascal's triangle"},
\vadjust{\vskip 14pt plus 4pt minus 2pt
\setbox0=\hbox{$\biggr)$}
\def\\#1{\displaystyle{n\choose#1}\kern-\wd0}
\table Pascal's triangle.\tabref|pascal-triangle|
\offinterlineskip
\halign to\hsize{\strut$\hfil#$\quad&\vrule#\kern-5pt\tabskip10pt plus 100pt&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$\tabskip\wd0\cr
\omit&height 3pt\cr
n &&\\0&\\1&\\2&\\3&\\4&\\5&\\6&\\7&\\8&\\9&\\{10}\cr
\omit&height 2pt\cr
\noalign{\hrule width\hsize}
\omit&height 3pt\cr
0 && 1 \cr
1 && 1 & 1 \cr
2 && 1 & 2 & 1 \cr
3 && 1 & 3 & 3 & 1 \cr
4 && 1 & 4 & 6 & 4 & 1 \cr
5 && 1 & 5 & 10 & 10 & 5 & 1 \cr
6 && 1 & 6 & 15 & 20 & 15 & 6 & 1 \cr
7 && 1 & 7 & 21 & 35 & 35 & 21 & 7 & 1 \cr
8 && 1 & 8 & 28 & 56 & 70 & 56 & 28 & 8 & 1 \cr
9 && 1 & 9 & 36 & 84 & 126 & 126 & 84 & 36 & 9 & 1 \cr
10 && 1 & 10 & 45 & 120 & 210 & 252 & 210 & 120 & 45 & 10 & 1\ \cr
\omit&height 2pt\cr}
\hrule height .5pt width\hsize
\vskip 14pt plus 4pt minus 2pt}% end the \vadjust
named after Blaise "Pascal" (1623--1662)
because he wrote an influential treatise about them [|pascal-treatise|].
\g Binomial coefficients were well known in Asia,
many centuries before Pascal was born~[|edwards|],
but he had no way to know that.\g
The empty entries in this table are actually $0@$'s,
because of a zero in the numerator of \eq(|bc-def|);
for example, ${1 \choose 2} = (1 \cdt 0) / (2 \cdt 1) = 0$.
These entries have been left blank
simply to help emphasize the rest of the table.

It's worthwhile to memorize formulas for the first three columns,
\begindisplay
{r \choose 0} = 1 \,,
	\qquad {r \choose 1} = r \,,
	\qquad {r \choose 2} = {r(r-1)\over 2} \,;
\eqno
\enddisplay
these hold for arbitrary reals.
(Recall that ${n+1\choose2}=\half n(n+1)$ is the formula we derived
for triangular numbers in Chapter~1;
triangular numbers are
conspicuously present in the $n\choose 2$ column of Table~|pascal-triangle|.)
It's also a good idea to memorize the first five rows or so of Pascal's triangle,
so that when the pattern $1$,~$4$, $6$, $4$,~$1$ appears in some problem
we will have a clue that binomial coefficients probably lurk nearby.

The numbers in Pascal's triangle satisfy, practically speaking,
\g In Italy it's called Tartaglia's triangle.\g
infinitely many identities,
so it's not too surprising that we can find some surprising relationships
by looking closely.
For example, there's a curious ``"hexagon property",\qback'' illustrated by
the six numbers $56$,~$28$, $36$, $120$, $210$,~$126$ that surround $84$
in the lower right portion of Table~|pascal-triangle|.
Both ways of multiplying alternate numbers from this hexagon
give the same product:
$56\cdt36\cdt210=28\cdt120\cdt126=423360$. The same thing holds if we
extract such a hexagon from any other part of Pascal's triangle.

\goodbreak
And now the identities. Our goal in this section will be to learn
\g\noindent\llap{``}C'est une chose estrange combien il est fertile en proprietez.''
\par\hfill\dash---B. Pascal [|pascal-treatise|]\g
a few simple rules by which we can solve the vast majority
of practical problems involving binomial coefficients.

Definition \eq(|bc-def|) can be recast in terms of factorials
in the common case that the upper index~$r$ is an integer, $n$, that's
greater than or equal to the lower index~$k$:
\begindisplay
{n \choose k}
	= {n!\over k! \, (n-k)!} \,,
			\qquad\hbox{integers $n \geq k \geq 0$.}
\eqno\eqref|bc-factorial|
\enddisplay
To get this formula, we just multiply the
numerator and denominator of \eq(|bc-def|) by $(n-k)!$.
It's occasionally useful to expand a binomial coefficient into this
"!factorial expansion"
factorial form (for example, when proving the hexagon property). And we often
want to go the other way, changing factorials into binomials.

The factorial representation hints at a symmetry in Pascal's triangle:
Each row reads the same left-to-right as right-to-left.
The identity reflecting this\dash---called the {\it symmetry\/}
"!symmetry identity"
identity\dash---is obtained by changing $k$ to $n-k$:
\begindisplay
{n \choose k}
	= {n \choose n-k} \,,
		\qquad\tworestrictions{integer $n \geq 0$,}{integer $k$.}
\eqno\eqref|bc-symmetry|
\enddisplay
This formula makes combinatorial sense,
because by specifying the $k$~chosen things out of~$n$
we're in effect specifying the $n-k$~unchosen things.

The restriction that $n$~and~$k$ be integers in identity \thiseq\ is obvious,
since each lower index must be an integer.
But why can't $n$~be negative?
Suppose, for example, that $n=-1$. Is
\begindisplay
 {-1 \choose k}\buildrel ? \over={-1 \choose -1-k}
\enddisplay
a valid equation?
No.
For instance, when $k=0$ we get $1$ on the left and $0$ on the right.
In fact, for any integer~$k \geq 0$ the left side is
\begindisplay
 {-1 \choose k}
	= {(-1)(-2) \ldots (-k)\over k!}
	= (-1)^k \,,
\enddisplay
which is either $1$~or~$-1$;
but the right side is~$0$, because the lower index is negative.
And for negative~$k$ the left side is~$0$ but the right side is
\begindisplay
 {-1 \choose -1-k}
	= (-1)^{-1-k} \,,
\enddisplay
which is either $1$~or~$-1$.
So the equation `${-1 \choose k} = {-1 \choose -1-k}$' is always false!

The symmetry identity fails for all other negative integers~$n$, too.
But unfortunately it's all too easy to forget this restriction,
since the expression in the upper index is sometimes negative
only for obscure (but legal) values of its variables.
\g I just hope I don't fall into this trap during the midterm.\g
Everyone who's manipulated binomial coefficients much
has fallen into this "trap" at least three times.

But the symmetry identity does have a big redeeming feature:
It works for all values of $k$, even when $k<0$ or $k>n$. (Because
both sides are zero in such cases.) Otherwise $0\le k\le n$, and
symmetry follows immediately from \eq(|bc-factorial|):
\begindisplay
{n\choose k}
	= {n!\over k! \, (n-k)!}
	= {n!\over \bigl( n - (n-k) \bigr)! \; (n-k)!}
	= {n \choose n-k} \,.
\enddisplay

Our next important identity lets us move things
in and out of binomial coefficients:
\begindisplay
{r \choose k}
	= {r\over k} {r-1 \choose k-1} \,, \qquad\hbox{integer $k \neq 0$.}
\eqno\eqref|bc-absorb|
\enddisplay
The restriction on $k$ prevents us from dividing by~$0$ here.
We call \thiseq\ an {\it absorption\/} "!absorption identity" identity,
because we often use it
to absorb a variable into a binomial coefficient when
that variable is a nuisance outside.
The equation follows from definition \eq(|bc-def|),
because $r\_k=r(r-1)\_{k-1}$ and $k!=k(k-1)!$ when $k>0$;
both sides are zero when $k<0$.

If we multiply both sides of \thiseq\ by $k$, we get an absorption identity
that works even when $k=0$:
\begindisplay
k {r \choose k}
	&= r {r-1 \choose k-1} \,, \qquad\hbox{integer $k$.}
\eqno\eqref|bc-absorb-k|
\enddisplay
This one also has a companion that keeps the lower index intact:
\begindisplay
(r-k){r \choose k}
	= r {r-1 \choose k} \,, \qquad\hbox{integer $k$.}
\eqno\eqref|bc-absorb-r-k|
\enddisplay
We can derive \thiseq\ by "sandwiching" an application of~\eq(|bc-absorb-k|)
between two applications of symmetry:
\begindisplay \openup4pt
(r-k){r \choose k}
&	= (r-k){r \choose r-k}&\qquad\hbox{(by symmetry)}\cr
&	= r{r-1\choose r-k-1}&\qquad\hbox{$\bigl($by \eq(|bc-absorb-k|)$\bigr)$}\cr
&	= r {r-1 \choose k} \,.&\qquad\hbox{(by symmetry)}
\enddisplay

\looseness=-1
But wait a minute.
We've claimed that the identity holds
for {\it all\/} real~$r$,
yet the derivation we just gave holds only
when $r$ is a positive integer.
(The upper index $r-1$ must be
a nonnegative integer if we're
to use the symmetry property \eq(|bc-symmetry|) with impunity.)
Have we been "cheating"?
No. \g(Well, not here anyway.)\g
It's true that the derivation is valid only for positive integers~$r@$;
but we can claim that the identity holds for all values of~$r$,
because both sides of \thiseq\ are polynomials in~$r$ of degree~$k+1$.
A nonzero polynomial of degree~$d$ or less can have at most $d$~distinct zeros;
therefore
the difference of two such polynomials, which also has degree~$d$ or less,
cannot be zero at more than $d$~points unless it is identically zero.
In other words, if two polynomials of degree~$d$ or less agree at more than
$d$ points, they must agree everywhere. We have shown that
$(r-k) {r \choose k}=r {r-1 \choose k}$ whenever $r$ is a positive
integer; so these two polynomials agree at infinitely many points, and
they must be identically equal.

The proof technique in the previous paragraph,
which we will call the {\it"polynomial argument"},
is useful for extending many identities from integers to reals;
we'll see it again and again. Some equations, like the
symmetry identity \eq(|bc-symmetry|),
are not identities between polynomials, so we can't always use
this method. But many identities do have the necessary form.

For example, here's another polynomial identity, perhaps the most
important binomial identity of all, known as the {\it"addition formula"}:
\begindisplay
{r \choose k}
	= {r-1 \choose k} + {r-1 \choose k-1} \,, \qquad\hbox{integer $k$.}
\eqno\eqref|bc-addition|
\enddisplay
When $r$ is a positive integer,
the addition formula tells us that every number in Pas\-cal's triangle
is the sum of two numbers in the previous row,
one directly above it and the other just to the left.
And the formula applies also when $r$ is negative, real, or complex;
the only restriction is that $k$ be an integer, so that
the binomial coefficients are defined.

One way to prove the addition formula is to assume that $r$
is a positive integer and to use the
"combinatorial interpretation".
Recall that $r \choose k$ is the number of possible
$k$-element subsets chosen from an $r$-element set.
If we have a set of $r$~"eggs" that includes exactly one bad egg,
there are $r\choose k$ ways to select $k$ of the eggs. Exactly
$r-1\choose k$ of these selections involve nothing but good eggs;
and $r-1\choose k-1$ of them contain the bad egg, because such
selections have $k-1$ of the $r-1$ good eggs.
Adding these two numbers together gives~\eq(|bc-addition|).
This derivation
assumes that $r$ is a positive integer, and that $k\ge0$.
But both sides of the identity are zero when $k<0$, and the polynomial argument
establishes \thiseq\ in all remaining cases.

We can also derive \thiseq\ by adding together the two absorption identities
\eq(|bc-absorb-r-k|)~and~\eq(|bc-absorb-k|):
\begindisplay
(r-k) {r \choose k} + k {r \choose k}
= r {r-1 \choose k} + r {r-1 \choose k-1}\,;
\enddisplay
the left side is $r{r\choose k}$, and we can divide through by~$r$.
This derivation is valid for everything but~$r=0$,
and it's easy to check that remaining case.

Those of us who tend not to discover such slick proofs,
or who are otherwise into tedium,
might prefer to derive \thiseq\ by
a straightforward manipulation of the definition.
If $k>0$,
\begindisplay \openup3pt
{r-1 \choose k} + {r-1 \choose k-1}
	&= {(r-1)\_k\over k!}
		+ {(r-1)\_{k-1}\over (k-1)!} \cr
	&= {(r-1)\_{k-1}\,(r-k)\over k!}
		+ {(r-1)\_{k-1}\,k\over k!} \cr
	&= {(r-1)\_{k-1}\,r\over k!} ={r\_k\over k!}={r \choose k} \,.
\enddisplay
Again, the cases for $k\le0$ are easy to handle.

We've just seen three rather different proofs of the addition formula.
This is not surprising;
binomial coefficients have many useful properties,
several of which are bound to lead to proofs
of an identity at hand.

The addition formula is essentially a recurrence for the numbers
of Pascal's triangle, so we'll see that it is especially useful for proving other
identities by induction. We can also get a new identity immediately
by unfolding the recurrence. For example,
\begindisplay \openup4pt
{5 \choose 3}
	&= {4 \choose 3} + {4 \choose 2} \cr
	&= {4 \choose 3} + {3 \choose 2} + {3 \choose 1} \cr
	&= {4 \choose 3} + {3 \choose 2} + {2 \choose 1}
			+ {2 \choose 0} \cr
	&= {4 \choose 3} + {3 \choose 2} + {2 \choose 1}
			+ {1 \choose 0} + {1 \choose -1} \,.
\enddisplay
Since ${1 \choose -1} = 0$, that term disappears and we can stop. This
method yields the general formula
\begindisplay
\sum_{k \leq n} {r+k \choose k}
	&= {r \choose 0} + {r+1 \choose 1} + \cdots + {r+n \choose n}\cr
	&= {r+n+1 \choose n} \,, \qquad\hbox{integer $n$.}
\eqno\eqref|bc-sum-both|
\enddisplay
Notice that we don't need the lower limit~$k \geq 0$ on the index of summation,
"!parallel summation"
because the terms with~$k<0$ are~zero.

This formula expresses one binomial coefficient as the sum of others
whose upper and lower indices stay the same distance apart.
We found it by repeatedly expanding
the binomial coefficient with the smallest lower index: first
$5 \choose 3$, then $4 \choose 2$, then $3 \choose 1$, then $\2 \choose 0$.
What happens if we unfold the other way, repeatedly
expanding the one with largest lower index? We get
\begindisplay \openup4pt
{5 \choose 3}
	&= {4 \choose 3} + {4 \choose 2} \cr
	&= {3 \choose 3} + {3 \choose 2} + {4 \choose 2} \cr
	&= {2 \choose 3} + {2 \choose 2} + {3 \choose 2}
			+ {4 \choose 2} \cr
	&= {1 \choose 3} + {1\choose 2} + {2 \choose 2} + {3 \choose 2}
			+ {4 \choose 2} \cr
	&= {0 \choose 3} + {0 \choose 2} + {1 \choose 2}
			+ {2 \choose 2} + {3 \choose 2} + {4 \choose 2} \,.
\enddisplay
Now $0 \choose 3$ is zero
(so are $0 \choose 2$ and~$1 \choose 2$, but these make the identity nicer),
and we can spot the general pattern:
\begindisplay
\sum_{0 \leq k \leq n} {k \choose m}
	&= {0 \choose m} + {1 \choose m} + \cdots + {n \choose m}\cr
	&= {n+1 \choose m+1} \,, \qquad\hbox{integers $m,n \geq 0$.}
\eqno\eqref|bc-sum-upper|
\enddisplay
This identity, which we call {\it"summation on the upper index"},
"!upper summation"
expresses a binomial coefficient as the sum of others
whose lower indices are constant.
In this case the sum needs the lower limit~$k \geq 0$,
because the terms with~$k<0$ {\it aren't\/} zero.
Also, $m$~and~$n$ can't in general be negative.

Identity~\eq(|bc-sum-upper|) has an interesting "combinatorial interpretation".
If we want to choose $m+1$ tickets from a set of $n+1$ tickets numbered
$0$~through~$n$, there are $k\choose m$ ways to do this when the largest
ticket selected is number~$k$.

We can prove both \eq(|bc-sum-both|)~and~\eq(|bc-sum-upper|)
by induction using the addition formula,
but we can also prove them from each other.
For example, let's prove~\eq(|bc-sum-both|) from~\eq(|bc-sum-upper|);
our proof will illustrate
some common binomial coefficient manipulations.
Our general plan will be to
massage the left side $\sum {r+k \choose k}$ of~\eq(|bc-sum-both|)
so that it
looks like the left side $\sum {k \choose m}$ of~\eq(|bc-sum-upper|);
then we'll invoke that identity,
replacing the sum by a single binomial coefficient;
finally we'll transform that coefficient
into the right side of~\eq(|bc-sum-both|).

We can assume for convenience
that $r$~and~$n$ are nonnegative integers;
the general case of \eq(|bc-sum-both|) follows from this special case,
by the polynomial argument.
Let's write $m$ instead of~$r$, so
that this variable looks more like a nonnegative integer. 
The plan can now be carried out systematically as follows:
\begindisplay \openup 3pt
\sum_{k \leq n} {m+k \choose k}
	&= \sum_{-m \leq k \leq n}  {m+k \choose k} \cr
	&= \sum_{-m \leq k \leq n}  {m+k \choose m} \cr
	&= \sum_{0 \leq k \leq m+n} {k \choose m} \cr
	&= {m+n+1 \choose m+1}
\;	 =\;{m+n+1 \choose n} \,.
\enddisplay
Let's look at this derivation blow by blow.
The key step is in the second line, where we apply the symmetry
law \eq(|bc-symmetry|) to replace $m+k\choose k$ by $m+k\choose m$.
We're allowed to do this only when $m+k\ge0$, so our first step
restricts the
range of~$k$ by discarding the terms with~$k<-m$.
(This is legal because those terms are zero.)
Now we're almost ready to apply \eq(|bc-sum-upper|); the third
line sets this up, replacing $k$ by $k-m$ and tidying up the
range of summation.
This step, like the first, merely plays around with $\sum$-notation.
Now $k$~appears by itself in the upper index
and the limits of summation are in the proper form,
so the fourth line applies~\eq(|bc-sum-upper|). One more use of symmetry
finishes the job.

Certain sums that we did in Chapters 1 and~2 were actually special cases
of \eq(|bc-sum-upper|), or disguised versions of this identity.
For example, the case~$m=1$
gives the sum of the nonnegative integers up through~$n$:
\begindisplay
 {0 \choose 1} + {1 \choose 1} + \cdots + {n \choose 1}
 	= 0 + 1 + \cdots + n
&	= {(n+1)n\over 2}
&	= {n+1 \choose 2}\,.
\enddisplay
And the general case is equivalent to Chapter 2's rule
\begindisplay
\sum_{0\le k\le n} k\_m={(n+1)\_{m+1}\over m+1},
	\qquad\hbox{integers $m,n\ge0$},
\enddisplay
if we divide both sides of this formula by $m!$. In fact, the addition
formula \eq(|bc-addition|) tells us that
\begindisplay
\Delta\biggl({x\choose m}\biggr)
={x+1\choose m}-{x\choose m}
={x\choose m-1}\,,
\enddisplay
if we replace $r$ and $k$ respectively by $x+1$ and $m$.
Hence the methods of Chapter 2 give us the handy
"indefinite summation" formula
\begindisplay
\sum{x\choose m}\,\delta x={x\choose m+1}+C\,.
\eqno
\enddisplay

Binomial coefficients get their name from the {\it "binomial theorem"},
which deals with powers of the binomial expression $x+y$.
\g\noindent\llap{``}At the age of twenty-one he~[\hbox{Moriarty}] wrote a treatise upon the
Binomial Theorem, which has had a European vogue. On the strength of it,
he won the Mathematical Chair at one of our smaller Universities.''\par
\hfill\dash---S. "Holmes"~[|holmes-final|]\g
Let's look at the smallest cases of this theorem:
\begindisplay \openup2pt
(x+y)^0	&=1x^0y^0\cr
(x+y)^1	&=1x^1y^0&+1x^0y^1\cr
(x+y)^2	&=1x^2y^0&+2x^1y^1&+1x^0y^2\cr
(x+y)^3	&=1x^3y^0&+3x^2y^1&+3x^1y^2&+1x^0y^3\cr
(x+y)^4	&=1x^4y^0&+4x^3y^1&+6x^2y^2&+4x^1y^3&+1x^0y^4\,.\cr
\enddisplay
It's not hard to see why these coefficients
are the same as the numbers in Pascal's triangle:
When we expand the product
\begindisplay
 (x+y)^n
	= \overbrace{(x+y)(x+y)\ldots(x+y)}^{n \rm\ factors} \,,
\enddisplay
every term is itself the product of $n$~factors, each either an $x$~or~$y$.
The number of such terms with $k$~factors of~$x$ and $n-k$~factors of~$y$
is the coefficient of~$x^k y^{n-k}$ after we combine like terms. And
this is exactly the number of ways to choose $k$ of the~$n$~binomials
from which an~$x$ will be contributed;
that is, it's~$n \choose k$.

Some textbooks leave the quantity "$0^0$" undefined, because the
functions $x^0$ and $0^x$ have different limiting values when $x$
decreases to~$0$.
But this is a mistake. We must define
\begindisplay
x^0=1\,,\qquad\hbox{for all~$x$},
\enddisplay
if the binomial theorem is to be valid when $x=0$, $y=0$,
and/or $x=-y$.
The theorem is too important to be arbitrarily restricted! By contrast,
the function $0^x$ is quite unimportant.
(See [|knuth-tnn|] for further discussion.)

But what exactly is the binomial theorem? In its full glory it is
the following identity:
\begindisplay
(x+y)^r
	= \sum_k {r \choose k} x^k y^{r-k} \,,
	\qquad\tworestrictions{integer $r \geq 0$}{or $\vert x/y\vert < 1$.}
\eqno\eqref|bin-thm-xy|
\enddisplay
The sum is over all integers $k$; but it is really a finite sum
when $r$~is a nonnegative integer, because all terms are zero
except those with $0 \leq k \leq r$.
On the other hand, the theorem is also valid when $r$ is negative,
or even when $r$ is an arbitrary real or complex number.
In such cases the sum really is infinite,
and we must have $\vert x/y\vert < 1$
to guarantee the sum's absolute convergence.

Two special cases of the binomial theorem are worth special attention,
even though they are extremely simple. If $x=y=1$ and $r=n$ is nonnegative,
we get
\begindisplay
2^n={n\choose0}+{n\choose1}+\cdots+{n\choose n}\,,\qquad\hbox{integer $n\ge0$.}
\enddisplay
This equation tells us that row $n$ of Pascal's triangle sums to $2^n$.
"!Pascal's triangle, row sums"
 And
when $x$ is $-1$ instead of~$+1$, we get
\begindisplay
0^n={n\choose0}-{n\choose1}+\cdots+(-1)^n{n\choose n}\,,
\qquad\hbox{integer $n\ge0$.}
\enddisplay
For example, $1-4+6-4+1=0$;
the elements of row $n$ sum to zero if we give them
alternating signs, except in the top row (when $n=0$ and $0^0=1$).

When $r$ is not a nonnegative integer, we most often use the binomial theorem
in the special case $y=1$. Let's state this special case explicitly,
writing $z$ instead of $x$ to emphasize the fact that an arbitrary
complex number can be involved here:
\begindisplay
(1+z)^r
	= \sum_k {r \choose k} z^k \,,
			\qquad\hbox{$\vert z\vert < 1$.}
\eqno\eqref|bin-thm-z|
\enddisplay
The general formula in \eq(|bin-thm-xy|) follows from this one if we set $z=x/y$
and multiply both sides by~$y^r$.

We have proved the binomial theorem only when $r$ is a nonnegative integer,
by using a combinatorial interpretation. We can't deduce the general
case from the nonnegative-integer case
by using the polynomial argument, because the sum is
infinite in the general case.
But when $r$ is arbitrary,
we can use "Taylor" series and the theory of complex variables:
\begindisplay
f(z)
&={f(0)\over0!}z^0+{f'(0)\over1!}z^1+{f''(0)\over2!}z^2+\cdots\,\cr
&=\sum_{k\ge0}{f^{(k)}(0)\over k!}z^k\,.
\enddisplay
The derivatives of the function $f(z)=(1+z)^r$ are easily evaluated; in fact,
$f^{(k)}(z)=r\_k\,(1+z)^{r-k}$. Setting $z=0$ gives \thiseq.

We also need to prove that the infinite sum converges, when $\vert z\vert
\g(Chapter 9 tells the meaning of\/~$O$.)\g
<1$. It does, because ${r\choose k}=O(k^{-1-r})$ by
equation \eq(|f-def-lim|) below.

\topinsert
\setbox0=\hbox{$\biggr)$}
\def\\#1{\displaystyle{n\choose#1}\kern-\wd0}
\table "Pascal's triangle, extended upward".\tabref|pascal-triangle-up|
\offinterlineskip
\halign to\hsize{\strut$\hfil#$\quad&\vrule#\kern-5pt\tabskip10pt plus 100pt&
 \hfil$#$&
 \hfil$#$&
 \hfil$#$&
\kern-3pt\hfil$#$&
 \hfil$#$&
\kern-3pt\hfil$#$&
 \hfil$#$&
\kern-3pt\hfil$#$&
 \hfil$#$&
\kern-3pt\hfil$#$&
 \hfil$#$\tabskip2pt\cr
\omit&height 4pt\cr
n &&\\0&\\1&\\2&\\3\kern.1em&\\4\kern.25em&\\5\kern.25em&\\6\kern.25em&
 \\7\kern.4em&\\8\kern.45em&\\9\kern.45em&\\{10}\kern.45em\cr
\omit&height 3pt\cr
\noalign{\hrule width\hsize}
\omit&height 3pt\cr
-4&& 1 &-4 & 10 &-20 & 35 &-56 & 84 &-120 & 165 &-220 & 286\cr
-3&& 1 &-3 & 6 &-10 & 15 &-21 & 28 &-36 & 45 &-55 & 66\cr
-2&& 1 &-2 & 3 &-4 & 5 &-6 & 7 &-8 & 9 &-10 & 11\cr
-1&& 1 &-1 & 1 &-1 & 1 &-1 & 1 &-1 & 1 &-1 & 1\cr
0 && 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\cr
\omit&height 2pt\cr}
\hrule width\hsize height.5pt
\kern4pt
\endinsert

Now let's look more closely at the values of $n\choose k$ when $n$ is
a negative integer. One way to approach these values is to use the
addition law \eq(|bc-addition|) to fill in the entries that lie
above the numbers in Table~|pascal-triangle|, thereby obtaining
Table~|pascal-triangle-up|.
For example, we must have ${-1\choose0}=1$, since
 ${@0@\choose0}
={-1\choose0}+{-1\choose-1}$ and ${-1\choose-1}=0$; then we must have
${-1\choose1}=-1$, since ${0\choose1}={-1\choose1}+{-1\choose0}$; and so on.

All these numbers are familiar. Indeed, the rows and columns
of Table~|pascal-triangle-up| appear
as columns in Table~|pascal-triangle| (but minus the minus signs).
So there must be
a connection between the values of $n\choose k$ for negative~$n$
and the values for positive~$n$. The general rule is
\begindisplay
{r \choose k}
	= (-1)^k {k-r-1 \choose k} \,, \qquad\hbox{integer $k$};
\eqno\eqref|negate-upper|
\enddisplay
it is easily proved, since
\begindisplay
r\_k&=r(r-1)\ldots(r-k+1)\cr
&=(-1)^k(-r)(1-r)\ldots(k-1-r)=(-1)^k(k-r-1)\_k
\enddisplay
when $k\ge0$, and both sides are zero when $k<0$.

Identity \thiseq\
is particularly valuable because it holds without any restriction.
(Of course, the lower index must be an integer so that the
binomial coefficients are defined.) The transformation in \thiseq\ is
called {\it"negating the upper index"}, or ``"upper negation".\qback''

But how can we remember this important formula? The other identities
we've seen\dash---symmetry, absorption, addition, etc.\dash---are pretty
simple, but this one looks rather messy. Still, there's a mnemonic
\g You call this a mnemonic? I'd~call it pneumatic\dash---\par
"!pneumathic comment"
full of air.\par It does help me remember, though.\g
that's not too bad: To negate
the upper index, we begin by writing down
$(-1)^k$, where $k$ is the lower index. (The lower index
doesn't change.) Then we immediately write $k$ again, twice, in both
lower and upper index positions. Then we negate the original upper index
by {\it subtracting\/} it from the new upper index. And we complete the job by
{\it subtracting\/}~$1$ more (always subtracting, not adding, because this is
a negation process).

Let's negate the upper index twice in succession, for practice. We get
\g(Now is a good time to do warmup exercise~|-1-choose-k|.)\g
\begindisplay \openup3pt
{r\choose k}
&=(-1)^k{k-r-1\choose k}\cr
&=(-1)^{2k}{k-(k-r-1)-1\choose k}={r\choose k}\,,
\enddisplay
\looseness=-1
so we're right back where we started. This is probably not what the
framers of the identity intended; but it's reassuring to know
\g It's also frustrating, if we're trying to get somewhere else.\g
that we haven't gone astray.

Some applications of \thiseq\ are, of course, more useful than this.
We can use upper negation, for example, to move quantities between upper and
lower index positions. The identity has a symmetric formulation,
\begindisplay
(-1)^m{-n-1\choose m}=(-1)^n{-m-1\choose n}\,,\qquad\hbox{integers $m,n\ge0$},
\eqno\eqref|bc-switch|
\enddisplay
which holds because both sides are equal to ${m+n\choose n}$.

Upper negation can also be used to derive the following interesting sum:
\begindisplay
\sum_{k \leq m} {r \choose k} (-1)^k
	&= {r \choose 0} - {r \choose 1} + \cdots +
					(-1)^m {r \choose m} \cr
	&= (-1)^m {r-1 \choose m} \,, \qquad\hbox{integer $m$.}
\eqno\eqref|bc-alt-sum|
\enddisplay
The idea is to negate the upper index, then apply
\eq(|bc-sum-both|), and negate again:
\g\vskip.5in(Here double negation helps, because we've "sandwich"ed
another operation in between.)\g
\begindisplay \openup3pt
 \sum_{k \leq m} {r \choose k} (-1)^k
	&= \sum_{k \leq m} {k-r-1 \choose k}\cr
\noalign{\smallskip}
	&= {-r+m \choose m}\cr
	&= (-1)^m {r-1 \choose m} \,.
\enddisplay
This formula gives us a partial sum
of the $r$th row of Pascal's triangle, provided that
the entries of the row have been given alternating signs.
For instance, if $r=5$ and $m=2$ the formula gives
$1 - 5 + 10 = 6 = (-1)^2 {4 \choose 2}$.

Notice that if $m \geq r$, \thiseq\ gives the alternating sum of the entire row,
and this sum is zero when $r$ is a positive integer. We proved this before,
"!Pascal's triangle, row sums"
when we expanded $(1-1)^r$ by the binomial theorem;
it's interesting to know that the "partial sums" of this expression can
also be evaluated in closed form.

How about the simpler partial sum,
\begindisplay
\sum_{k\le m}{n\choose k}={n\choose 0}+{n\choose1}+\cdots+{n\choose m}\,;
\eqno\eqref|bc-partial|
\enddisplay
surely if we can evaluate the corresponding sum with alternating signs,
we ought to be able to do this one? But no; there is no closed form
for the partial sum of a row of Pascal's triangle. We can do columns\dash---%
that's \eq(|bc-sum-upper|)\dash---but not rows. Curiously, however,
there is a way to partially sum the row elements if they have been multiplied
by their distance from the center:
\begindisplay
\sum_{k\le m}{r\choose k}\Bigl({r\over 2}-k\Bigr)={m+1\over2}{r\choose m+1},
 \qquad\hbox{integer $m$}.
\eqno\eqref|bc-partial-k|
\enddisplay
(This formula is easily verified by induction on $m$.)
The relation between these partial sums
with and without the factor of~$(r/2-k)$ in the summand
is analogous to the relation between the "integral"s
\begindisplay
 \int_{-\infty}^\alpha  x e^{-x^2} dx
		= - {\textstyle\half} e^{-\alpha^2}\quad\And\quad
	\int_{-\infty}^\alpha e^{-x^2} dx \,.
\enddisplay
The apparently more complicated integral on the left,
with the factor of~$x$, has a closed form,
while the simpler-looking integral on the right,
without the factor, has none.
\g(Well, the right-hand integral is ${}\kern2pt\half\sqrt\pi
(1+\mathop{\rm erf}\alpha)$, a constant plus a multiple of
the ``"error function"'' of~$\alpha$, if we're willing to accept that
as a closed form.)\g
 Appearances can be deceiving. "!cliches"

Near the end of this chapter, we'll study a method by which it's possible
to determine whether or not there is a closed form for the partial
sums of a given series involving binomial coefficients, in a fairly general
setting. This method is capable of discovering identities \eq(|bc-alt-sum|) and
\eq(|bc-partial-k|), and it also will tell us that \eq(|bc-partial|)
is a dead end.

Partial sums of the binomial series lead to a curious relationship of another
kind:
\begindisplay
\sum_{k\le m}{m{+}r\choose k}x^ky^{m-k}=
 \sum_{k\le m}{-r\choose k}(-x)^k(x+y)^{m-k}\,,\,\enspace\hbox{integer $m$}.
\eqno\eqref|partial-binomial|
\enddisplay
This identity isn't hard to prove by induction: Both sides are zero when
$m<0$ and~$1$ when $m=0$. If we let $S_m$ stand for the
sum on the left, we can apply the addition formula \eq(|bc-addition|)
and show easily that
\begindisplay \openup 4pt
S_m=
\sum_{k\le m}{m-1+r\choose k}x^ky^{m-k}
+\sum_{k\le m}{m-1+r\choose k-1}x^ky^{m-k}\,;
\enddisplay
and
\begindisplay
&\sum_{k\le m}{m-1+r\choose k}x^ky^{m-k}=yS_{m-1}+{m-1+r\choose m}x^m\,,\cr
&\sum_{k\le m}{m-1+r\choose k-1}x^ky^{m-k}=xS_{m-1}\,,\cr
\enddisplay
when $m>0$. Hence
\begindisplay
S_m=(x+y)S_{m-1}+{-r\choose m}(-x)^m\,,
\enddisplay
and this recurrence is satisfied also by the right-hand side of \thiseq.
By induction, both sides must be equal; QED.

But there's a neater proof. When $r$ is an integer in the range $0\ge r\ge-m$,
the binomial theorem tells us that both sides of \thiseq\ are
$(x+y)^{m+r}y^{-r}$. And since both sides are polynomials in~$r$ of degree
$m$~or less, agreement at $m+1$ different values is enough (but just
barely!) to prove equality in general.

It may seem foolish to have an identity where one sum equals another. Neither
side is in closed form. But sometimes one side turns out to be easier to
evaluate than the other. For example, if we set $x=-1$ and $y=1$, we get
\begindisplay
\sum_{k\le m}{m+r\choose k}(-1)^k={-r\choose m}\,,\qquad\hbox{integer $m\ge0$},
\enddisplay
an alternative form of identity \eq(|bc-alt-sum|). And if we set $x=y=1$
and $r=m+1$, we get
\begindisplay
\sum_{k\le m}{2m+1\choose k}=\sum_{k\le m}{m+k\choose k}2^{m-k}\,.
\enddisplay
The left-hand side sums just half of the binomial coefficients with upper
index $2m+1$, and these are equal to their counterparts in the other half
because Pascal's triangle has left-right symmetry. Hence the left-hand side
is just $\half 2^{2m+1}=2^{2m}$. This yields a formula that is quite
unexpected, "!unexpected sum"
\g(There's a nice combinatorial proof of this formula~[|world-series|].)\g
\begindisplay
\sum_{k\le m}{m+k\choose k}2^{-k}=2^m\,,\qquad\hbox{integer $m\ge0$}.
\eqno\eqref|half/2|
\enddisplay
Let's check it when $m=2$:\quad ${\2\choose0}+\half{3\choose1}+{1\over4}
 {4\choose2}=1+{3\over2}+{6\over4}=4$. Astounding.

\smallbreak
So far we've been looking either at binomial coefficients
by themselves or at sums of terms in which there's only one binomial coefficient
per term. But many of the challenging problems we face
involve products of two or more binomial coefficients,
so we'll spend the rest of this section considering how to
deal with such cases.

Here's a handy rule that often helps to simplify the
product of two binomial coefficients:
\begindisplay
{r \choose m} {m \choose k}
	= {r \choose k} {r-k \choose m-k} \,,
					\qquad\hbox{integers $m$, $k$.}
\eqno\eqref|bc-tc|
\enddisplay
We've already seen the
special case~$k=1$; it's the absorption identity~\eq(|bc-absorb-k|).
Although both sides of \thiseq\ are products of binomial coefficients,
one side often is easier to sum because of interactions with the
rest of a formula. For example, the left side uses $m$ twice,
the right side uses it only once. Therefore we usually want to replace
${r\choose m}{m\choose k}$ by
${r \choose k} {r-k \choose m-k}$ when summing on~$m$.

Equation \thiseq\ holds primarily because of cancellation between
$m!$'s in the factorial representations of $r\choose m$ and $m\choose k$.
If all variables are integers and $r\ge m\ge k\ge0$, we have
\begindisplay \openup3pt
{r \choose m} {m \choose k}
	&= {r!\over m!\,(r-m)!} \, {m!\over k!\,(m-k)!} \cr
	&= {r!\over k!\,(m-k)!\,(r-m)!} \cr
	&= {r!\over k!\,(r-k)!} \, {(r-k)!\over (m-k)!\,(r-m)!}
	 = {r \choose k} {r-k \choose m-k} \,.
\enddisplay
That was easy.
\g Yeah, right.\g
 Furthermore, if $m<k$ or $k<0$, both sides of \thiseq\ are
zero; so the identity holds for all integers $m$~and~$k$.
Finally, the polynomial argument extends its validity to all real~$r$.

A binomial coefficient ${r\choose k}=r!/(r-k)!\,k!$
 can be written
in the form $(a+b)!/a!\,b!$ after a suitable renaming of variables.
Similarly, the quantity in the middle of the derivation above,
$r!/k!\,(m-k)!\,(r-m)!$,
can be written in the form $(a+b+c)!/a!\,b!\,c!$.
This is a ``"trinomial" coefficient,\qback'' which arises in the
``trinomial theorem'':
\begindisplay \openup3pt
(x+y+z)^n
	&= \sum\twoconditions{0 \leq a,b,c \leq n}{a+b+c = n}
		\!{(a+b+c)!\over a!\,b!\,c!} x^a y^b z^c \cr
	&= \sum\twoconditions{0 \leq a,b,c \leq n}{a+b+c = n}
		{a+b+c \choose b+c}{b+c \choose c} x^a y^b z^c \,.
\enddisplay
So ${r \choose m} {m \choose k}$ is really a trinomial coefficient in disguise.
\g\vskip-60pt
\noindent\llap{``}Excogitavi autem olim mirabilem regulam pro numeris coefficientibus
potestatum, non tantum a binomio\/ $x+y$, sed et a trinomio\/ $x+y+z$,
imo a polynomio quocunque, ut data potentia gradus cujuscunque v.\ gr.\
decimi, et potentia in ejus valore comprehensa, ut\/~$x^5y^3z^2$, possim
statim assignare numerum coefficientem, quem habere debet, sine ulla
Tabula jam calculata.''\par\hfill\kern-4pt
 \dash---G.\thinspace W\kern-1pt.\thinspace"Leibniz"~[|leibniz|]\g
Trinomial coefficients pop up occasionally in applications, and we can
conveniently write them as
\begindisplay
 {a+b+c \choose a,\, b,\, c}={(a+b+c)!\over a!\,b!\,c!}
\enddisplay
in order to emphasize the symmetry present.

Binomial and trinomial coefficients generalize to {\it multinomial
coeffi\-cients}, which are always "!multinomial coefficients"
expressible as products of binomial coefficients:
\begindisplay \openup2pt
 {a_1+a_2+\cdots+a_m\choose a_1,a_2,\ldots,a_m}
 &={(a_1+a_2+\cdots+a_m)!\over a_1!\,a_2!\,\ldots\,a_m!}\cr
 &={a_1+a_2+\cdots+a_m\choose a_2+\cdots+a_m}\ldots{a_{m-1}+a_m\choose a_m}\,.\cr
\enddisplay
Therefore, when we run across such a beastie, our standard techniques apply.

Now we come to Table |bc-prod|, which lists identities that are
among the most important of our standard techniques.
These are the ones we rely on when struggling with a sum
involving a product of two binomial coefficients.
Each of these identities
is a sum over~$k$, with one appearance of~$k$ in each binomial
coefficient; there also are four nearly independent parameters, called
$m$, $n$, $r$, etc., one
in each index position. Different cases arise depending on whether $k$
appears in the upper or lower index,
and on whether it appears with a plus or minus sign.
Sometimes there's an additional factor of $(-1)^k$, which is needed to
make the terms summable in closed form.

\topinsert
\table Sums of products of binomial coefficients.\tabref|bc-prod|
%\begindisplay\abovedisplayskip=-2pt \belowdisplayskip=6pt \openup7pt%
\begindisplay\abovedisplayskip=0pt \belowdisplayskip=8pt \openup9pt%
 \advance\displayindent-\parindent\advance\displaywidth\parindent
&\sum_k{r\choose m+k}{s\choose n-k}={r+s\choose m+n}\,,
 \qquad}\hfill{\hbox{integers $m,n$.}
 \eqno\eqref|bc-prod1|\cr
&\sum_k{l\choose m+k}{s\choose n+k}={l+s\choose l-m+n}\,,
 \qquad}\hfill{\tworestrictions{integer $l\ge0$,}{integers $m,n$.}
 \eqno\eqref|bc-prod2|\cr
&\sum_k{l\choose m+k}{s+k\choose n}(-1)^k=(-1)^{l+m}{s-m\choose n-l}\,,
 \qquad}\hfill{\tworestrictions{integer $l\ge0$,}{integers $m,n$.}
 \eqno\eqref|bc-prod3|\cr
&\sum_{k\le l}{l-k\choose m}{s\choose k-n}(-1)^k=(-1)^{l+m}{s-m-1\choose l-m-n}\,,
 \quad}\hfill{\tworestrictions{integers}{$l,m,n\ge0$.}
 \eqno\eqref|bc-prod4|\cr
&\sum_{0\le k\le l}{l-k\choose m}{q+k\choose n}={l+q+1\choose m+n+1}\,,
 \quad}\hfill{\tworestrictions{integers $l,m\ge0$,}{integers $n\ge q\ge0$.}
 \eqno\eqref|bc-prod5|\cr
\enddisplay
\hrule width\hsize height.5pt
\kern4pt
\endinsert

Table |bc-prod| is far too complicated to memorize in full;
\g Fold down the corner on this page,
so you can find the table quickly later. You'll need it!\g
it is intended only for reference. 
But the first identity in this table is by far
the most memorable, and it should be remembered. It states that the
sum (over all integers~$k$) of the product
of two binomial coefficients, in which the upper indices are constant
and the lower indices have a constant sum for all~$k$,
is the binomial coefficient obtained by summing both lower and upper indices.
This identity is known as {\it "Vandermonde's convolution"}, because
Alexandre "Vandermonde" wrote a significant paper
about it in the late 1700s [|vandermonde|]; it was, however,
known to "Chu Shih-Chieh"
in China as early as 1303. All of the other identities in Table~|bc-prod|
can be obtained from Vandermonde's convolution by doing things
like negating upper indices or applying the symmetry law, etc., with
care; therefore Vandermonde's convolution is the most basic of all.

We can prove Vandermonde's convolution by giving it
a nice "combinatorial interpretation".
If we replace $k$ by~$k-m$ and $n$ by~$n-m$, we can assume that $m=0$; hence the
identity to be proved is
\begindisplay
\sum_k{r\choose k}{s\choose n-k}={r+s\choose n},\qquad\hbox{integer $n$}.
\eqno\eqref|vandermonde-conv|
\enddisplay
Let $r$ and $s$ be nonnegative integers; the general case then
follows by the polynomial argument.
On the right side, $r+s \choose n$ is the number of ways to
choose $n$~people from among $r$~men and $s$~women.
\g Sexist! You mentioned men first.\g
On the left, each term of the sum is the number of ways
to choose $k$~of the men and $n-k$~of the women.
Summing over all~$k$ counts each possibility exactly once.

Much more often than not we use these identities left to right,
since that's the direction of simplification.
But every once in a while it pays to go the other direction,
temporarily making an expression more complicated.
When this works, we've usually created a double sum
for which we can interchange the order of summation and then simplify.

Before moving on let's look at proofs for two more of the identities
in Table~|bc-prod|.
It's easy to prove \eq(|bc-prod2|); all we need to do is replace
the first binomial coefficient by $l\choose l-m-k$, then Vandermonde's
\eq(|bc-prod1|) applies.

 The next one, \eq(|bc-prod3|), is a bit more difficult.
We can reduce it to Vandermonde's convolution by a sequence of
transformations, but we can just as easily prove it by resorting
to the old reliable technique of mathematical induction.
Induction is often the first thing to try when
"!philosophy"
nothing else obvious jumps out at us, and
induction on~$l$ works just fine here.

For the basis~$l=0$, all terms are zero except when $k=-m$;
so both sides of the equation are~$(-1)^m{s-m\choose n}$.
Now suppose that the identity holds for all values less than some fixed~$l$,
where~$l>0$. We can use
the addition formula to replace $l\choose m+k$ by ${l-1\choose m+k}+
{l-1\choose m+k-1}$; the original sum now breaks into two sums, each
of which can be evaluated by the induction hypothesis:
\begindisplay \openup2pt
&\sum_k{l-1\choose m+k}{s+k\choose n}(-1)^k
 +\sum_k{l-1\choose m+k-1}{s+k\choose n}(-1)^k\cr
&\hskip5em=(-1)^{l-1+m}{s-m\choose n-l+1}+(-1)^{l+m}{s-m+1\choose n-l+1}\,.\cr
\enddisplay
And this simplifies to the right-hand side of \eq(|bc-prod3|), if we
apply the addition formula once again.

Two things about this derivation are worthy of note. First, we see again
the great convenience of summing over all integers~$k$, not just over
a certain range, because there's no need to fuss over boundary conditions.
Second, the addition formula works nicely with mathematical induction,
because it's a recurrence for binomial coefficients.
A binomial coefficient whose upper index is~$l$
is expressed in terms of two whose upper indices are~$l-1$,
and that's exactly what we need to apply the induction hypothesis.

So much for Table~|bc-prod|.
What about sums with three or more binomial coefficients?
If the index of summation is spread over all the coefficients,
our chances of finding a closed form aren't great:
Only a few closed forms are known for sums of this kind,
hence the sum we need might not match the given specs.
One of these rarities, proved in exercise~|prove-saalschutz|, is
\begindisplay
&\sum_k{m-r+s\choose k}{n+r-s\choose n-k}{r+k\choose m+n}\cr
&\qquad= {r\choose m}{s\choose n}\,,\qquad\hbox{integers $m,n\ge0$.}
\eqno\eqref|bc-saalschutz|
\enddisplay

Here's another, more symmetric example:
\begindisplay \openup3pt
&\sum_k{a+b\choose a+k}{b+c\choose b+k}{c+a\choose c+k}(-1)^k\cr
&\qquad ={(a+b+c)!\over a!\,b!\,c!}\,,\qquad\hbox{integers $a,b,c\ge0$.}
\eqno\eqref|bc-dixon|
\enddisplay
This one has a two-coefficient counterpart,
\begindisplay
\sum_k {a+b\choose a+k} {b+a\choose b+k} (-1)^k
	&= {(a+b)!\over a!\, b!} \,,\qquad\hbox{integers $a,b\ge0$,}
\eqno\eqref|bc-half-dixon|
\enddisplay
which incidentally doesn't appear in Table |bc-prod|. The analogous
four-coefficient sum doesn't have a closed form, but a similar sum does:
\begindisplay
&\sum_k(-1)^k{a+b\choose a+k}{b+c\choose b+k}{c+d\choose c+k}{d+a\choose d+k}
 \bigg/{2a+2b+2c+2d\choose a+b+c+d+k}\cr
&\global\medmuskip=1mu
\qquad={(a+b+c+d)!\,(a+b+c)!\,(a+b+d)!\,(a+c+d)!\,(b+c+d)!\over
 (2a+2b+2c+2d)!\,(a+c)!\,(b+d)!\,a!\,b!\,c!\,d!}\,,\cr
\noalign{\smallskip}
&\global\medmuskip=\normalmedmu
\hskip63mm\hbox{integers $a,b,c,d\ge0$.}
\enddisplay
This was discovered by John "Dougall" [|dougall|] early in the twentieth century.

Is Dougall's identity
 the hairiest sum of binomial coefficients known? No! The champion
so far is
\begindisplay \openup3pt
&\sum_{k_{ij}}(-1)^{\Sigma_{i<j}k_{ij}}
 \Biggl(\prod_{1\le i<j<n}\!{a_i{+}a_j\choose a_j{+}k_{ij}}\!\Biggr)
 \Biggl(\prod_{1\le j<n}\!{a_j+a_n\choose a_n+\Sigma_{i<j}k_{ij}-
  \Sigma_{i>j}k_{ji}}\!\Biggr)\cr
&\qquad={a_1+\cdots+a_n\choose a_1,a_2,\ldots,a_n}\,,
 \qquad\hbox{integers $a_1,a_2,\ldots,a_n\ge0$.}
\eqno\eqref|bc-dyson|
\enddisplay
Here the sum is over $n-1\choose2$ index variables $k_{ij}$ for $1\le i<j<n$.
Equation \eq(|bc-dixon|) is the special case $n=3$; the case $n=4$ can
be written out as follows, if we use $(a,b,c,d)$ for $(a_1,a_2,a_3,a_4)$ and
$(i,j,k)$ for $(k_{12},k_{13},k_{23})$:
\begindisplay \openup2pt
&\global\medmuskip=1mu
\sum_{i,j,k}(-1)^{i+j+k}{a+b\choose b+i}{a+c\choose c+j}{b+c\choose c+k}
 {a+d\choose d-i-j}{b+d\choose d+i-k}{c+d\choose d+j+k}\cr
&\global\medmuskip=\normalmedmu
\qquad={(a+b+c+d)!\over a!\,b!\,c!\,d!}\,,
 \qquad\hbox{integers $a,b,c,d\ge0$.}
\enddisplay
The left side of \thiseq\ is the coefficient of $z_1^0z_2^0\ldots z_n^0$
after
the product of $n(n-1)$ fractions
\begindisplay
\prod\twoconditions{1\le i,j\le n}{i\ne j}\left(1-{z_i\over z_j}\right)^{a_i}
\enddisplay
has been
fully expanded into positive and negative powers of the $z$'s. The
right side of \thiseq\ was conjectured by Freeman "Dyson" in 1962 and proved
by several people
shortly thereafter. Exercise~|dyson| gives a ``simple'' proof of \thiseq.

Another noteworthy identity involving lots of binomial coefficients is
\begindisplay
&\sum_{j,k}(-1)^{j+k}{j+k\choose k+l}{r\choose j}{n\choose k}
  {s+n-j-k\choose m-j}\cr
&\qquad =(-1)^l{n+r\choose n+l}{s-r\choose m-n-l}\,,
 \quad\hbox{integers $l,m,n$; \quad$n\ge0$.}
\eqno\eqref|bc-quad|
\enddisplay
This one, proved in exercise |prove-bc-quad|,
even has a chance of arising in practical applications. But we're
getting far afield from our theme of ``basic identities,\qback''
so we had better stop and take stock of what we've learned.

We've seen that binomial coefficients satisfy an almost bewildering
variety of identities. Some of these, fortunately,
are easily remembered, and we can use the memorable ones
to derive most of the others in a few steps. Table~|bc-tops|
collects ten of the most useful formulas, all in one place; these are the
best identities to know.

\beginsection 5.2 Basic Practice

In the previous section
 we derived a bunch of identities by manipulating
sums and plugging in other identities. It wasn't
too tough to find those derivations\dash---%
we knew what we were trying to prove,
so we could formulate a general plan and fill in the details
without much trouble.
Usually, however, out in the real world,
we're not faced with an identity to prove;
we're faced with a sum to simplify.
And we don't know what a simplified form might look like
(or even if one exists).
By tackling many such sums in this section and the next,
we will hone our binomial coefficient tools.

To start, let's try our hand at a few sums
involving a single binomial coefficient.

\subhead Problem 1: A sum of ratios.

We'd like to have a closed form for
\begindisplay
 \sum_{k=0}^m{m \choose k} \bigg/ {n \choose k} \,,
				\qquad\hbox{integers $n \geq m \geq 0$.}
\enddisplay
At first glance this sum evokes panic,
because we haven't seen any identities that
deal with a quotient of binomial coefficients.
(Furthermore the sum involves two binomial coefficients,
which seems to contradict the sentence preceding this problem.)
\g\vskip-1.5in\mathsurround=0pt\everypar{\hangindent2em}\hbadness=1200
\def\goto{g\kern-.1em\undertext{\kern.1emoto}}
 Algorithm \hfil\undertext{self-teach}:\par
1\enspace read problem\par
2\enspace attempt solution\par
3\enspace skim book solution\par
4\enspace \undertext{if} attempt failed
 \goto\ 1\par
\leavevmode\phantom{4\enspace}\undertext{else} \goto\ next problem
\vskip.75in\everypar{}
Unfortunately, that~algorithm can put you in an infinite loop.\smallskip
"!goto"
Suggested patches:\smallskip
\noindent\hbox to 1.5em{0\hfil}\undertext{set} $\,c\gets0$\par
\noindent\hbox to 1.5em{3a\hfil}\undertext{set} $\,c\gets c+1$\par
\noindent\hbox to 1.5em{3b\hfil}\undertext{if} $\,c=N$\par
\hskip2em\goto\ your TA\vskip.75in
\unitlength=1pt
\beginpicture(40,40)(-20,-20)
\thicklines
\put(0,0){\circle{28}}
\put(1,-1){\makebox(0,0){\goto}}
\put(-9.9,-9.9){\line(1,1){19.8}}\endpicture\smallskip
\hfill\dash---E.\thinspace W. "Dijkstra"
\vskip.75in
\dots\ But this sub\-chapter is called BASIC practice.\g
However, just as we can
use the factorial representations to reexpress a product
of binomial coefficients as another product\dash---%
that's how we got identity~\eq(|bc-tc|)\dash---%
we can do likewise with a quotient.
In fact we can avoid the grubby factorial representations
by letting $r=n$ and dividing both sides of equation~\eq(|bc-tc|)
by ${n \choose k} {n \choose m}$; this yields
\begindisplay
 {m \choose k} \bigg/ {n \choose k}
	= {n-k \choose m-k} \bigg/ {n \choose m} \,.
\enddisplay
So we replace the quotient on the left,
which appears in our sum,
by the one on the right; the sum becomes
\begindisplay
 \sum_{k=0}^m{n-k \choose m-k} \bigg/ {n \choose m} \,.
\enddisplay
We still have a quotient, but
the binomial coefficient in the denominator
doesn't involve the index of summation~$k$,
so we can remove it from the sum.
We'll restore it later.

\topinsert
\let\savethiseq=\thiseq
\def\\#1{\gdef\thiseq{\gtext#1}\eqno\global\advance\eqcount-1\cr}
\table The top ten binomial coefficient identities.\tabref|bc-tops|
\begindisplay\abovedisplayskip=-2pt \belowdisplayskip=6pt \openup7pt%
 \advance\displayindent-\parindent\advance\displaywidth\parindent
{n\choose k}&={n!\over k!\,(n-k)!}\,,
 \quad}\hfill{\tworestrictions
 {integers}{$n\ge k\ge0$.}\\{factorial expansion}
{n\choose k}&={n\choose n-k}\,,
 \quad}\hfill{\tworestrictions
 {integer $n\ge0$,}{integer $k$.}\\{symmetry}
{r\choose k}&={r\over k}{r-1\choose k-1}\,,
 \quad}\hfill{\hbox{integer $k\ne0$.}\\{absorption/extraction}
{r\choose k}&={r-1\choose k}+{r-1\choose k-1}\,,
 \quad}\hfill{\hbox{integer $k$.}\\{addition/induction}
{r\choose k}&=(-1)^k{k-r-1\choose k}\,,
 \quad}\hfill{\hbox{integer $k$.}\\{upper negation}
{r\choose m}{m\choose k}&={r\choose k}{r-k\choose m-k}\,,
 \quad}\hfill{\hbox{integers $m,k$.}\\{trinomial revision}
\sum_k{r\choose k}x^ky^{r-k}&=(x+y)^r\,,
 \quad}\hfill{\tworestrictions
 {integer $r\ge0$,}{or $\vert x/y\vert<1$.}\\{binomial theorem}
\noalign{\vskip-2pt}
\sum_{k\le n}{r+k\choose k}&={r+n+1\choose n}\,,
 \quad}\hfill{\hbox{integer $n$.}\\{parallel summation}
\noalign{\vskip-4pt}
\sum_{0\le k\le n}{k\choose m}&={n+1\choose m+1}\,,
 \quad}\hfill{\tworestrictions
 {integers}{$m,n\ge0$.}\\{upper summation}
\noalign{\vskip-2pt}
\sum_{k}{r\choose k}{s\choose n-k}&={r+s\choose n}\,,
 \quad}\hfill{\hbox{integer $n$.}\\{Vandermonde convolution}
\enddisplay
\hrule width\hsize height.5pt
\kern4pt
\global\let\thiseq=\savethiseq
\endinsert

We can also simplify the boundary conditions by summing over all $k\ge0$;
the terms for $k>m$ are zero.
The sum that's left isn't so intimidating:
\begindisplay
 \sum_{k\ge0}{n-k \choose m-k} \,.
\enddisplay
It's similar to the one in identity~\eq(|bc-sum-both|),
because the index~$k$ appears twice with the same sign.
But here it's $-k$ and in~\eq(|bc-sum-both|) it's not.
The next step should therefore be obvious; there's only one reasonable
thing to do:
\begindisplay
 \sum_{k\ge0}{n-k \choose m-k}
&= \sum_{m-k\ge0}{n-(m-k) \choose m-(m-k)}\cr
&=\sum_{k \leq m} {n-m+k \choose k} \,.
\enddisplay
And now we can apply the parallel summation identity, \eq(|bc-sum-both|):
\begindisplay
 \sum_{k \leq m} {n-m+k \choose k}
	= {(n-m)+m+1 \choose m}
	= {n+1 \choose m} \,.
\enddisplay

Finally we reinstate the $n \choose m$ in the denominator
that we removed from the sum earlier,
and then apply~\eq(|bc-absorb-r-k|)
to get the desired closed form:
\begindisplay
 {n+1 \choose m} \bigg/ {n \choose m}
	= {n+1\over n+1-m} \,.
\enddisplay
This derivation actually works for any real value of~$n$,
as long as no division by zero occurs;
that is, as long as $n$~isn't one of the integers $0$,~$1$, \dots,~$m-1$.

The more complicated the derivation,
the more important it is to check the answer.
This one wasn't too complicated but we'll check anyway.
In the small case $m=2$ and $n=4$ we have
\begindisplay
 {2 \choose 0} \bigg/ {4 \choose 0}
		\;+\; {2 \choose 1} \bigg/ {4 \choose 1}
		\;+\; {2 \choose 2} \bigg/ {4 \choose 2}
	= 1 + {1\over 2} + {1\over 6}
	= {5\over 3} \,;
\enddisplay
yes, this agrees perfectly with our closed form $(4+1)/(4+1-2)$.

\subhead Problem 2: From the literature of "sorting".

Our next sum appeared way back in ancient times (the early 1970s)
before people were fluent with binomial coefficients. A paper that
introduced an improved "merging" technique [|jones|] concludes with
the following remarks: ``It can be shown that the expected number
of saved transfers \dots\ is given by the expression
\begindisplay
 T = \sum_{r=0}^n \,r\,\, {_{m-r-1} C_{m-n-1}\over _m C_n}
\enddisplay
Here $m$ and $n$ are as defined above, and $_mC_n$ is the symbol for
the number of combinations of $m$ objects taken $n$ at a time. \dots\thinspace
The author is grateful to the "referee" for reducing a more complex equation
for expected transfers saved to the form given here.''

We'll see that this is definitely not a final answer to the author's
problem. It's not even a midterm answer.
\g Please, don't remind me of the midterm.\g

First we should translate the sum into something we can work with;
the ghastly notation $_{m-r-1} C_{m-n-1}$ is enough to stop anybody,
"!notation, ghastly"
save the enthusiastic referee~(please). "!Youngman, Henny" % "take my wife"
In our language we'd write
\begindisplay
 T = \sum_{k=0}^n\, k {m-k-1 \choose m-n-1} \bigg/
 {m \choose n} \,,\qquad\hbox{integers $m > n \geq 0$.}
\enddisplay
The binomial coefficient in the denominator
doesn't involve the index of summation,
so we can remove it and work with the new sum
\begindisplay
 S = \sum_{k=0}^n \,k {m-k-1 \choose m-n-1} \,.
\enddisplay

\looseness=-1
What next?
The index of summation appears
in the upper index of the binomial coefficient but not in the lower index.
So if the other~$k$ weren't there,
we could massage the sum and apply
summation on the upper index~\eq(|bc-sum-upper|).
With the extra~$k$, though, we can't.
If we could somehow absorb that~$k$ into the binomial coefficient,
using one of our absorption identities,
we could then sum on the upper index.
Unfortunately those identities don't work here.
But if the $k$ were instead $m-k$,
we could use absorption identity~\eq(|bc-absorb-k|):
\begindisplay
 (m-k) {m-k-1 \choose m-n-1}
	= (m-n) {m-k \choose m-n} \,.
\enddisplay

So here's the key: We'll
rewrite~$k$ as~$m - (m-k)$
and split the sum~$S$ into two sums:
\begindisplay \openup4pt
\sum_{k=0}^n \,k {m-k-1 \choose m-n-1}
	&=\sum_{k=0}^n \,\bigl(m-(m-k)\bigr) {m-k-1 \choose m-n-1}\cr
	&=\sum_{k=0}^n \,m{m-k-1 \choose m-n-1}
	        -\sum_{k=0}^n\, (m-k){m-k-1 \choose m-n-1}\cr
	&=m\sum_{k=0}^n {m-k-1 \choose m-n-1}
	        -\sum_{k=0}^n (m-n){m-k \choose m-n}\cr
\noalign{\smallskip}
	&=mA-(m-n)B\,,
\enddisplay
where
\begindisplay
A&=\sum_{k=0}^n{m-k-1\choose m-n-1}\,,\qquad
B&=\sum_{k=0}^n{m-k\choose m-n}\,.
\enddisplay

The sums $A$ and $B$ that remain are none other than our old friends
in which the upper index varies while the lower index stays fixed.
Let's do $B$ first, because it looks simpler. A little bit of massaging
is enough to make the summand match the left side of \eq(|bc-sum-upper|):
\begindisplay
\sum_{0 \leq k \leq n} {m-k \choose m-n}
 &= \sum_{0\le m-k\le n}{m-(m-k)\choose m-n}\cr
 &= \sum_{m-n\le k\le m}{k\choose m-n}\cr
 &= \sum_{0\le k\le m}{k\choose m-n}\,.\cr
\enddisplay
In the last step we've included the terms
with $0 \leq k < m-n$ in the sum;
they're all zero,
because the upper index is less than the lower.
Now we sum on the upper index, using \eq(|bc-sum-upper|), and get
\begindisplay
B=\sum_{0 \leq k \leq m} {k \choose m-n} = {m+1 \choose m-n+1} \,.
\enddisplay

The other sum $A$ is the same, but with $m$ replaced by $m-1$. Hence
we have a closed form for the given sum~$S$, which can be further simplified:
\begindisplay \openup7pt
S=mA-(m-n)B&=m{m\choose m-n}-(m-n){m+1\choose m-n+1}\cr
&=\left(m-(m-n){m+1\over m-n+1}\right){m\choose m-n}\cr
&=\left(n\over m-n+1\right){m\choose m-n}\,.\cr
\enddisplay
And this gives us a closed form for the original sum:
\begindisplay \openup5pt
 T &= S\,\bigg/ {m \choose n}\cr
	&= {n\over m-n+1} {m \choose m-n} \bigg/ {m \choose n}\cr
	&= {n\over m-n+1} \,.
\enddisplay
Even the referee can't simplify this.

Again we use a small case to check the answer.
When $m=4$ and $n=2$, we have
\begindisplay
\textstyle T = 0 \cdt {3 \choose 1} \big/ {4 \choose 2}
		\,+\, 1 \cdt {\2 \choose 1} \big/ {4 \choose 2}
		\,+\, 2 \cdt {1 \choose 1} \big/ {4 \choose 2}
	= 0 + {2\over 6} + {2\over 6}
	= {2\over 3} \,,
\enddisplay
which agrees with our formula $2/(4-2+1)$.

\subhead Problem 3: From an old exam.

Let's do one more sum that involves a single binomial coefficient. This
one, unlike the last, originated in the halls of academia; it was a problem
\g Do old exams ever~die?\g
on a take-home test. We want the value of $Q_{1000000}$, when
\begindisplay
 Q_n = \sum_{k \leq 2^n} {2^n - k \choose k} (-1)^k \,,
					\qquad\hbox{integer $n \geq 0$.}
\enddisplay
This one's harder than the others; we can't apply {\it any\/} of the
identities we've seen so far. And we're faced with
 a sum of $2^{1000000}$ terms, so we can't just add them up.
The index of summation~$k$ appears in both indices, upper and lower,
but with opposite signs.
Negating the upper index doesn't help, either;
it removes the factor of~$(-1)^k \bex$, but
it introduces a~$2k$ in the upper index.

When nothing obvious works,
we know that it's best to look at small cases.
If we can't spot a pattern and prove it by induction,
at least we'll have some data for checking our results.
Here are the nonzero terms and their sums
for the first four values of~$n$.
\begindisplay\def\preamble{\bigstrut\hfil$##$\hfil\ &\vrule##\ &&${}##$\hfil}%
 \setbox\bigstrutbox=\hbox{\vrule height13pt depth6pt width0pt}\offinterlineskip
n&&	&	& \quad Q_n \cr
\omit&height 2pt\cr
\noalign{\hrule}
\omit&height 2pt\cr
0&&\,{1\choose0}&=1&=\,1\cr
1&&\,{2\choose0}-{1\choose1}&=1-1&=\,0\cr
2&&\,{4\choose0}-{3\choose1}+{2\choose2}&=1-3+1&=-1\cr
3&&\,{8\choose0}-{7\choose1}+{6\choose2}-{5\choose3}+{4\choose4}
 &=1-7+15-10+1&=\,0\cr
\enddisplay
We'd better not try the next case, $n=4$;
the chances of making an arithmetic error are too high.
(Computing terms like $12@\choose 4$ and $11@\choose 5$ by hand,
let alone combining them with the others,
is worthwhile only if we're desperate.)

So the pattern starts out $1$,~$0$, $-1$,~$0$.
Even if we knew the next term or two,
the closed form wouldn't be obvious.
But if we could find and prove a recurrence for~$Q_n$
we'd probably be able to guess and prove its closed form.
To find a recurrence,
we need to relate~$Q_n$ to~$Q_{n-1}$ (or to~$Q_{\rm smaller\ values}$);
but to do this
we need to relate a term like $128 - 13 \choose 13$, which arises when
$n=7$ and $k=13$,
to terms like $64 - 13 \choose 13$.
This doesn't look promising;
we don't know any neat relations between entries in Pascal's triangle
that are 64~rows apart.
The addition formula,
our main tool for induction proofs,
only relates entries that are one row apart.

But this leads us to a key observation:
There's no need to deal with entries that are $2^{n-1}$ rows apart.
The variable~$n$ never appears by itself,
it's always in the context~$2^n \bex$.
So the~$2^n$ is a red herring!
\g Oh, the sneakiness of the instructor who set that exam.\g
If we replace~$2^n$ by~$m$, all we need to do is
find a closed form for the more general (but easier) sum
\begindisplay
 R_m = \sum_{k \leq m} {m-k \choose k} (-1)^k \,,
					\qquad\hbox{integer $m \geq 0$;}
\enddisplay
then we'll also have a closed form for $Q_n = R_{2^n}$.
And there's a good chance that the addition formula
will give us a recurrence for the sequence~$R_m$.

Values of $R_m$ for small $m$ can be read from Table |pascal-triangle|,
if we alternately
add and subtract values that appear in a southwest-to-northeast
diagonal. The results are:
\begindisplay \let\preamble=\tablepreamble
m&&0&1&2&3&4&5&6&7&8&9&10\cr
\noalign{\hrule}
R_m&& 1 & 1 & 0 &-1 & -1 & 0 & 1 & 1 & 0 & -1 & -1\cr
\enddisplay
There seems to be a lot of cancellation going on.

Let's look now at the formula for $R_m$ and see if it defines a recurrence.
Our strategy is to apply the addition formula~\eq(|bc-addition|) and to find
sums that
 have the form $R_k$ in the resulting expression, somewhat as
we did in the "perturbation" method of Chapter~2:
\begindisplay \openup2pt
R_m	&= \sum_{k \leq m} {m-k \choose k} (-1)^k \cr
	&= \sum_{k \leq m} {m-1-k \choose k} (-1)^k
		\;+\; \sum_{k \leq m} {m-1-k \choose k-1} (-1)^k \cr
	&= \sum_{k \leq m} {m-1-k \choose k} (-1)^k
		\;+\; \sum_{k+1 \leq m} {m-2-k \choose k}
								(-1)^{k+1} \cr
\noalign{\smallskip}
	&= \sum_{k \leq m-1}{m-1-k \choose k} (-1)^k
					\;+\; {-1 \choose m} (-1)^m \cr
\noalign{\vskip-2pt}
	&\hskip60pt
		-\; \sum_{k \leq m-2} {m-2-k \choose k} (-1)^k
					\;-\; {-1 \choose m-1} (-1)^{m-1} \cr
\noalign{\vskip2pt}
	&= R_{m-1} \,+\, (-1)^{2m} \,-\, R_{m-2} \,-\, (-1)^{2(m-1)}
	 = R_{m-1} \,-\, R_{m-2}\,.
\enddisplay
(In the next-to-last step we've used the formula
${-1 \choose m} = (-1)^m$, which we know is true when $m\ge0$.)
\g Anyway those of us who've done warmup exercise~|-1-choose-k| know it.\g
This derivation is valid for~$m \geq 2$.

From this recurrence we can generate values of~$R_m$ quickly,
and we soon perceive that the sequence is periodic. Indeed,
\begindisplay
 R_m=\left\{\,\vcenter{\baselineskip12.5pt
		\halign{$\hfil#\hfil$\cr 1\cr 1\cr 0\cr -1\cr -1\cr 0\cr}}
	\right.\qquad\hbox{if $m \bmod 6$}
    =\left\{\,\vcenter{\baselineskip12.5pt
		\halign{$\hfil#\hfil$\cr 0\cr 1\cr 2\cr 3\cr 4\cr 5\cr}}
	\right.\,.
\enddisplay
The proof by induction is by inspection. Or, if we must give a more
academic proof, we can unfold the recurrence one step to obtain
\begindisplay
R_m	= (R_{m-2} - R_{m-3}) \,-\, R_{m-2} = - R_{m-3} \,,
\enddisplay
whenever~$m \geq 3$. Hence $R_m=R_{m-6}$ whenever $m\ge6$.

Finally, since $Q_n = R_{2^n}$,
we can determine~$Q_n$ by determining $2^n \bmod 6$
and using the closed form for~$R_m$.
When $n=0$ we have $2^0 \bmod 6 = 1$;
after that we keep multiplying by~$2$ \tmod6,
so the pattern $2$,~$4$ repeats.
Thus
\begindisplay
 Q_n
	= R_{2^n}
	= \cases{R_1=1,&if $n=0$;\cr\noalign{\vskip2pt}
		R_2=0,&if $n$ is odd;\cr\noalign{\vskip2pt}
		R_4=-1,&if $n>0$ is even.\cr}
\enddisplay
This closed form for $Q_n$ agrees with the first four values we calculated
when we started on the problem.
We conclude that $Q_{1000000} = R_4 = -1$.

\subhead Problem 4: A sum involving two binomial coefficients.

Our next task is to find a closed form for
\begindisplay
 \sum_{k=0}^n \,k {m-k-1 \choose m-n-1} \,,
					\qquad\hbox{integers $m > n \geq 0$.}
\enddisplay
Wait a minute.
Where's the second binomial coefficient promised in the title of this problem?
And why should we try to simplify a sum we've already simplified?
(This is the sum~$S$ from Problem~2.)

Well, this is a sum that's easier to simplify
if we view the summand as a product of two binomial coefficients,
and then use one of the general identities found in Table~|bc-prod|.
The second binomial coefficient materializes
when we rewrite~$k$ as~$k \choose 1$:
\begindisplay
 \sum_{k=0}^n \,k {m-k-1 \choose m-n-1}
	= \sum_{0 \leq k \leq n} {k \choose 1} {m-k-1 \choose m-n-1} \,.
\enddisplay
And identity~\eq(|bc-prod5|) is the one to apply,
since its index of summation appears
in both upper indices and with opposite signs.

But our sum isn't quite in the correct form yet.
The upper limit of summation should be $m-1$, if we're to have
a perfect match with~\eq(|bc-prod5|). No problem; the
terms for $n<k\le m-1$ are zero. So we can plug in,
with $(l,m,n,q)\gets(m-1,m-n-1,1,0)$; the answer is
\begindisplay
S={m\choose m-n+1}\,.
\enddisplay
This is cleaner than the formula we got before. We can convert
it to the previous formula by using \eq(|bc-absorb-r-k|):
\begindisplay
{m\choose m-n+1}={n\over m-n+1}{m\choose m-n}\,.
\enddisplay

Similarly, we can get interesting results by plugging special values
into the other general identities we've seen. Suppose, for example, that we
set $m=n=1$ and $q=0$ in~\eq(|bc-prod5|). Then the identity reads
\begindisplay
\sum_{0\le k\le l}(l-k)k={l+1\choose 3}\,.
\enddisplay
The left side is $l\bigl((l+1)l/2\bigr)-(1^2+2^2+\cdots+l^2)$, so this gives us
a brand new way to solve the sum-of-squares problem that we beat
"!sum of consecutive squares"
to death in Chapter~2.

The moral of this story is:
Special cases of very general sums
are sometimes best handled in the general form. When learning general
forms, it's wise to learn their simple specializations.

\subhead Problem 5: A sum with three factors.

Here's another sum that isn't too bad.
We wish to simplify
\begindisplay
 \sum_k {n \choose k} {s \choose k} k \,,
					\qquad\hbox{integer $n \geq 0$.}
\enddisplay
The index of summation~$k$ appears in both lower indices
and with the same sign;
therefore identity~\eq(|bc-prod2|) in Table~|bc-prod|
looks close to what we need. With a bit of manipulation,
we should be able to use it.

The biggest difference between \eq(|bc-prod2|) and what we have
is the extra~$k$ in our sum.
But we can absorb~$k$ into one of the binomial coefficients
by using one of the absorption identities:
\begindisplay \openup3pt
\sum_k {n \choose k} {s \choose k} k
	&= \sum_k {n \choose k} {s-1 \choose k-1} s \cr
	&= s \sum_k {n \choose k} {s-1 \choose k-1} \,.
\enddisplay
We don't care that the $s$~appears when the~$k$ disappears,
because it's constant.
And now we're ready to apply the identity and get the closed form,
\begindisplay
 s \sum_k {n \choose k} {s-1 \choose k-1}
	= s {n+s-1 \choose n-1} \,.
\enddisplay
If we had chosen in the first step to absorb~$k$
into $n \choose k$, not $s \choose k$,
we wouldn't have been allowed to apply~\eq(|bc-prod2|) directly,
because $n-1$ might be negative; the identity requires a nonnegative
value in at least one of the upper indices.

\subhead Problem 6: A sum with menacing characteristics.

The next sum is more challenging.
We seek a closed form for
\begindisplay
 \sum_{k\ge0} {n+k \choose 2k} {2k \choose k} {(-1)^k\over k+1} \,,
					\qquad\hbox{integer $n \geq 0$.}
\enddisplay
One useful measure of a sum's difficulty
\g So we should deep~six this sum, right?\g
"!difficulty measure" "!philosophy" "!Catalan numbers"
is the number of times the index of summation appears.
By this measure we're in deep trouble\dash---$k$~appears six times.
Furthermore, the key step that worked in the previous problem\dash---%
to absorb something outside the binomial coefficients into one of them\dash---%
won't work here. If we absorb the~$k+1$
we just get another occurrence of~$k$ in its place.
And not only that: Our index $k$~is twice shackled with the coefficient~$2$
inside a binomial coefficient.
Multiplicative constants are usually harder
to remove than additive constants.

We're lucky this time, though.
The $2k$'s are right where we need them
for identity~\eq(|bc-tc|) to apply, so we get
\begindisplay
 \sum_{k\ge0}{n+k \choose 2k} {2k \choose k} {(-1)^k\over k+1}
	= \sum_{k\ge0}{n+k \choose k} {n \choose k}{(-1)^k\over k+1} \,.
\enddisplay
The two $2$'s disappear, and so does one occurrence of~$k$.
So that's one down and five to go.

The $k+1$ in the denominator is the most troublesome characteristic left,
and now we can absorb it into $n \choose k$
using identity~\eq(|bc-absorb-k|):
\begindisplay \openup2pt
\sum_{k\ge0}{n+k \choose k} {n \choose k} {(-1)^k\over k+1}
	&= \sum_k {n+k \choose k} {n+1 \choose k+1}{(-1)^k\over n+1} \cr
	&= {1\over n+1} \sum_k{n+k \choose k} {n+1 \choose k+1} (-1)^k \,.
\enddisplay
(Recall that $n\ge0$.) Two down, four to go.

To eliminate another~$k$ we have two promising options.
We could use symmetry on $n+k \choose k$;
or we could negate the upper index~$n+k$, thereby eliminating
that $k$ as well as the factor $(-1)^k$. Let's explore
both possibilities, starting with the symmetry option:
\begindisplay
 {1\over n+1} \sum_k {n+k \choose k} {n+1 \choose k+1} (-1)^k
	= {1\over n+1} \sum_k{n+k \choose n} {n+1 \choose k+1} (-1)^k \,.
\enddisplay
Third down, three to go,
\g For a minute I~thought we'd have~to punt.\g
and we're in position to make a big gain by plugging into
\eq(|bc-prod3|): Replacing $(l,m,n,s)$ by $(n+1,1,n,n)$, we get
\begindisplay
{1\over n+1} \sum_k{n+k \choose n} {n+1 \choose k+1} (-1)^k 
={1\over n+1}(-1)^n {n-1 \choose -1}=0\,.
\enddisplay
Zero, eh? After all that work?
Let's check it when $n=2$: ${\2\choose0}{0\choose 0}{1\over1}
-{3\choose2}{\2\choose1}{1\over2}+{4\choose4}{4\choose2}{1\over3}
=1-{6\over2}+{6\over3}=0$. It checks.

\smallskip
Just for the heck of it, let's explore our other option,
negating the upper index of $n+k \choose k$:
\begindisplay
 {1\over n+1} \sum_k {n+k \choose k} {n+1 \choose k+1} (-1)^k
	= {1\over n+1} \sum_k{-n-1 \choose k} {n+1 \choose k+1} \,.
\enddisplay
Now \eq(|bc-prod2|) applies, with $(l,m,n,s)\gets(n+1,1,0,-n-1)$, and
\begindisplay
 {1\over n+1} \sum_k{-n-1 \choose k} {n+1 \choose k+1} =
 {1\over n+1} {0\choose n}\,.
\enddisplay

Hey wait. This is zero when $n>0$, but it's $1$ when $n=0$. Our
other path to the solution told us that the sum was zero in all cases!
What gives? The sum actually does turn out to be~$1$ when $n=0$,
so the correct answer is `$\[n=0]$'.
We must have made a mistake in the previous derivation.

Let's do an instant replay on that derivation when $n=0$,
\g Try "binary search":\par Replay the middle formula first, to
see if the mistake was early or late.\g
in order to see where
the discrepancy first arises. Ah yes; we fell into the old "trap" mentioned
earlier: We tried to apply symmetry when the upper index could be negative!
We were not justified in replacing $n+k\choose k$ by $n+k\choose n$
when $k$ ranges over all integers, because this converts zero into a
nonzero value when $k<-n$. (Sorry about that.)

The other factor in the sum, $n+1\choose k+1$, turns out to be zero when
$k<-n$, except when $n=0$ and $k=-1$. Hence our error didn't show up
when we checked the case $n=2$. Exercise~|fix-symm-error| explains
what we should have done.

\subhead Problem 7: A new obstacle.

This one's even tougher; we want a closed form for
\begindisplay
 \sum_{k\ge0}{n+k \choose m+2k} {2k \choose k} {(-1)^k\over k+1} \,,
					\qquad\hbox{integers $m, n > 0$.}
\enddisplay
If $m$~were $0$ we'd have the sum from the problem we just finished.
But it's not,
and we're left with a real mess\dash---%
nothing we used in Problem~6 works here. (Especially not the crucial first step.)

However, if we could somehow get rid of the~$m$,
we could use the result just derived.
So our strategy is:
Replace $n+k \choose m+2k$
by a sum of terms like~$l+k \choose 2k$ for some nonnegative integer~$l@$;
the summand will then look like the summand in Problem~6, and
we can "interchange the order of summation".

What should we substitute for $n+k\choose m+2k$?
A painstaking examination of the identities derived
earlier in this chapter turns up only one suitable
candidate, namely equation \eq(|bc-prod5|) in Table~|bc-prod|. And one way to
use it is to replace the parameters $(l,m,n,q,k)$ by $({n+k-1},
2k,m-1,0,j)$, respectively:
\begindisplay \openup5pt
&\sum_{k\ge0}{n+k \choose m+2k} {2k \choose k} {(-1)^k\over k+1}\cr
&\qquad= \sum_{k\ge0}\,\,\sum_{0 \leq j \leq n+k-1}
		{n+k-1-j \choose 2k} {j \choose m-1}
					{2k \choose k} {(-1)^k\over k+1} \cr
&\qquad= \sum_{j \geq 0} {j \choose m-1}
  \sum_{\scriptstyle k \geq j-n+1\atop\scriptstyle k\ge0}
 {n+k-1-j \choose 2k}{2k \choose k} {(-1)^k\over k+1} \,.
\enddisplay
In the last step we've changed the order of summation,
manipulating the conditions below the $\sum$'s
according to the rules of Chapter~2.

We can't quite replace the inner sum using the result of Problem~6,
because it has the extra condition $k \geq j-n+1$.
But this extra condition is superfluous unless $j-n+1 > 0@$;
that is, unless~$j \geq n$.
And when $j \geq n$, the first binomial coefficient of the inner sum is zero,
because its upper index is between 0 and~$k-1$,
thus strictly less than the lower index~$2k$.
We may therefore place the additional restriction $j<n$ on the outer sum,
without affecting which nonzero terms are included. This makes
the restriction $k \geq j-n+1$ superfluous, and we can use the
result of Problem~6. The double sum now comes tumbling down:
\begindisplay
&\sum_{j \geq 0} {j \choose m-1}
  \sum_{\scriptstyle k \geq j-n+1\atop\scriptstyle k\ge0}
  {n+k-1-j \choose 2k}{2k \choose k} {(-1)^k\over k+1}\cr
&\qquad=\sum_{0\le j<n} {j \choose m-1}
 \sum_{k\ge0}{n+k-1-j \choose 2k}{2k \choose k} {(-1)^k\over k+1}\cr
&\qquad=\sum_{0\le j<n} {j \choose m-1}\[n-1-j=0]
	= {n-1 \choose m-1} \,.
\enddisplay
The inner sums vanish except when $j=n-1$, so we get a simple closed form
as our answer.

\subhead Problem 8: A different obstacle.

Let's branch out from Problem 6 in another way by considering
the sum
\begindisplay
S_m=\sum_{k\ge0}{n+k\choose 2k}{2k\choose k}{(-1)^k\over k+1+m}\,,
\qquad\hbox{integers $m,n\ge0$.}
\enddisplay
Again, when $m=0$ we have the sum we did before; but
now the $m$ occurs in a different place. This problem is a bit harder yet
than Problem~7, but (fortunately) we're getting better at finding solutions.
We can begin as in
Problem~6,
\begindisplay
S_m=\sum_{k\ge0}{n+k\choose k}{n\choose k}{(-1)^k\over k+1+m}\,.
\enddisplay
Now (as in Problem 7) we try to expand the part that depends on~$m$
into terms that we know how to deal with. When $m$~was zero, we absorbed
$k+1$ into $n\choose k$; if $m>0$, we can do the same thing if we
expand $1/(k+1+m)$ into absorbable terms. And our luck still holds:
We proved a suitable identity
\begindisplay
\sum_{j=0}^m{m\choose j}{r\choose j}^{\!-1}={r+1\over r+1-m}\,,
\qquad\tworestrictions{integer $m\ge0$,}{$r\notin\{0,1,\ldots,m-1\}$.}
\eqno\eqref|bc-quotient|
\enddisplay
in Problem~1. Replacing $r$ by $-k-2$ gives the desired expansion,
\begindisplay
S_m=\sum_{k\ge0}{n+k\choose k}{n\choose k}{(-1)^k\over k+1}
 \sum_{j\ge0}{m\choose j}{-k-2@\choose j}^{-1}.
\enddisplay
Now the $(k+1)^{-1}$ can be absorbed into $n\choose k$, as planned.
In fact, it could also be absorbed into ${-k-2@\choose j}{}^{-1}$.
Double absorption suggests that even more cancellation might be possible
behind the scenes. Yes\dash---expanding
everything in our new summand
into factorials and going back to binomial coefficients gives
a formula that we can sum
\g\vskip.3in They expect us to check this\par on a sheet of scratch~paper.\g
on~$k$:
\begindisplay \openup4pt
S_m
&={m!\,n!\over (m+n+1)!}\,\sum_{j\ge0}(-1)^j{m+n+1\choose n+1+j}
 \sum_k{n+1+j\choose k+j+1}{-n-1\choose k}\cr
&={m!\,n!\over (m+n+1)!}\,\sum_{j\ge0}(-1)^j{m+n+1\choose n+1+j}{j\choose n}\,.\cr
\enddisplay
The sum over all integers $j$ is zero, by \eq(|bc-prod3|). Hence $-S_m$ is the
sum for $j<0$.

To evaluate $-S_m$ for $j<0$, let's replace $j$ by $-k-1$ and sum for $k\ge0$:
\begindisplay \openup6pt
S_m
&={m!\,n!\over (m+n+1)!}\,\sum_{k\ge0}(-1)^k{m+n+1\choose n-k}{-k-1\choose n}\cr
&={m!\,n!\over (m+n+1)!}\,\sum_{k\le n}(-1)^{n-k}{m+n+1\choose k}{k-n-1\choose n}\cr
&={m!\,n!\over (m+n+1)!}\,\sum_{k\le n}(-1)^k{m+n+1\choose k}{2n-k\choose n}\cr
&={m!\,n!\over (m+n+1)!}\,\sum_{k\le2n}(-1)^k{m+n+1\choose k}{2n-k\choose n}\,.\cr
\enddisplay
Finally \eq(|bc-prod4|) applies, and we have our answer:
\begindisplay
S_m=(-1)^n{m!\,n!\over (m+n+1)!}{m\choose n}=(-1)^n m\_n m\_{-n-1}\,.
\enddisplay
Whew; we'd better check it. When $n=2$ we find
\begindisplay
S_m={1\over m+1}-{6\over m+2}+
{6\over m+3}={m(m-1)\over(m+1)(m+2)(m+3)}\,.
\enddisplay
Our derivation requires $m$ to be an integer,
but the result holds for all real~$m$, because the quantity
$(m+1)\_^{n+1}\,S_m$ is a polynomial in~$m$ of degree $\le n$.

\beginsection 5.3 Tricks of the Trade

Let's look next at three techniques that significantly amplify the
methods we have already learned.

\subhead Trick 1: Going halves.

\looseness=-1
Many \g\vskip-19pt This should really be called Trick~1/2.\g
"!halving"
of our identities involve an arbitrary real number~$r$.
When $r$ has the special form ``integer minus one half,\qback''
the binomial coefficient $r\choose k$ can be written as a quite
different-looking product of binomial coefficients. This leads
to a new family of identities that can be manipulated with surprising ease.

One way to see how this works is to begin with the
{\it"duplication formula"}
\begindisplay
\textstyle r\_k\,(r-\half)\_k=(2r)\_{2k}/2^{2k}\,,\qquad\hbox{integer $k\ge0$.}
\eqno\eqref|half-fact|
\enddisplay
This identity is obvious if we expand the falling powers and interleave the
factors on the left side:
\begindisplay \openup2pt
&{\textstyle r(r-\half)(r-1)(r-{3\over2})\ldots(r-k+1)(r-k+\half)}\cr
&\hskip8em=
{(2r)(2r-1)\ldots(2r-2k+1)\over 2\cdot2\cdot\ldots\cdot2}\,.
\enddisplay
Now we can divide both sides by $k!^2\mskip-1mu$, and we get
\begindisplay
{r\choose k}{r-1/2\choose k}={2r\choose2k}{2k\choose k}\bigg/2^{2k}\,,
 \qquad\hbox{integer $k$}.
\eqno\eqref|half-bc|
\enddisplay
If we set $k=r=n$, where $n$ is an integer, this yields
\begindisplay
{n-1/2\choose n}={2n\choose n}\bigg/2^{2n}\,,\qquad\hbox{integer $n$}.
\eqno\eqref|n-1/2-bc|
\enddisplay
And negating the upper index gives yet another useful formula,
\begindisplay
{-1/2\choose n}=\left(-1\over4\right)^{\!n}{2n\choose n}\,,\qquad\hbox{integer $n$}.
\eqno\eqref|minus-half-bc|
\enddisplay
For example, when $n=4$ we have\g\dots\thinspace we halve\thinspace\dots\g
\begindisplay \openup3pt \postdisplaypenalty=10000
{-1/2\choose4}&={(-1/2)(-3/2)(-5/2)(-7/2)\over4!}\cr
 &=\left(-1\over2\right)^{\!4}{1\cdt3\cdt5\cdt7\over1\cdt2\cdt3\cdt4}\cr
 &=\left(-1\over4\right)^{\!4}{1\cdt3\cdt5\cdt7\cdt2\cdt4\cdt6\cdt8 
           \over 1\cdt2\cdt3\cdt4\cdt1\cdt2\cdt3\cdt4}
  =\left(-1\over4\right)^{\!4}{8\choose4}\,.
\enddisplay
Notice how we've changed a product of odd numbers into a factorial.
"!product of consecutive odd integers"

Identity \eq(|half-bc|) has an amusing corollary. Let $r=\half n$, and take
the sum over all integers~$k$. The result is
\begindisplay \openup2pt \postdisplaypenalty=10000
\sum_k{n\choose2k}{2k\choose k}2^{-2k}
 &= \sum_k{n/2\choose k}{(n-1)/2\choose k}\cr
 &={n-1/2\choose\lfloor n/2\rfloor}\,,
\qquad\hbox{integer $n\ge0$}
\eqno
\enddisplay
by \eq(|bc-prod2|), because either $n/2$ or $(n-1)/2$ is $\lfloor n/2\rfloor$,
a nonnegative integer!

We can also use "Vandermonde's convolution" \eq(|vandermonde-conv|)
to deduce that
\begindisplay
\sum_k{-1/2\choose k}{-1/2\choose n-k}={-1\choose n}=(-1)^n\,,
\qquad\hbox{integer $n\ge0$.}
\enddisplay
Plugging in the values from \eq(|minus-half-bc|) gives
\begindisplay \openup4pt
{-1/2\choose k}{-1/2\choose n-k}
&=\left(-1\over4\right)^{\!k}\!{2k\choose k}\,
 \left(-1\over4\right)^{\!n-k}\!{2(n-k)\choose n-k}\cr
&={(-1)^n\over4^n}{2k\choose k}{2n-2k\choose n-k}\,;\cr
\enddisplay
this is what sums to $(-1)^n$.
Hence we have a remarkable property of the ``middle'' elements of
"!middle binomial coefficient"
Pascal's triangle:
\begindisplay
\sum_k{2k\choose k}{2n-2k\choose n-k}=4^n\,,\qquad\hbox{integer $n\ge0$}.
\eqno\eqref|sum-middle|
\enddisplay
For example, ${0\choose0}{6\choose3}+{\2\choose1}{4\choose2}
+{4\choose2}{\2\choose1}+{6\choose3}{0\choose0}=1\cdt20+2\cdt6+6\cdt2+20\cdt1
=64=4^3$.

\vskip1pt
These illustrations of our first trick
 indicate that it's wise to try changing binomial coefficients
of the form $2k\choose k$ into binomial coefficients of the form
$\smash{n-1/2@\choose k}$, where $n$ is some appropriate integer
(usually $0$, $1$, or~$k$); the resulting formula might be much simpler.

\subhead Trick 2: High-order differences.

We saw earlier that it's possible to evaluate partial sums of the
"!difference, $n$th order"
series ${n\choose k}(-1)^k$, but not of the series $n\choose k$.
It turns out that there are many important applications of binomial
coefficients with alternating signs, ${n\choose k}(-1)^k$. One of
the reasons for this is that such coefficients are intimately
associated with the difference operator $\Delta$ defined in Section~2.6.

The difference $\Delta f$ of a function $f$ at the point $x$ is
\begindisplay
\Delta f(x)=f(x+1)-f(x)\,;
\enddisplay
if we apply $\Delta$ again, we get the "second difference"
\begindisplay
\Delta^{2\,}f(x)=\Delta f(x+1)-\Delta f(x)
 &=\bigl(f(x{+}2)-f(x{+}1)\bigr)-\bigl(f(x{+}1)-f(x)\bigr)\cr
 &=f(x+2)-2f(x+1)+f(x)\,,
\enddisplay
which is analogous to the second derivative. Similarly, we have
\begindisplay
\Delta^{3\,}f(x)&=f(x+3)-3f(x+2)+3f(x+1)-f(x)\,;\cr
\Delta^{4\,}f(x)&=f(x+4)-4f(x+3)+6f(x+2)-4f(x+1)+f(x)\,;\cr
\enddisplay
and so on. Binomial coefficients enter these formulas with alternating signs.

In general, the $n$th difference is
\begindisplay
\Delta^{n\,} f(x)=\sum_k{n\choose k}(-1)^{n-k} f(x+k)\,,
 \qquad\hbox{integer $n\ge0$}.
\eqno\eqref|nth-diff|
\enddisplay
This formula is easily proved by induction, but there's also a nice way
to prove it directly using the elementary theory of "operators".
Recall that Section~2.6 defines the "shift operator"~$E$ by the rule
\begindisplay
E f(x)=f(x+1)\,;
\enddisplay
hence the operator $\Delta$ is
$E-1$, where $1$ is the identity operator defined by the rule
$1f(x)=f(x)$. By the binomial theorem,
\begindisplay
\Delta^n=(E-1)^n=\sum_k{n\choose k}E^k(-1)^{n-k}\,.
\enddisplay
This is an equation whose elements are operators; it is equivalent to
\thiseq, since $E^k$ is the operator that takes $f(x)$ into $f(x+k)$.

An interesting and important case arises when we consider "negative
falling powers". Let $f(x)=(x-1)\_{-1}=1/x$. Then, by rule~\equ(2.|delta-falling|),
we have $\Delta f(x)=(-1)(x-1)\_{-2}$,
$\Delta^{2\,} f(x)=(-1)(-2)(x-1)\_{-3}$, and in general
\begindisplay
\Delta^n\bigl((x-1)\_{-1}\bigr)=(-1)\_n\,(x-1)\_{-n-1}=(-1)^n
 {n!\over x(x+1)\ldots(x+n)}\,.
\enddisplay
Equation \thiseq\ now tells us that
\begindisplay
\sum_k{n\choose k}{(-1)^k\over x+k}
 &= {n!\over x(x+1)\ldots(x+n)}\cr
 &=x^{-1}{x+n\choose n}^{-1}\,,\qquad \hbox{$x\notin\{0,-1,\ldots,-n\}@$}.
\eqno\eqref|recip-bc|
\enddisplay
For example,
\begindisplay \openup3pt
&{1\over x}-{4\over x+1}+{6\over x+2}-{4\over x+3}+{1\over x+4}\cr
&\hskip4em={4!\over x(x+1)(x+2)(x+3)(x+4)}=1\bigg/x{x+4\choose4}\,.
\enddisplay
The sum in \thiseq\
"!binomial coefficient, reciprocal of"
is the "partial fraction expansion" of $n!/\bigl(x(x+1)\ldots(x+n)\bigr)$.

Significant results can be obtained from positive falling powers too.
If $f(x)$ is a polynomial of degree $d$, the difference $\Delta f(x)$
is a polynomial of degree $d-1$; therefore $\Delta^{d\,} f(x)$ is a
constant, and $\Delta^{n\,} f(x)=0$ if $n>d$. This extremely important
fact simplifies many formulas.

A closer look gives further information: Let
\begindisplay
f(x)=a_dx^d+a_{d-1}x^{d-1}+\cdots+a_1x^1+a_0x^0
\enddisplay
be any polynomial of degree $d$.
We will see in Chapter~6 that we
 can express ordinary powers as sums of falling powers (for example,
$x^2=x\_2+x\_1$); hence there are coefficients $b_d$, $b_{d-1}$, \dots,
$b_1$,~$b_0$ such that
\begindisplay
f(x)=b_dx\_d+b_{d-1}x\_{d-1}+\cdots+b_1x\_1+b_0x\_0\,.
\enddisplay
(It turns out that $b_d=a_d$ and $b_0=a_0$, but the intervening coefficients
are related in a more complicated way.)
Let $c_k=k!\,b_k$ for $0\le k\le d$. Then
\begindisplay
f(x)=c_d{x\choose d}+c_{d-1}{x\choose d-1}+\cdots+
 c_1{x\choose1}+c_0{x\choose0}\,;
\enddisplay
thus, any "polynomial" can be represented as a sum of
multiples of binomial coefficients.  Such an expansion is called the {\it
"Newton series"\/} of $f(x)$, because Isaac "Newton" used it extensively.

We observed earlier in this chapter that the addition formula implies
\begindisplay
\Delta\biggl({x\choose k}\biggr)={x\choose k-1}\,.
\enddisplay
Therefore, by induction, the $n$th difference of a Newton series is very simple:
\begindisplay
\Delta^{n\,} f(x)=c_d{x\choose d{-}n}+c_{d-1}{x\choose d{-}1{-}n}+\cdots+
 c_1{x\choose1{-}n}+c_0{x\choose-n}\,.
\enddisplay
If we now set $x=0$, all terms $c_k{x\choose k-n}$ on the right side are zero,
except the term with $k-n=0@$; hence
\begindisplay
\Delta^{n\,} f(0)=\cases{c_n\,,&if $n\le d$;\cr\noalign{\vskip2pt}
		 0\,,&if $n>d$.}
\enddisplay
The Newton series for $f(x)$ is therefore
\begindisplay
f(x)=\Delta^{d\,}f(0){x\choose d}+
     \Delta^{d-1\,}f(0){x\choose d-1}+\cdots
      +\Delta f(0){x\choose1}+f(0){x\choose 0}\,.
\enddisplay

For example, suppose $f(x)=x^3$. It's easy to calculate
\begindisplay \openup2pt \displaythick=\normalthick%
 \setmathsize{\Delta f(0)=0,\qquad}\let\\=\mathsize \postdisplaypenalty=10000
&\\{f(0)=0,}\\{f(1)=1,}\\{f(2)=8,}\\{f(3)=27;}\cr
&\hskip.5\wd\mathsizebox
 \\{\Delta f(0)=1,}\\{\Delta f(1)=7,}\\{\Delta f(2)=19;}\cr
&\hskip\wd\mathsizebox
 \\{\Delta^{2\,} f(0)=6,}\\{\Delta^{2\,} f(1)=12;}\cr
&\hskip1.5\wd\mathsizebox
 \\{\Delta^{3\,} f(0)=6.}\cr
\enddisplay
So the Newton series is
$x^3=6{x\choose 3}+6{x\choose 2}+1{x\choose 1}+0{x\choose 0}$.

Our formula $\Delta^{n\,}f(0)=c_n$ can also be stated in the following
way, using \eq(|nth-diff|) with $x=0$:
\begindisplay
\sum_k{n\choose k}(-1)^k\biggl(c_0{k\choose 0}+c_1{k\choose 1}+
c_2{k\choose 2}+\cdots\,\biggr)=(-1)^nc_n\,,\cr
\noalign{\vskip-5pt}
\hbox{integer $n\ge0$}.\hskip-10pt
\enddisplay
Here $\<c_0,c_1,c_2,\ldots\,\>$ is an arbitrary sequence of coefficients;
the infinite sum $c_0{k\choose0}+c_1{k\choose1}+c_2{k\choose2}+\cdots$
is actually finite for all $k\ge0$, so convergence is not an issue.
In particular, we can prove the important identity
\begindisplay
\sum_k{n\choose k}(-1)^k(a_0+a_1k+\cdots+a_nk^n)=(-1)^nn!\,a_n\,,\cr
\noalign{\vskip-3pt}
\hbox{integer $n\ge0$},\hskip-10pt
\eqno\eqref|nth-diff-extract|
\enddisplay
because the polynomial
 $a_0+a_1k+\cdots+a_nk^n$ can always be written as a Newton
series $c_0{k\choose0}+c_1{k\choose1}+\cdots+c_n{k\choose n}$ with
$c_n=n!\,a_n$.

Many sums that appear to be hopeless at first glance can actually be
summed almost trivially by using the idea of $n$th differences. For
example, let's consider the identity
\begindisplay
\sum_k{n\choose k}{r-sk\choose n}(-1)^k=s^n\,,\qquad\hbox{integer $n\ge0$}.
\eqno
\enddisplay
This looks very impressive, because it's quite different from anything
we've seen so far. But it really is easy to understand, once we notice
the telltale factor ${n\choose k}(-1)^k$ in the summand, because the function
\begindisplay
f(k)={r-sk\choose n}
 ={1\over n!}(-1)^ns^nk^n+\cdots\,=(-1)^ns^n{k\choose n}+\cdots
\enddisplay
is a polynomial in $k$ of degree $n$, with leading coefficient $(-1)^ns^n\!/n!$.
Therefore \thiseq\ is nothing more than an application of \eq(|nth-diff-extract|).

We have discussed Newton series under the assumption that $f(x)$ is
a polynomial. But we've also seen that infinite Newton series
\begindisplay
f(x)=c_0{x\choose0}+c_1{x\choose1}+c_2{x\choose2}+\cdots
\enddisplay
make sense too, because such sums are always finite when $x$
 is a nonnegative integer.
Our derivation
of the formula $\Delta^{n\,}f(0)=c_n$ works in the infinite case,
just as in the polynomial case; so we have the general identity
\begindisplay
f(x)=f(0){x\choose0}+\Delta f(0){x\choose1}+\Delta^{2\,}f(0){x\choose2}
 +\Delta^{3\,}f(0){x\choose3}+\cdots\,,\cr
\noalign{\vskip5pt}
\hbox{integer $x\ge0$}.\qquad
\eqno\eqref|newton-maclaurin|
\enddisplay
This formula is valid for any function $f(x)$ that is defined for
nonnegative integers $x$. Moreover, if the right-hand side converges
for other values of~$x$, it defines a function that ``"interpolates"''
$f(x)$ in a natural way. (There are infinitely many ways to
interpolate function values, so we cannot assert that \eq(|newton-maclaurin|)
is true for all $x$ that make the infinite series converge.
For example, if we let $f(x)=\sin(\pi x)$, we have $f(x)=0$ at all integer
points, so the right-hand side of \thiseq\ is identically zero;
but the left-hand side is nonzero at all noninteger~$x$.)

A Newton series is finite calculus's answer to infinite calculus's
"Taylor series". Just as a Taylor series can be written
\begindisplay
g(a+x)={g(a)\over0!}x^0+{g'(a)\over1!}x^1+{g''(a)\over2!}x^2
 +{g'''(a)\over3!}x^3+\cdots\,,
\enddisplay
the Newton series for $f(x)=g(a+x)$ can be written
\g(Since $E=1+\Delta$,\par\vskip1.5pt
\hfill$E^x=\Sigma_k{x\choose k}\Delta^k$;\smallskip
 and $E^xg(a)=$\par\hfill$g(a+x)$.)\g
\begindisplay
g(a+x)={g(a)\over0!}x\_0+{\Delta g(a)\over1!}x\_1+{\Delta^{2\,}g(a)\over2!}x\_2
 +{\Delta^{3\,}g(a)\over3!}x\_3+\cdots\,.
\eqno\eqref|newton-series|
\enddisplay
(This is the same as \eq(|newton-maclaurin|), because $\Delta^{n\,}f(0)
=\Delta^{n\,}g(a)$ for all $n\ge0$ when $f(x)=g(a+x)$.)
Both the Taylor and Newton series are finite when $g$ is a polynomial,
or when $x=0$; in addition, the Newton series is finite when
$x$ is a positive integer.
Otherwise the sums may or may not
converge for particular values of~$x$. If the Newton series converges
when $x$~is not a nonnegative integer,
it might actually converge to a value that's {\it different\/}
from $g(a+x)$, because the Newton series \thiseq\ depends only on
the spaced-out function values $g(a)$, $g(a+1)$, $g(a+2)$, \dots\thinspace.

\goodbreak
One example of a convergent Newton series is provided by the
binomial theorem. Let $g(x)=(1+z)^x$, where $z$~is a fixed complex number
such that $\vert z\vert<1$. Then $\Delta g(x)=(1+z)^{x+1}-(1+z)^x=z(1+z)^x$,
hence $\Delta^{n\,} g(x)=z^n(1+z)^x$. In this case
the infinite Newton series
\begindisplay \advance\belowdisplayskip-3pt
g(a+x)=\sum_n\Delta^{n\,} g(a){x\choose n}
=(1+z)^a\sum_n{x\choose n}z^n
\enddisplay
converges to the ``correct'' value $(1+z)^{a+x}$, for all~$x$.

James "Stirling" tried to use Newton series to generalize the factorial
"!factorial, generalized"
function to noninteger values. First he found coefficients~$S_n$
such that
\begindisplay
x!=\sum_n S_n{x\choose n}=S_0{x\choose0}+S_1{x\choose1}+S_2{x\choose2}+\cdots
\eqno\eqref|stirling-try1|
\enddisplay
is an identity for $x=0$, $x=1$, $x=2$, etc.
\g\noindent\llap{``}Forasmuch as these terms increase very fast, their differences will make
a diverging progression, which hinders the ordinate of the parabola from
approaching to the truth; therefore in this and the like cases, I interpolate
the logarithms of the terms, whose differences constitute a series swiftly
converging.''\par\hfill\dash---J. Stirling [|stirling-method|]\g
But he discovered that the resulting series doesn't converge except when $x$
is a nonnegative integer. So he tried again, this time writing
\begindisplay
\ln x!=\sum_n s_n{x\choose n}=s_0{x\choose0}+s_1{x\choose1}+s_2{x\choose2}+\cdots\,.
\eqno\eqref|stirling-try2|
\enddisplay
Now $\Delta(\ln x!)=\ln(x+1)!-\ln x!=\ln(x+1)$, hence
\begindisplay \openup3pt
s_n&=\Delta^n(\ln x!)\big\vert_{x=0}\cr
 &=\Delta^{n-1}\bigl(\ln(x+1)\bigr) \big\vert_{x=0}\cr
 &=\sum_k{n-1\choose k}(-1)^{n-1-k}\ln(k+1)
\enddisplay
by \eq(|nth-diff|). The coefficients are
therefore $s_0=s_1=0$; $s_2=\ln 2$; $s_3=\ln3-2\ln2=
\ln{3\over4}$; $s_4=\ln4-3\ln3+3\ln2=\ln{32\over27}$; etc.
\g(Proofs of convergence were not invented until the nineteenth century.)\g
In this way Stirling obtained a series that does converge
(although he didn't prove it); in fact, his series converges for all $x>-1$.
He was thereby able to evaluate $\half!$ satisfactorily.
Exercise |stirling-gamma| tells the rest of the story.

\subhead Trick 3: Inversion.

A special case of the rule \eq(|newton-series|)
 we've just derived for Newton's series can
be rewritten in the following way:
\begindisplay
g(n)=\sum_k{n\choose k}(-1)^k f(k)\!\iff\!
f(n)=\sum_k{n\choose k}(-1)^k g(k)\,.
\eqno\eqref|binomial-inversion|
\enddisplay
This dual relationship between $f$ and $g$ is called an
{\it"inversion formula"}\kern1pt;
 it's rather like the M\"obius inversion formulas
\equ(4.|mobius-inversion|) and \equ(4.|mobius-real-inversion|) that
we encountered in Chapter~4. Inversion formulas tell us how to solve
\g Invert this:\smallskip `z{\i}nb ppo'.\g % odd qu{\i}z
``"implicit recurrences",\qback'' where an unknown sequence is
embedded in a sum.

 For example, $g(n)$ might be a known function,
and $f(n)$ might be unknown; and we might have found a way to
show that $g(n)=\sum_k\kern-.34pt{n\choose k}(-1)^k f(k)$. Then \thiseq\ lets us
express $f(n)$ as a sum of known values.

We can prove \thiseq\ directly by using the basic methods at the
beginning of this chapter. If
$g(n)=\sum_k{n\choose k}(-1)^kf(k)$ for all $n\ge0$, then
\begindisplay \openup3pt
\sum_k{n\choose k}(-1)^k g(k)
&=\sum_k{n\choose k}(-1)^k\sum_j{k\choose j}(-1)^j f(j)\cr
&=\sum_jf(j)\sum_k{n\choose k}(-1)^{k+j}{k\choose j}\cr
&=\sum_jf(j)\sum_k{n\choose j}(-1)^{k+j}{n-j\choose k-j}\cr
&=\sum_jf(j){n\choose j}\sum_k(-1)^k{n-j\choose k}\cr
&=\sum_jf(j){n\choose j}\[n-j=0]\;=\;f(n)\,.\cr
\enddisplay
The proof in the other direction is, of course, the same, because the
relation between $f$ and $g$ is symmetric.

Let's illustrate \thiseq\ by applying it to the ``"football victory
problem"'': A group of $n$ "fan"s of the winning football team throw
"!sports, see football"
their "hats" high into the air. The hats come back randomly, one hat
to each of the $n$~fans. How many ways $h(n,k)$ are there for
exactly $k$~fans to get their own hats back?

For example, if $n=4$ and if the hats and fans are named $A$, $B$,
$C$,~$D$, the $4!=24$ possible ways for hats to land generate the
following numbers of rightful owners:
\begindisplay \def\preamble{&$##\hfil$\quad&$\hfil##$\hskip3em} \openup-2pt
ABCD&4&BACD&2&CABD&1&DABC&0\cr
ABDC&2&BADC&0&CADB&0&DACB&1\cr
ACBD&2&BCAD&1&CBAD&2&DBAC&1\cr
ACDB&1&BCDA&0&CBDA&1&DBCA&2\cr
ADBC&1&BDAC&0&CDAB&0&DCAB&0\cr
ADCB&2&BDCA&1&CDBA&0&DCBA&0\cr
\enddisplay
Therefore $h(4,4)=1$; \ $h(4,3)=0$; \ $h(4,2)=6$; \ $h(4,1)=8$; \ $h(4,0)=9$.

We can determine $h(n,k)$ by noticing that it is the number of ways to
choose $k$ lucky hat owners, namely $n\choose k$, times the number of
ways to arrange the remaining $n-k$ hats so that none of them
goes to the right owner,
namely $h(n-k,0)$. A permutation is called a {\it"derangement"\/} if it
moves every item, and the number of "derangements" of $n$~objects is
sometimes denoted by the symbol `$n\?$', read ``$n$~"subfactorial".\qback''
\tabref|nn:subfact|%
Therefore $h(n-k,0)=(n-k)\?$, and we have the general formula
\begindisplay
h(n,k)={n\choose k}h(n-k,0)={n\choose k}(n-k)\?\,.
\enddisplay
(Subfactorial "notation" isn't standard, and it's not clearly a great idea;
but let's try it awhile to see if we grow to like it. We can always
resort to `$D_n$' or something, if `$n\?$' doesn't work out.)

Our problem would be solved if we had a closed form for $n\?$, so let's
see what we can find. There's an easy way to get a recurrence, because
the sum of $h(n,k)$ for all~$k$ is the total number of permutations
of $n$~hats:
\begindisplay
n! =\sum_k h(n,k)
  &=\sum_k{n\choose k}(n-k)\?\cr
	&=\sum_k{n\choose k}k\?\,,
\qquad\hbox{integer $n\ge0$}.
\eqno\eqref|subfactorial-rec|
\enddisplay
(We've changed $k$ to $n-k$ and $n\choose n-k$ to $n\choose k$ in
the last step.) With this implicit recurrence we can compute all the
$h(n,k)$'s we like:
\begindisplay \let\preamble=\tablepreamble \offinterlineskip%
 \let\bigstrut=\strut \openup-2pt
n&&h(n,0)&h(n,1)&h(n,2)&h(n,3)&h(n,4)&h(n,5)&h(n,6)\cr
\omit&height 4pt\cr
\noalign{\hrule}
\omit&height 4pt\cr
0&	& 1 \cr
1&	& 0	& 1 \cr
2&	& 1	& 0	& 1 \cr
3&	& 2	& 3 	& 0	& 1 \cr
4&	& 9	& 8	& 6 	& 0	& 1 \cr
5&	& 44	& 45	& 20	& 10	& 0	& 1 \cr
6&	& 265	& 264	& 135	& 40	& 15	& 0	& 1
\enddisplay
For example, here's how the row for $n=4$ can be computed:
The two rightmost entries are obvious\dash---%
there's just one way for all hats to land correctly,
and there's no way for just three fans to get their own.
(Whose hat would the fourth fan get?)
When $k=2$ and $k=1$, we can use our equation for $h(n,k)$,
giving $h(4,2) = {4 \choose 2} h(2,0) = 6 \cdt 1 = 6$, and
$h(4,1) = {4 \choose 1} h(3,0) = 4 \cdt 2 = 8$.
We can't use this equation for $h(4,0)$; rather, we can,
but it gives us $h(4,0) = {4 \choose 0} h(4,0)$,
which is true but useless.
\g \vskip-11pt The art of mathematics, as of life,
"!philosophy"
is knowing which truths are useless.\g
Taking another tack,
we can use the relation $h(4,0)+8+6+0+1=4!$ to deduce that $h(4,0)=9$;
this is the value of $4\?$.
Similarly $n\?$ depends on the values of $k\?$ for $k<n$.

How can we solve a recurrence like \thiseq? Easy; it has the form of
\eq(|binomial-inversion|), with $g(n)=n!$ and $f(k)=(-1)^k k\?$. Hence its
solution is
\begindisplay
n\?=(-1)^n\sum_k{n\choose k}(-1)^k k!\,.
\enddisplay
Well, this isn't really a solution; it's a sum that should be put into
closed form if possible.
But it's better than a recurrence.
 The sum can be simplified, since $k!$ cancels
with a hidden $k!$ in $n\choose k$, so let's try that: We get
\begindisplay
n\?=\sum_{0\le k\le n}{n!\over(n-k)!}(-1)^{n+k}=n!\sum_{0\le k\le n}{(-1)^k
 \over k!}\,.
\eqno\eqref|subfactorial-sum|
\enddisplay
The remaining sum converges rapidly to the number $\sum_{k\ge0}(-1)^k\!/k!
=e^{-1}$. In fact, the terms that are excluded from the sum are
\begindisplay
n!\sum_{k>n}{(-1)^k\over k!}
&={(-1)^{n+1}\over n+1}\sum_{k\ge0}(-1)^k{(n+1)!\over(k+n+1)!}\cr
&={(-1)^{n+1}\over n+1}\biggl(1-{1\over n+2}
 +{1\over(n+2)(n+3)}-\cdots\,\biggr)\,,
\enddisplay
and the parenthesized quantity lies between $1$ and $1-{1\over n+2}=
{n+1\over n+2}$.
Therefore the difference between $n\?$ and $n!/e$ is roughly $1/n$ in
absolute value; more precisely, it lies between $1/(n+1)$ and $1/(n+2)$.
But $n\?$ is an integer. Therefore it must be what we get
when we round $n!/e$ to the nearest integer,
if $n>0$. So we have the closed form we seek:
\begindisplay
n\?=\biggl\lfloor {n!\over e}+\half\biggr\rfloor\,+\,\[n=0]\,.
\eqno\eqref|subfactorial-sol|
\enddisplay

This is the number of ways that no fan gets the right hat back.
When $n$~is large,
it's more meaningful to know the {\it"probability"\/} that this
\g Baseball fans:
.367 is also Ty "Cobb"'s lifetime batting average,
the all-time record. Can this be a coincidence?
\bigskip (Hey wait, you're fudging. Cobb's average was
$4191/11429\approx.366699$, while
$1/e\approx.367879$. But maybe if Wade "Boggs" has a few really good
seasons\dots\thinspace)\g
happens. If we assume that each of the
"!permutations"
$n!$~arrangements is equally likely\dash---because the hats were
thrown extremely high\dash---this probability is
\begindisplay
 {n\?\over n!}
	= {n!/e + O(1)\over n!}
	\sim {1\over e}
	= .367\!\ldots\,.
\enddisplay
"!lies, statistics" % [the next digit is 8]
So when $n$~gets large the probability that all hats are misplaced
is almost~37\%.

Incidentally, recurrence \eq(|subfactorial-rec|) for subfactorials is
exactly the same as \eq(|stirling-try1|),
the first recurrence considered by "Stirling" when he
was trying to generalize the factorial function.
Hence $S_k=k\?$. These coefficients are so large, it's no wonder the
infinite series \eq(|stirling-try1|) diverges for noninteger $x$.

\nobreak
Before leaving this problem, let's look briefly
at two interesting patterns that leap out at us in the
table of small~$h(n,k)$.
First, it seems that the numbers $1$,~$3$, $6$, $10$, $15$,~\dots\
below the all-$0$ diagonal are the triangular numbers.
This observation is easy to prove, since
those table entries are the $h(n,n{-}2)$'s, and we have
\begindisplay
h(n,n{-}2)={n\choose n-2}2\?={n\choose 2}\,.
\enddisplay

It also seems that the numbers in the first two columns differ by~$\pm1$.
Is this always true? Yes,
\begindisplay \openup3pt
h(n,0)-h(n,1)
&=n\?-n(n-1)\?\cr
&=\biggl(n!\sum_{0\le k\le n}{(-1)^k\over k!}\biggr) -
  \biggl(n(n-1)!\!\sum_{0\le k\le n-1}\!{(-1)^k\over k!}\biggr)\cr
&=n!{(-1)^n\over n!}=(-1)^n\,.
\enddisplay
In other words, $n\?=n(n-1)\?+(-1)^n$.
This is a much simpler recurrence for the derangement numbers than
we had before.

Now let's invert something else. If we apply inversion to the formula
\g But inversion is the source of smog.\g
\begindisplay
\sum_k{n\choose k}{(-1)^k\over x+k}={1\over x}{x+n\choose n}^{-1}
\enddisplay
that we derived in \eq(|recip-bc|), we find
\begindisplay
{x\over x+n}=\sum_{k\ge0}{n\choose k}(-1)^k{x+k\choose k}^{\!-1}\,.
\enddisplay
This is interesting, but not really new. If we negate the upper
index in ${x+k\choose k}$, we have merely
discovered identity \eq(|bc-quotient|) again.

\beginsection 5.4 Generating Functions

We come now to the most important idea in this whole book, the notion
of a {\it"generating function"}. An infinite sequence $\langle a_0,a_1,a_2,
\ldots\,\rangle$ that we wish to deal with in some way can conveniently
be represented as a {\it"power series"\/} in an auxiliary variable~$z$,
\begindisplay
A(z)=a_0+a_1z+a_2z^2+\cdots\,=\sum_{k\ge0} a_k z^k\,.
\eqno\eqref|generic-gf|
\enddisplay
It's appropriate to use the letter $z$ as the name of the auxiliary variable,
because we'll often be thinking of $z$ as a complex number.
The theory of complex variables conventionally uses `$z$' in its formulas;
power series (a.k.a.~"analytic functions" or "holomorphic functions") are
central to that theory.

We will be seeing lots of generating functions in subsequent chapters.
Indeed, Chapter~7 is entirely devoted to them. Our present goal is simply
to introduce the basic concepts, and to demonstrate the relevance of
generating functions to the study of binomial coefficients.

A generating function is useful because it's a single quantity that represents
an entire infinite sequence. We can often solve problems by first setting
up one or more generating functions, then by fooling around with those
functions until we know a lot about them, and finally by looking again
at the coefficients. With a little bit of luck, we'll know enough about the
function to understand what we need to know about its coefficients.

If $A(z)$ is any power series $\sum_{k\ge0}a_kz^k$, we will find it
\g (See [|knuth-bn|] for a discussion of the history and usefulness
of this notation.)\g
convenient to write
\begindisplay
[z^n]\,A(z)=a_n\,;
\eqno\eqref|bracket-notation|
\enddisplay
\tabref|nn:coeff-brack|%
in other words, $[z^n]\,A(z)$ denotes the coefficient of $z^n$ in $A(z)$.

Let $A(z)$ be the generating function for
$\langle a_0,a_1,a_2,\ldots\,\rangle$
as in \eq(|generic-gf|),
 and let $B(z)$ be the generating function for another sequence
$\langle b_0,b_1,b_2,\ldots\,\rangle$. Then the product $A(z)@B(z)$ is
the power series
\begindisplay \openup4pt
&(a_0+a_1z+a_2z^2+\cdots\,)
(b_0+b_1z+b_2z^2+\cdots\,)\cr
&\qquad=a_0b_0 \,+\, (a_0b_1+a_1b_0)z \,+\, (a_0b_2+a_1b_1+a_2b_0)z^2\,+\,\cdots\,;
\enddisplay
the coefficient of $z^n$ in this product is
\begindisplay
a_0b_n+a_1b_{n-1}+\cdots+a_nb_0=\sum_{k=0}^n a_kb_{n-k}\,.
\enddisplay
Therefore if we wish to evaluate any sum that has the general form
\begindisplay
c_n=\sum_{k=0}^n a_kb_{n-k}\,,
\eqno\eqref|convolution|
\enddisplay
and if we know the generating functions $A(z)$ and $B(z)$, we have
\begindisplay
c_n=[z^n]\,A(z)@B(z)\,.
\enddisplay

The sequence $\langle c_n\rangle$ defined by \thiseq\
is called the {\it "convolution"\/} of the sequences $\langle a_n\rangle$ and
$\langle b_n\rangle$; two sequences are ``convolved''
by forming the sums of all products whose subscripts add up
to a given amount. The gist of the previous paragraph is that convolution
of sequences corresponds to multiplication of their generating functions.

\goodbreak
Generating functions give us powerful ways to discover and/or prove
identities.
For example, the binomial theorem tells us that $(1+z)^r$ is the generating
function for the sequence $\langle{r\choose 0},{r\choose 1},{r\choose 2},
\ldots\,\rangle$:
\begindisplay
(1+z)^r=\sum_{k\ge0}{r\choose k}z^k\,.
\enddisplay
Similarly,
\begindisplay
(1+z)^s=\sum_{k\ge0}{s\choose k}z^k\,.
\enddisplay
If we multiply these together, we get another generating function:
\begindisplay
(1+z)^r(1+z)^s=(1+z)^{r+s}\,.
\enddisplay
And now comes the punch line: Equating coefficients of $z^n$ on both
sides of this equation gives us
\begindisplay
\sum_{k=0}^n{r\choose k}{s\choose n-k}={r+s\choose n}\,.
\enddisplay
\g\eq(|vandermonde-conv|)! =\par
 \quad\equ(5.|vandermonde-conv|)\equ(4.|vandermonde-conv|)\par
 \quad\equ(3.|vandermonde-conv|)\equ(2.|vandermonde-conv|)\par
 \quad\equ(1.|vandermonde-conv|)\equ(0.|vandermonde-conv|)!.\g
We've discovered "Vandermonde's convolution", \eq(|vandermonde-conv|)!

That was nice and easy; let's try another. This time we use $(1-z)^r$,
which is the generating function for the sequence $\bigl\langle(-1)^n
{r\choose n}\bigr\rangle=\bigl\langle{r\choose0},-{r\choose1},{r\choose2},
\ldots\,\bigr\rangle$. Multiplying by $(1+z)^r$ gives another generating
function whose coefficients we know:
\begindisplay
(1-z)^r(1+z)^r=(1-z^2)^r\,.
\enddisplay
Equating coefficients of $z^n$ now gives the equation
\begindisplay
\sum_{k=0}^n {r\choose k}{r\choose n-k}(-1)^k=(-1)^{n/2}{r\choose n/2}
 \[\hbox{$n$ even}]\,.
\eqno\eqref|alt-convolution|
\enddisplay

We should check this on a small case or two. When $n=3$, for example,
the result is
\begindisplay
{r\choose 0}{r\choose 3}
-{r\choose 1}{r\choose 2}
+{r\choose 2}{r\choose 1}
-{r\choose 3}{r\choose 0}=0\,.
\enddisplay
Each positive term is cancelled by a corresponding negative term.
And the same thing happens whenever $n$ is odd, in which case the sum isn't very
interesting. But when $n$ is even, say $n=2$, we get a nontrivial sum
that's different from Vandermonde's convolution:
\begindisplay
{r\choose 0}{r\choose 2}
-{r\choose 1}{r\choose 1}
+{r\choose 2}{r\choose 0}
=2{r\choose 2}-r^{\mskip2mu2}=-r\,.
\enddisplay
So \thiseq\ checks out fine when $n=2$. It turns out that
\eq(|bc-half-dixon|) is a special case of our new identity \thiseq.

Binomial coefficients also show up in some other generating functions,
most notably the following important identities in which the
lower index stays fixed and the upper index varies:
\g\vskip25pt If you have a highlighter pen,
 these two equations have got to be marked.\g
\begindisplay
&{1\over(1-z)^{n+1}}=\sum_{k\ge0}{n+k\choose n}z^k\,,\qquad
 \hbox{integer $n\ge0$}\eqno\eqref|neg-binomial1|\cr
&{z^n\over(1-z)^{n+1}}=\sum_{k\ge0}{k\choose n}z^k\,,\qquad
 \hbox{integer $n\ge0$}.
\eqno\eqref|neg-binomial2|
\enddisplay
The second identity here
is just the first one multiplied by $z^n$, that is, ``shifted right'' by
$n$ places. The first identity is just a special
case of the binomial theorem in slight disguise: If we expand $(1-z)^{-n-1}$
by \eq(|bin-thm-z|), the coefficient of $z^k$ is ${-n-1\choose k}(-1)^k$,
which can be rewritten as ${k+n\choose k}$ or ${n+k\choose n}$ by
negating the upper index. These
special cases are worth noting explicitly, because they
arise so frequently in applications.

When $n=0$ we get a special case of a special case, the geometric series:
\begindisplay
{1\over 1-z}=1+z+z^2+z^3+\cdots\,=\sum_{k\ge0}z^k\,.
\enddisplay
This is the generating function for the sequence $\langle 1,1,1,\ldots\,\rangle$,
and it is especially useful because the convolution of any other sequence
with this one is the sequence of sums: When $b_k=1$ for all~$k$,
\eq(|convolution|) reduces to
\begindisplay
c_n=\sum_{k=0}^n a_k\,.
\enddisplay
Therefore if $A(z)$ is the generating function for the summands
$\langle a_0,a_1,a_2,\ldots\,\rangle$,
then $A(z)/(1-z)$ is the generating function for the sums
$\langle c_0,c_1,c_2,\ldots\,\rangle$.

The problem of "derangements", which we solved by inversion in connection
with "hats" and football fans,
can be resolved with generating functions in
an interesting way. The basic recurrence
\begindisplay
n!=\sum_k{n\choose k}(n-k)\?
\enddisplay
can be put into the form of a convolution if we expand $n\choose k$ in
factorials and divide both sides by $n!$:
\begindisplay
1=\sum_{k=0}^n\,{1\over k!}\,{(n-k)\?\over(n-k)!}\,.
\enddisplay
The generating function for the sequence
$\langle{1\over0!},{1\over1!},{1\over2!},\ldots\,\rangle$ is $e^z$;
hence if we let
\begindisplay
D(z)=\sum_{k\ge0}{k\?\over k!}z^k\,,
\enddisplay
the convolution/recurrence tells us that
\begindisplay
{1\over1-z}=e^z\,D(z)\,.
\enddisplay
Solving for $D(z)$ gives
\begindisplay
D(z)={1\over1-z}\,e^{-z}={1\over1-z}
 \biggl({1\over0!}z^0-{1\over1!}z^1+{1\over2!}z^2+\cdots\,\biggr)\,.
\enddisplay
Equating coefficients of $z^n$ now tells us that
\begindisplay
{n\?\over n!}=\sum_{k=0}^n{(-1)^k\over k!}\,;
\enddisplay
this is the formula we derived earlier by inversion.

So far our explorations with generating functions have given us
slick proofs of things that we already knew how to derive by
more cumbersome methods. But we haven't used generating
functions to obtain any
new results, except for \eq(|alt-convolution|). Now we're ready for
something new and more surprising. There are two families of power
series that generate an especially rich class of binomial coefficient
identities: Let us define the {\it generalized binomial series\/}~$\Bscr_t(z)$
"!binomial series, generalized" "!exponential series, generalized"
and the {\it generalized exponential series\/}~$\Escr_t(z)$ as follows:
\setmathsize{\Bscr_t(z)^{1-t}-\Bscr_t(z)^{-t}=z\,;\qquad}
\begindisplay
\mathsize{\Bscr_t(z)=\sum_{k\ge0}(tk)\_{k-1}\,{z^k\over k!}\,;\qquad}
\Escr_t(z)=\sum_{k\ge0}(tk+1)^{k-1}\,{z^k\over k!}\,.
\eqno\eqref|t-series-def|
\enddisplay
It can be shown that these functions satisfy the identities
\begindisplay
\mathsize{\Bscr_t(z)^{1-t}-\Bscr_t(z)^{-t}=z\,;}
\Escr_t(z)^{-t}\ln \Escr_t(z)=z\,.
\eqno\eqref|t-series-rec|
\enddisplay
In the special case $t=0$, we have
\begindisplay
\mathsize{\Bscr_0(z)=1+z\,;}
\Escr_0(z)=e^z\,;
\enddisplay
this explains why the series with parameter $t$
are called ``generalized'' binomials and exponentials.

\setmathsize{\hskip.8\hsize}
The following pairs of identities are valid for all real~$r$:
\g The generalized binomial series $\Bscr_t(z)$ was discovered in the
1750s by J.\thinspace H. Lambert~[|lamb1|,~\S38],
who noticed a few years later~[|lamb2|]
that its powers satisfy the first identity in~\eq(|t-series-power|).\g
\begindisplay \openup3pt
&\Bscr_t(z)^r=\sum_{k\ge0}{tk+r\choose k}{r\over tk+r}z^k\,;\cr
&\mathsize{\hfill \Escr_t(z)^r=\sum_{k\ge0}r{(tk+r)^{k-1}\over k!}z^k\,;}
\eqno\eqref|t-series-power|\cr
\noalign{\smallskip}
&{\Bscr_t(z)^r\over1-t+t\Bscr_t(z)^{-1}}=\sum_{k\ge0}{tk+r\choose k}z^k\,;\cr
&\mathsize{\hfill
 {\Escr_t(z)^r\over1-zt\Escr_t(z)^t}=\sum_{k\ge0}{(tk+r)^k\over k!}z^k\,.}
\eqno\eqref|t-series-mod-power|\cr
\enddisplay
(When $tk+r=0$, we have to be a little careful about how the coefficient
of $z^k$ is interpreted; each coefficient is a polynomial in~$r$.
For example, the constant term of $\Escr_t(z)^r$ is $r(0+r)^{-1}$, and this
is equal to~$1$ even when $r=0$.)

Since equations \eq(|t-series-power|) and \eq(|t-series-mod-power|)
hold for all~$r$, we get very general identities when we multiply
together the series that correspond to different powers $r$ and~$s$.
For example,
\begindisplay
\Bscr_t(z)^r{\Bscr_t(z)^s\over1-t+t\Bscr_t(z)^{-1}}
&=\sum_{k\ge0}{tk+r\choose k}{r\over tk+r}z^k\,
  \sum_{j\ge0}{t@j+s\choose j}z^j\cr
&=\sum_{n\ge0}z^n\sum_{k\ge0}{tk+r\choose k}{r\over tk+r}{t(n-k)+s\choose n-k}\,.
\enddisplay
This power series must equal
\begindisplay
{\Bscr_t(z)^{r+s}\over1-t+t\Bscr_t(z)^{-1}}=\sum_{n\ge0}{tn+r+s\choose n}z^n\,;
\enddisplay
hence we can equate coefficients of $z^n$ and get the identity
\begindisplay
\sum_k{tk+r\choose k}{t(n-k)+s\choose n-k}{r\over tk+r}
={tn+r+s\choose n}\,,\qquad\hbox{integer $n$},
\enddisplay
valid for all real $r$, $s$, and $t$. When $t=0$ this identity reduces to
"Vandermonde's convolution". (If by chance $tk+r$ happens to equal zero
in this for\-mula, the denominator
factor $tk+r$ should be considered to cancel with the
$tk+r$ in the numerator of the binomial coefficient. Both sides
of the identity are polynomials in $r$, $s$, and~$t$.)
Similar identities hold when we multiply $\Bscr_t(z)^r$ by $\Bscr_t(z)^s$, etc.;
Table~|conv-table| presents the results.

\topinsert
\table General convolution identities, valid for integer $n\ge0$.\tabref|conv-table|
\begindisplay\abovedisplayskip=-2pt \belowdisplayskip=6pt \openup7pt%
 \advance\displayindent-\parindent\advance\displaywidth\parindent
&\setmathsize{\sum_k{tk+r\choose k}{tn-tk+s\choose n-k}{r\over tk+r}
={tn+r+s\choose n}\,.}\copy\mathsizebox\eqno\eqref|t-b1|\cr
&\sum_k{tk+r\choose k}{tn-tk+s\choose n-k}{r\over tk+r}\cdot{s\over tn-tk+s}\cr
&\mathsize{\hfill={tn+r+s\choose n}{r+s\over tn+r+s}\,.}\eqno\eqref|t-b2|\cr
\noalign{\smallskip}
&\setmathsize{\sum_k{n\choose k}(tk+r)^k\bigl(tn-tk+s\bigr)^{n-k}\,{r\over tk+r}
=(tn+r+s)^n\,.}\copy\mathsizebox\eqno\eqref|t-e1|\cr
&\sum_k{n\choose k}(tk+r)^k\bigl(tn-tk+s\bigr)^{n-k}
 \,{r\over tk+r}\cdot{s\over tn-tk+s}\cr
&\mathsize{\hfill=(tn+r+s)^n{r+s\over tn+r+s}\,.}\eqno\eqref|t-e2|\cr
\enddisplay
\hrule width\hsize height.5pt
\kern4pt
\endinsert

We have learned that it's generally a good idea to look at special cases
of general results. What happens, for example, if we set $t=1$?
The generalized binomial $\Bscr_1(z)$ is very simple\dash---it's just
\begindisplay
\Bscr_1(z)=\sum_{k\ge0}z^k={1\over 1-z}\,;
\enddisplay
therefore $\Bscr_1(z)$ doesn't give us anything we didn't already know from
Vandermonde's convolution. But $\Escr_1(z)$ is an important function,
\begindisplay
\Escr(z)=\sum_{k\ge0}(k+1)^{k-1}{z^k\over k!}=1+z+{3\over2}z^2+{8\over3}z^3
+{125\over24}z^4+\cdots
\eqno
\enddisplay
which we haven't seen before; it satisfies the basic identity
\g Aha! This is the iterated power function\vskip-3pt
$\Escr(\ln z)=z^{z^{z^{.^{.^.}}}}$\par
that I've often wondered about.\bigskip
\gmathlet Z\mathlet z\gmathlet z\mathsublet z\gmathsublet z\mathsubsublet z\dots\g
\begindisplay
\Escr(z)=e^{z\Escr(z)}\,.
\eqno
\enddisplay
This function, first studied by "Euler" [|euler-lambert|] and
"Eisenstein" [|eisenstein-exp|], arises in a great many applications
[|knoebel|, |bgc|].

The special cases $t=2$ and $t=-1$ of the generalized binomial are
of particular interest, because their coefficients occur again and
again in problems that have a recursive structure. Therefore it's
useful to display these series explicitly for future reference:
\g The power series for $\Bscr_{1/2_{\mathstrut}}(z)^r
=(\sqrt{z^{2\mathstrut}+4}+z)^{2r}\!/4^r$
is noteworthy too.\g
\begindisplay \openup3pt
\Bscr_2(z)&=\sum_k{2k\choose k}{z^k\over 1+k}\cr
 &=\sum_k{2k+1\choose k}{z^k\over 1+2k}={1-\sqrt{@1^{\mathstrut}-4z}\over2z}\,.
\eqno\eqref|cat|\cr
\noalign{\smallskip}
\Bscr_{-1}(z)&=\sum_k{1-k\choose k}{z^k\over 1-k}\cr
 &=\sum_k{2k-1\choose k}{(-z)^k\over 1-2k}=
 {1+\sqrt{@1^{\mathstrut}+4z}\over2}\,.
\eqno\eqref|cat+-|\cr
\noalign{\smallskip}
\Bscr_2(z)^r&=\sum_k{2k+r\choose k}{r\over 2k+r}\,z^k\,.
\eqno\eqref|cat:r|\cr
\Bscr_{-1}(z)^r&=\sum_k{r-k\choose k}{r\over r-k}\,z^k\,.
\eqno\eqref|cat+-:r|\cr
\noalign{\smallskip}
{\Bscr_2(z)^r\over\strutsqrt{@1-4z}}&=\sum_k{2k+r\choose k}z^k\,.
\eqno\eqref|cat:mod-r|\cr
\noalign{\smallskip}
{\Bscr_{-1}(z)^{r+1}\over\strutsqrt{@1+4z}}&=\sum_k{r-k\choose k}z^k\,.
\eqno\eqref|cat+-:mod-r|\cr
\enddisplay
The coefficients ${2n\choose n}{1\over{n+1}}$ of $\Bscr_2(z)$ are called
"!Catalan numbers, table of identities"
the {\it "Catalan numbers"\/}~$C_n$, because Eug\`ene "Catalan"
wrote an influential paper about them in the 1830s [|catalan-paper|].
The sequence begins as follows:
\begindisplay \let\preamble=\tablepreamble
n&&0&1&2&3&4&5&6&7&8&9&10\cr
\noalign{\hrule}
C_n&& 1 & 1 & 2 & 5 & 14 & 42 & 132 & 429 & 1430 & 4862 & 16796\cr
\enddisplay
The coefficients of $\Bscr_{-1}(z)$ are essentially the same, but there's
an extra $1$ at the beginning and the other numbers alternate in
sign: {\thinmuskip=3mu minus .5mu$\langle 1,1,-1,2,-5,14,\ldots\,\rangle$}.
Thus $\Bscr_{-1}(z)=
1+z\Bscr_2(-z)$. We also have $\Bscr_{-1}(z)=\Bscr_2(-z)^{-1}$.

Let's close this section by deriving an important consequence of
\eq(|cat:mod-r|) and \eq(|cat+-:mod-r|), a relation that shows further
connections between the functions $\Bscr_{-1}(z)$ and $\Bscr_2(-z)$:
\begindisplay
{\Bscr_{-1}(z)^{n+1}-(-z)^{n+1}\Bscr_2(-z)^{n+1}\over\strutsqrt{@1+4z}}
=\sum_{k\le n}{n-k\choose k}z^k\,.
\enddisplay
\noindent
This holds because the coefficient of $z^k$ in $(-z)^{n+1}\Bscr_2(-z)^{n+1}\!/
\sqrt{@1^{\mathstrut}+4z}\,$ is
\begindisplay \openup4pt
[z^k]\,{(-z)^{n+1}\Bscr_2(-z)^{n+1}\over\strutsqrt{@1+4z}}
&=(-1)^{n+1}[z^{k-n-1}]\,{\Bscr_2(-z)^{n+1}\over\strutsqrt{@1+4z}}\cr
&=(-1)^{n+1}(-1)^{k-n-1}[z^{k-n-1}]\,{\Bscr_2(z)^{n+1}
  \over\strutsqrt{@1-4z}}\cr
&=(-1)^k{2(k-n-1)+n+1\choose k-n-1}\cr
&=(-1)^k{2k-n-1\choose k-n-1}=(-1)^k{2k-n-1\choose k}\cr
&={n-k\choose k} =[z^k]\,{\Bscr_{-1}(z)^{n+1}\over\strutsqrt{@1+4z}}
\enddisplay
when $k>n$. The terms nicely cancel each other out.
We can now use \eq(|cat|) and \eq(|cat+-|) to obtain the closed form
\begindisplay \openup5pt
&\sum_{k\le n}{n-k\choose k}z^k={1\over\strutsqrt{@1{+}4z}}\biggl(
\biggl({1+\sqrt{@1^{\mathstrut}{+}4z}\over2}@\biggr)^{\!n+1}\!\!-\;
\biggl({1-\sqrt{@1^{\mathstrut}{+}4z}\over2}@\biggr)^{\!n+1}\biggr)\,,\cr
&\hskip20em\hbox{integer $n\ge0$.}
\eqno\eqref|bc-gen-fib|
\enddisplay
(The special case $z=-1$ came up in Problem 3 of Section 5.2. Since
the numbers $\half(1\pm\sqrt{-3}\,)$ are sixth roots of unity, the sums
$\sum_{k\le n}{n-k\choose k}(-1)^k$ have the periodic behavior we observed
in that problem.)
Similarly we can combine \eq(|cat:r|) with \eq(|cat+-:r|) to cancel
the large coefficients and get
\begindisplay
&\sum_{k<n}{n-k\choose k}{n\over n-k}\,z^k=
 \biggl({1+\sqrt{@1^{\mathstrut}{+}4z}\over2}@\biggr)^{\!n}+
 \biggl({1-\sqrt{@1^{\mathstrut}{+}4z}\over2}@\biggr)^{\!n},\cr
&\hskip20em\hbox{integer $n>0$.}
\eqno\eqref|bc-gen-luc|
\enddisplay

\beginsection 5.5 Hypergeometric Functions

\looseness=-1
The methods we've been applying to binomial coefficients are very
effective, when they work, but we must admit that they often appear
to be ad~hoc\dash---more like tricks than techniques.
When we're working on a problem,
we often have many directions to pursue, and we might find ourselves
 \g They're even more versatile than chameleons;
we can dissect them and put them back together in different~ways.\g
going around in circles.
Binomial coefficients are like chameleons, changing their
appearance easily.
 Therefore it's natural
to ask if there isn't some unifying principle that will systematically handle a
great variety of binomial coefficient summations all at once. Fortunately,
the answer is yes. The unifying principle is based on the theory
of certain infinite sums called {\it"hypergeometric series"}.

The study of hypergeometric series was launched many years ago by
"Euler", "Gauss", and "Riemann"; such series, in fact, are still the subject of
considerable research. But hypergeometrics have
a somewhat formidable notation, which takes a little time to get used to.%
\g Anything that has survived for centuries with such awesome
notation must be really useful.\g

The general hypergeometric series is a power series in~$z$ with
$m+n$ parameters, and it is defined as follows in terms of
rising factorial powers:
\tabref|nn:hyp|%
\begindisplay
\hyp{a_1,\, \ldots,\, a_m}{b_1,\, \ldots,\, b_n}z
	= \sum_{k \geq 0}
 {a_1\_^k\ldots a_m\_^k \over b_1\_^k\ldots b_n\_^k}
	\,{z^k\over k!\hypstrut} \,.
\eqno\eqref|hyp-def|
\enddisplay
To avoid division by zero, none of the $b$'s may be zero or a negative
integer. Other than that, the $a$'s and $b$'s may be anything we like.
The notation `$F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)$' is also used as
an alternative to the two-line form \thiseq, since a one-line form
sometimes works better typographically. The $a$'s are said to be {\it"upper
parameters"\/}; they occur
in the numerator of the terms of~$F$. The $b$'s are
{\it"lower parameters"}, and they occur in the denominator.
The final quantity~$z$ is called the {\it"argument"}.

Standard reference books often use `$_mF_n$' instead of~`$F$' as the
name of a hypergeometric with $m$ upper parameters and $n$~lower
parameters. But the extra subscripts tend to clutter up the formulas
and waste our time, if we're compelled to
write them over and over. We can count how
many parameters there are, so we usually don't need extra additional
unnecessary redundancy.

Many important functions occur as special cases of the general
hypergeometric; indeed, that's why hypergeometrics are so powerful.
For example, the simplest case occurs when $m=n=0$: There are no
parameters at all, and we get the familiar series
\begindisplay
\hyp\ {}z=\sum_{k\ge0}{z^k\over k!}=e^z\,.
\enddisplay
Actually the notation looks a bit unsettling when $m$ or $n$ is zero.
We can add an extra `$1$' above and below in order to avoid this:
\begindisplay
\hyp 11z=e^z\,.
\enddisplay
In general we don't change the function if we cancel a parameter
 that occurs in both
numerator and denominator, or if we insert two identical parameters.

The next simplest case has $m=1$, $a_1=1$, and $n=0$; we change the parameters
to $m=2$, $\,a_1=a_2=1$, $\,n=1$, and $b_1=1$, so that $n>0$. This series
also turns out to be familiar, because $1\_^k=k!$:
\begindisplay
\hyp{1,1}1z=\sum_{k\ge0}z^k={1\over 1-z}\,.
\enddisplay
It's our old friend,
the geometric series; $F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)$
is called hypergeometric because it includes the geometric series $F(1,1;1;z)$
as a very special case.

The general case $m=1$ and $n=0$ is, in fact, easy to sum in closed form,
\begindisplay
\hyp{a,\,1}1z=\sum_{k\ge0}a\_^k\,{z^k\over k!}
=\sum_k{a+k-1\choose k}z^k={1\over(1-z)^a}\,,
\eqno\eqref|1F0|
\enddisplay
using \eq(|neg-binomial1|).
If we replace $a$ by $-a$ and $z$ by $-z$, we get the "binomial theorem",
\begindisplay
\hyp{-a,\,1\,}{\,1}{-z}=(1+z)^a\,.
\enddisplay
A negative integer as upper parameter causes the infinite series to become
finite, since $(-a)\_^k=0$ whenever $k>a\ge0$ and $a$ is an integer.

The general case $m=0$, $n=1$ is another famous series, but it's not as
well known in the literature of discrete mathematics:
\begindisplay
\hyp1{b,1}z=\sum_{k\ge0}{(b-1)!\over(b-1+k)!}\,{z^k\over k!}
 =I_{b-1}\bigl(2\sqrt z\,\bigr){(b-1)!\over z^{(b-1)/2}}\,.
\eqno\eqref|0F1|
\enddisplay
This function $I_{b-1}$ is called a ``modified "Bessel function"'' of order~$b-1$.
The special case $b=1$ gives us $\hyp1{1,1}z=I_0(2\sqrt z@)$,
 which is the interesting series $\sum_{k\ge0}z^k\!/k!^2$.

The special case $m=n=1$ is called a ``confluent hypergeometric series''
and often denoted by the letter~$M$:
\begindisplay
\hyp abz=\sum_{k\ge0}{a\_^k\over b\_^k}\,{z^k\over k!^{\phantom{\overline k}}}
 =M(a,b,z)\,.
\eqno\eqref|1F1|
\enddisplay
This function, which has important applications to engineering, was
introduced by Ernst "Kummer".

By now a few of us are wondering why we haven't discussed "convergence"
of the infinite series \eq(|hyp-def|). The answer is that we can
ignore convergence if we are using $z$ simply as a formal symbol.
It is not difficult to verify that formal infinite sums of the form $\sum_{k\ge n}
"!formal power series"
\alpha_kz^k$ form a field, if the coefficients $\alpha_k$ lie in a
field. We can add, subtract, multiply, divide, differentiate, and
do functional composition on such formal sums without worrying about
convergence; any identities we derive will still be formally true.
For example, the hypergeometric $\hyp{1,1,1}1z=\sum_{k\ge0}k!\,z^k$
doesn't converge for any nonzero~$z$; yet we'll see in Chapter~7 that
we can still use it to solve problems. On the other hand, whenever we
replace $z$ by a particular numerical value, we do have to be sure
that the infinite sum is well defined.

The next step up in complication is actually the most famous hypergeometric
of all. In fact, it was {\it the\/} hypergeometric series until about
1870, when everything was generalized to arbitrary $m$ and~$n$.
This one has two upper parameters and one lower parameter:
\begindisplay
\hyp{a,b}cz=\sum_{k\ge0}{a\_^k b\_^k \,z^k\over c\_^k\, k!}\,.
\eqno
\enddisplay
It is often called the Gaussian hypergeometric, because many of its subtle
\g\noindent\llap{``}There must be many
universities to-day where 95 per cent, if~not 100 per cent,
of the functions studied by physics, engineering, and even
mathematics students, are covered by this single symbol
$F(a,b;c;x)$.''\par
\hskip0pt plus1fill minus 3pt\hbox{\dash---}%
W\kern-.8pt.\thinspace W\kern-.8pt.\thinspace Sawyer\thinspace[|sawyer|]\g
properties were first proved by "Gauss" in his doctoral dissertation
of 1812~[|gauss-hyp|], although "Euler"~[|euler-hyp|] and "Pfaff"~[|pfaff|]
had already discovered some remarkable things about it.
One of its important special cases is
\begindisplay \openup3pt
\ln(1+z)=z\hyp{1,1\,}2{-z}
&=z\,\sum_{k\ge0}{k!\,k!\over(k+1)!}\,{(-z)^k\over k!}\cr
&=z-{z^2\over2}+{z^3\over3}-{z^4\over4}+\cdots\,.
\enddisplay
Notice that $z^{-1}\ln(1+z)$ is a hypergeometric function,
but $\ln(1+z)$ itself cannot
be hypergeometric, since a hypergeometric series always has the
value~$1$ when $z=0$.

So far hypergeometrics haven't actually done anything for us except
provide an excuse for name-dropping. But we've seen that several very different
functions can all be regarded as hypergeometric; this will be the main
point of interest in what follows.
We'll see that a large class of sums can be
written as hypergeometric series in a ``canonical'' way, hence we will
have a good filing system for facts about binomial coefficients.

What series are hypergeometric? It's easy to answer this question if we
"!term ratio"
look at the ratio between consecutive terms:
\begindisplay
\hyp{a_1,\ldots,a_m}{b_1,\ldots,b_n}z=\sum_{k\ge0}t_k,\qquad
 t_k={a_1\_^k\ldots a_m\_^k\,z^k\over b_1\_^k\ldots b_n\_^k\,k!}\,.
\enddisplay
The first term is $t_0=1$, and the other terms have ratios given by
\begindisplay \openup3pt
{t_{k+1}\over t_k}&=
{a_1\_^{k+1}\ldots a_m\_^{k+1}\over a_1\_^k\ldots a_m\_^k\hypstrut}
\,{b_1\_^k\ldots b_n\_^k\over b_1\_^{k+1}\ldots b_n\_^{k+1}\hypstrut}
\,{k!\over (k+1)!\hypstrut}
\,{z^{k+1}\over z^k\hypstrut}\cr
&={(k+a_1)\ldots(k+a_m)\,z\over(k+b_1)\ldots(k+b_n)(k+1)}\,.
\eqno\eqref|t-ratio|
\enddisplay
This is a {\it"rational function"\/} of $k$, that is, a quotient of polynomials
in~$k$.
According to the "Fundamental Theorem of Algebra",
any rational function of~$k$ can be factored over the
complex numbers and put into this form. The $a$'s are the negatives of the
roots of the polynomial in the numerator, and the $b$'s are the negatives
of the roots of the polynomial in the denominator. If the denominator
doesn't already contain the special factor $(k+1)$, we can include
$(k+1)$ in both numerator and denominator. A constant factor remains,
and we can call it~$z$.
 Therefore hypergeometric series
are precisely those series whose first term is~$1$ and whose term
ratio $t_{k+1}/t_k$ is a rational function of~$k$.

Suppose, for example, that we're given an infinite series with term ratio
\begindisplay
{t_{k+1}\over t_k}={k^2+7k+10\over 4k^2+1}\,,
\enddisplay
a rational function of $k$. The numerator polynomial splits nicely into
two factors,
$(k+2)(k+5)$, and the denominator is $4(k+i/2)(k-i/2)$. Since the denominator
is missing the required factor $(k+1)$, we write the term ratio as
\begindisplay
{t_{k+1}\over t_k}={(k+2)(k+5)(k+1)(1/4)\over(k+i/2)(k-i/2)(k+1)}\,,
\enddisplay
and we can read off the results: The given series is
\begindisplay
\sum_{k\ge0}t_k=t_0\,\hyp{2,\,5,\,1}{i/2,-i/2\,}{1/4}\,.
\enddisplay

Thus, we have a general method for finding the hypergeometric representation
of a given quantity~$S$, when such a representation is possible:
First we write~$S$ as an infinite series whose first term
is nonzero. We choose a notation so that the series is $\sum_{k\ge0}t_k$
\g (Now is a good time to do warmup exercise~|sine-arcsine|.)\g
with $t_0\ne0$. Then we calculate $t_{k+1}/t_k$. If the term ratio is
not a rational function of~$k$, we're out of luck. Otherwise
we express it in the form \eq(|t-ratio|); this gives parameters
$a_1$, \dots,~$a_m$, $b_1$,~\dots,~$b_n$, and an argument~$z$, such that
$S=t_0\,F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)$.

Gauss's hypergeometric series can be written in the recursively factored form
\begindisplay \medmuskip=2mu
\hyp{a,b}cz=1\,+\,{a\over1}{b\over c}z\left(1\,+\,{a+1\over2}{b+1\over c+1}z
 \left(1\,+\,{a+2\over3}{b+2\over c+2}z\left(1\,+\,\cdots\,\right)\right)
 \mskip-1mu\right)
\enddisplay
if we wish to emphasize the importance of term ratios.

Let's try now to reformulate the binomial coefficient identities derived
earlier in this chapter, expressing them as hypergeometrics.
For example, let's figure out what the "parallel summation law",
\begindisplay
\sum_{k\le n}{r+k\choose k}={r+n+1\choose n}\,,\qquad\hbox{integer $n$},
\enddisplay
looks like in hypergeometric notation. We need to write the sum
as an infinite series that starts at $k=0$, so we replace $k$ by $n-k$:
\begindisplay
\sum_{k\ge0}{r+n-k\choose n-k}
 =\sum_{k\ge0}{(r+n-k)!\over r!\,(n-k)!}=\sum_{k\ge0}t_k\,.
\enddisplay
This series is formally infinite but actually finite, because the $(n-k)!$
in the denominator will make $t_k=0$ when $k>n$.
(We'll see later that $1/x!$ is defined for all~$x$, and that $1/x!=0$
when $x$ is a negative integer. But for now, let's blithely disregard
such technicalities until we gain more hypergeometric experience.)
 The term ratio is
\begindisplay
{t_{k+1}\over t_k}={(r+n-k-1)!\,r!\,(n-k)!\over r!\,(n-k-1)!\,(r+n-k)!}
&={n-k\over r+n-k}\cr
&={(k+1)(k-n)(1)\over(k-n-r)(k+1)}\,.
\enddisplay
Furthermore $t_0={r+n\choose n}$. Hence the parallel summation law
is equivalent to the hypergeometric identity
\begindisplay
{r+n\choose n}\hyp{1,\,-n}{-n-r}1={r+n+1\choose n}\,.
\enddisplay
Dividing through by ${r+n\choose n}$ gives a slightly simpler version,
\begindisplay
\hyp{1,\,-n}{-n-r}1={r+n+1\over r+1}\,,
 \qquad\hbox{if $\displaystyle{r+n\choose n}\ne0$}.
\eqno\eqref|Fa1c|
\enddisplay

Let's do another one.
The term ratio of identity \eq(|bc-alt-sum|),
\begindisplay
\sum_{k \leq m} {r \choose k} (-1)^k
	&= (-1)^m {r-1 \choose m} \,, \qquad\hbox{integer $m$},
\enddisplay
is $(k-m)/(r-m+k+1)=(k+1)(k-m)(1)/(k-m+r+1)(k+1)$,
 after we replace $k$ by $m-k$;
 hence \eq(|bc-alt-sum|) gives a closed form for
\begindisplay
\hyp{1,\,-m}{-m+r+1}1\,.
\enddisplay
This is essentially the same as the hypergeometric function on the left
of \thiseq,
but with $m$ in place
of~$n$ and $r+1$ in place of $-r$. Therefore identity \eq(|bc-alt-sum|)
could have been derived from~\thiseq, the hypergeometric version of
\eq(|bc-sum-both|).
(No wonder we found it easy to
prove \eq(|bc-alt-sum|) by using \eq(|bc-sum-both|).)

Before we go further, we should think about degenerate cases,
\g First derangements, now degenerates.\g
because hypergeometrics are not defined when a lower parameter is zero
or a negative integer. We usually apply the parallel summation identity
when $r$ and~$n$ are positive integers; but then $-n-r$ is a
negative integer and the hypergeometric \eq(|hyp-def|) is undefined.
How then can we consider \eq(|Fa1c|) to be legitimate? The answer
is that we can take the limit of $\hyp{1,\,-n}{-n-r+\epsilon}1$ as
$\epsilon\to0$.

We will look at such things more closely later in this chapter, but for
now let's just be aware that some denominators can be dynamite.
It is interesting, however, that the very first
\g(We proved the identities originally for integer\/~$r$, and
used the polynomial argument to show that they hold in general.
Now we're proving them first for irrational\/~$r$, and using a limiting
argument to show that they hold for integers!)\g
"!degenerate hypergeometric"
sum we've tried to express hypergeometrically has turned out to be
degenerate.

Another possibly sore point in our derivation of
\thiseq\ is that we expanded ${r+n-k\choose n-k}$
as $(r+n-k)!@/@r!\,(n-k)!$.
This expansion fails when $r$ is a negative integer,
because $(-m)!$ has to be $\infty$ if the law
\begindisplay
0!=0\cdot(-1)\cdot(-2)\cdot\ldots\cdot(-m+1)\cdot(-m)!
\enddisplay
is going to hold. Again, we need to approach integer results by
considering a limit of $r+\epsilon$ as $\epsilon\to0$.

But we defined the factorial representation ${r\choose k}=r!@/@
k!\,(r-k)!$
only when $r$ is an integer! If we want to work effectively with
hypergeometrics, we need a factorial function that is
defined for all complex numbers. Fortunately there is such a
function, and it can be defined in many ways. Here's one of
the most useful definitions of $z!$, actually a definition of $1/z!@$:
\begindisplay
{1\over z!}=\lim_{n\to\infty}{n+z\choose n}n^{-z}\,.
\eqno\eqref|f-def-lim|
\enddisplay
(See exercise |factorial-def|.
"Euler" [|euler-factorial|, |euler-gamma|, |davis|]
discovered this when he was 22 years old.)
"!factorial, generalized"
The limit can be
shown to exist for all complex~$z$, and it is zero only when $z$~is
a negative integer. Another significant definition is
\begindisplay
z!=\int_0^\infty t^ze^{-t}\,dt\,,\qquad\hbox{ if $\Re z>-1$}.
\eqno\eqref|f-def-int|
\enddisplay
This integral exists only when the real part of $z$ exceeds~$-1$,
but we can use the formula
\begindisplay
z!=z\,(z-1)!
\eqno\eqref|f-rec|
\enddisplay
to extend the definition to all complex $z$ (except negative integers).
Still another definition comes from "Stirling"'s interpolation of
$\ln z!$ in \eq(|stirling-try2|). All of these approaches lead to
the same generalized factorial function.

There's a very similar function called the {\it"Gamma function"}, which relates
to ordinary factorials somewhat as rising powers relate to
falling powers. Standard reference books often use factorials and
Gamma functions simultaneously, and it's convenient to convert
between them if necessary using the following formulas:
\begindisplay
\Gamma(z+1)&=z!\,;\eqno\eqref|gamma-f|\cr
\noalign{\smallskip}
(-z)!\,\,\Gamma(z)&={\pi\over\sin\pi z}\,.\eqno\eqref|gamma--f|\cr
\enddisplay

We can use these generalized factorials to
\g How do you write $z$~to the $\overline w$ power,
when $\overline w$ is the complex conjugate of~$w$?\bigskip
$z^{(\overline w)}$.\g
define generalized factorial powers, when $z$ and~$w$ are
"!factorial powers, complex"
arbitrary complex numbers:
\tabref|nn:gfall|%
\tabref|nn:grise|%
\begindisplay \openup4pt
z\_w&={z!\over(z-w)!}\,;
\eqno\eqref|gen-falling-powers|\cr
z\_^w&={\Gamma(z+w)\over\Gamma(z)}\,.
\eqno\eqref|gen-rising-powers|\cr
\enddisplay
The only proviso is that we must use appropriate limiting values
when these formulas give $\infty/\infty$. (The formulas never give
$0/0$, because factorials and Gamma-function values are never zero.)
A "binomial coefficient" can be written
\begindisplay
{z\choose w}=\lim_{\zeta\to z}\,\lim_{\omega\to w}\,{\zeta@!\over\omega!\,
 (\zeta-\omega)!}\eqno
\enddisplay
when $z$ and $w$ are any complex numbers whatever.
\g I see, the lower index arrives at its limit first.
That's why $z\choose w$ is zero when $w$ is a~negative integer.\g

Armed with generalized factorial tools, we can return to our
goal of reducing the identities derived earlier to their
hypergeometric essences. The binomial theorem \eq(|bin-thm-z|) turns
out to be neither more nor less than \eq(|1F0|), as we might expect.
So the next most interesting identity to try is "Vandermonde's
convolution" \eq(|vandermonde-conv|):
\begindisplay
\sum_k{r\choose k}{s\choose n-k}={r+s\choose n},\qquad\hbox{integer $n$}.
\enddisplay
The $k$th term here is
\begindisplay
t_k={r@!\over(r-k)!\,k!}\,{s!\over(s-n+k)!\,(n-k)!}\,,
\enddisplay
and we are no longer too shy to use generalized factorials in these
expressions. Whenever $t_k$ contains a factor like $(\alpha+k)!$,
with a plus sign before the~$k$,
we get $(\alpha+k+1)!/(\alpha+k)!=k+\alpha+1$ in the term ratio
$t_{k+1}/t_k$, by \eq(|f-rec|); this contributes the parameter
`$\alpha+1$' to the corresponding hypergeometric\dash---as an upper
parameter if $(\alpha+k)!$ was in the numerator of~$t_k$, but as
a lower parameter otherwise. Similarly, a
factor like $(\alpha-k)!$ leads to $(\alpha-k-1)!/(\alpha-k)!=(-1)/(k-\alpha)$;
this contributes `$-\alpha$' to the opposite set of parameters (reversing
the roles of upper and lower),
 and negates the hypergeometric argument.
Factors like $r!$, which are independent of~$k$, go
 into~$t_0$ but disappear
from the term ratio. Using
such tricks we can predict without further calculation that
the term ratio of \eq(|vandermonde-conv|) is
\begindisplay
{t_{k+1}\over t_k}={k-r\over k+1}\,{k-n\over k+s-n+1}
\enddisplay
times $(-1)^2=1$, and Vandermonde's convolution becomes
\begindisplay
{s\choose n}\hyp{-r,\,-n}{s-n+1}1={r+s\choose n}\,.
\eqno
\enddisplay
We can use this equation to determine
$F(a,b;c;z)$ in general, when $z=1$ and when
$b$~is a negative integer.

Let's rewrite \thiseq\ in a form so that table lookup is easy
when a new sum needs to be evaluated. The result turns out to be
\begindisplay
\hyp{a,b}c1={\Gamma(c-a-b)\,\Gamma(c)\over\Gamma(c-a)\,\Gamma(c-b)}\,;
\qquad\tworestrictions{integer $b\le0$}{or $\Re c>\Re a+\Re b$.}
\eqno\eqref|Fabc|
\enddisplay
Vandermonde's convolution \eq(|vandermonde-conv|) covers only the case that
one of the upper parameters, say~$b$, is a nonpositive integer;
\g A few weeks ago, we were studying what Gauss had done in kindergarten.\par
Now we're studying stuff beyond his Ph.D. thesis.\par Is~this intimidating
or what?\g
but "Gauss" proved that \thiseq\ is valid also when $a$, $b$,~$c$ are
complex numbers whose real parts satisfy $\Re c>\Re a+\Re b$. In
other cases, the infinite series $\hyp{a,b}c1$ doesn't converge.
When $b=-n$, the identity can be written more conveniently with
factorial powers instead of Gamma functions:
\begindisplay
\hyp{a,\,-n\,}c1={(c-a)\_^n\over c\_^n\hypstrut}
 ={(a-c)\_n\over(-c)\_n\hypstrut}\,,
\qquad\hbox{integer $n\ge0$}.
\eqno\eqref|Fa-nc|
\enddisplay
It turns out that all five of the identities in Table~|bc-prod|
are special cases of Vandermonde's convolution; formula \thiseq\
covers them all, when proper attention is paid to degenerate situations.

Notice that \eq(|Fa1c|) is just the special case $a=1$ of \thiseq.
Therefore we don't
really need to remember \eq(|Fa1c|); and we don't really need the identity
\eq(|bc-sum-both|) that led us to \eq(|Fa1c|), even though Table~|bc-tops|
said that it was memorable. A computer program for formula manipulation,
faced with the problem of evaluating $\sum_{k\le n}{r+k\choose k}$,
could convert the sum to a hypergeometric and plug~into the
general identity for Vandermonde's convolution.

Problem 1 in Section 5.2 asked for the value of
\begindisplay
\sum_{k\ge0}{m\choose k}\bigg/{n\choose k}\,.
\enddisplay
 This problem is a natural for hypergeometrics,
and after a bit of practice any hyper\-geometer can read off
the parameters immediately as $F(1,-m;-n;1)$. Hmmm; that problem was
yet another special takeoff on Vandermonde!

The sum in Problem 2 and Problem 4 likewise yields $F(2,1-n;2-m;1)$.
(We need to replace $k$ by $k+1$ first.) And the ``menacing''
sum in Problem~6 turns out to be just $F(n+1,-n;2;1)$. Is there
nothing more to sum, besides disguised versions of Vandermonde's
powerful convolution?

Well, yes, Problem 3 is a bit different. It deals with a special
case of the general sum $\sum_k{n-k\choose k}z^k$ considered in
\eq(|bc-gen-fib|), and this leads to a closed-form expression for
\begindisplay
\hyp{1+2\lceil n/2\rceil,\,-n\,}{1/2}{-z/4}\,.
\enddisplay

We also proved something new in \eq(|alt-convolution|), when we looked
at the coefficients of $(1-z)^r(1+z)^r$:
\begindisplay
\hyp{1{-}c{-}2n,\,-2n}c{-1}=(-1)^n{(2n)!\over n!}{(c-1)!\over(c+n-1)!}\,,
\qquad\hbox{integer $n\ge0$}.
\enddisplay
\g Kummer was a summer.\g
This is called {\it "Kummer's formula"\/} when it's generalized to
complex numbers:
\begindisplay
\hyp{a,\,b}{1+b-a}{-1}={(b/2)!\over b!}(b-a)\_{b/2}\,.
\eqno\eqref|hyp-kummer|
\enddisplay
\g The summer of '36.\g
(Ernst Kummer [|kummer|] proved this in 1836.)

It's interesting to compare these two formulas. Replacing $c$ by $1-2n-a$,
we find that the results are consistent if and only if
\begindisplay
(-1)^n{(2n)!\over n!}=\lim_{b\to-2n}{(b/2)!\over b!}=\lim_{x\to-n}{x!\over
(2x)!}\eqno
\enddisplay
when $n$ is a positive integer. Suppose, for example, that $n=3$; then
we should have $-6!/3!=\lim_{x\to-3}x!/(2x)!$. We know that $(-3)!$ and~%
$(-6)!$ are both infinite; but we might choose to ignore that difficulty
and to imagine that $(-3)!=(-3)(-4)(-5)(-6)!$, so that the two occurrences
of $(-6)!$ will cancel. Such temptations must, however, be resisted, because
they lead to the wrong answer! The limit of $x!/(2x)!$ as $x\to-3$ is
not $(-3)(-4)(-5)$ but rather $-@6!/3!=(-4)(-5)(-6)$, according to~\thiseq.

The right way to evaluate the limit in \thiseq\ is to use equation
\eq(|gamma--f|), which relates negative-argument factorials to positive-argument
"Gamma function"s. "!factorial, generalized"
If we replace $x$ by $-n-\epsilon$ and let $\epsilon\to0$, two applications
of \eq(|gamma--f|) give
\begindisplay
{(-n-\epsilon)!\over(-2n-2\epsilon)!}\,
 {\Gamma(n+\epsilon)\over\Gamma(2n+2\epsilon)}
 = {\sin(2n+2\epsilon)\pi\over\sin(n+\epsilon)\pi}\,.
\enddisplay
Now $\sin(x+y)=\sin x\cos y+\cos x\sin y$; so this ratio of sines is
\begindisplay
{\cos2n\pi\,\sin2\epsilon\pi\over\cos n\pi\,\sin\epsilon\pi}=
 (-1)^n\bigl(2+O(\epsilon)\bigr)\,,
\enddisplay
by the methods of Chapter 9. Therefore, by \eq(|gamma-f|), we have
\begindisplay
\lim_{\epsilon\to0}{(-n-\epsilon)!\over(-2n-2\epsilon)!}
=2(-1)^n{\Gamma(2n)\over\Gamma(n)}=2(-1)^n{(2n-1)!\over(n-1)!}=(-1)^n
{(2n)!\over n!}\,,
\enddisplay
as desired.

Let's complete our survey by restating the other identities we've seen
so far in this chapter,
clothing them in hypergeometric garb.
The triple-binomial sum in \eq(|bc-dixon|) can be written
\begindisplay \openup3pt
&\hyp{1-a-2n,\,1-b-2n,\,-2n}{a,\,b}1\cr
&\qquad=(-1)^n{(2n)!\over n!}
 {(a+b+2n-1)\_^n\over a\_^n\,b\_^n}\,,\qquad\hbox{integer $n\ge0$}.
\enddisplay
When this one is generalized to complex numbers, it is called
{\it "Dixon's formula"\/}:
\begindisplay
\hyp{a,\,b,\,c}{1+c-a,\,1+c-b}1
&={(c/2)!\over c!}\,{(c-a)\_{c/2}\,(c-b)\_{c/2}
 \over(c-a-b)\_{c/2}}\,,
\eqno\eqref|hyp-dixon|\cr
\noalign{\nobreak\smallskip}
&\hskip8em\hbox{$\Re a+\Re b<1+\Re c/2$}.
\enddisplay

One of the most general formulas we've encountered
is the triple-binomial sum
\eq(|bc-saalschutz|), which yields {\it "Saalsch\"utz"'s identity\/}:
\g(Historical note: Saalsch\"utz~[|saalschutz|] independently discovered
this formula almost 100 years after Pfaff~[|pfaff|] had first published it.
Taking the limit as $n\to\infty$ yields equation \eq(|Fabc|).)\g
\begindisplay \openup3pt
\hyp{a,\,b,\,-n}{c,\,a+b-c-n+1}1
&={(c-a)\_^n\,(c-b)\_^n\over c\_^n\,(c-a-b)\_^n}
\eqno\eqref|hyp-saalschutz|\cr
&={(a-c)\_n\,(b-c)\_n\over (-c)\_n\,(a+b-c)\_n}\,,
\qquad\hbox{integer $n\ge0$}.
\enddisplay
This formula gives the value at $z=1$ of
the general hypergeometric series with three upper parameters
and two lower parameters, provided that
one of the upper parameters is a nonpositive integer and that
$b_1+b_2=a_1+a_2+a_3+1$. (If the sum of the lower parameters
exceeds the sum of the upper parameters by~$2$ instead of by~$1$,
the formula of exercise~|increase-denominator| can be used to express
$F(a_1,a_2,a_3;b_1,b_2;1)$ in terms of two hypergeometrics that
satisfy Saalsch\"utz's identity.)

Our hard-won identity in Problem 8 of Section 5.2 reduces to
\begindisplay
{1\over1+x}\hyp{x+1,\,n+1,\,-n\,}{1,\,x+2}1=(-1)^nx\_n\,x\_{\smash{-n-1}}\,.
\enddisplay
Sigh. This is just the special case $c=1$ of Saalsch\"utz's identity \thiseq,
so we could have saved a lot of work by going to hypergeometrics
directly!

What about Problem 7? That extra-menacing sum gives us the formula
\begindisplay
\hyp{n+1,\,m-n,\,1,\,\half}{\half m+1,\,\half m+\half,\,2}1={m\over n}\,,
\enddisplay
which is the first case we've seen with three lower parameters.
So it looks new. But it really isn't;
the left-hand side can be replaced by
\begindisplay
\hyp{n,\,\,m-n-1,\,\,-\half}{\half m,\,\,\half m-\half}1\,-\, 1\,,
\enddisplay
using exercise~|omit-first-term|, and Saalsch\"utz's
identity wins again.

Well, that's another deflating experience,
but it's also another reason
\g(Historical note: The great relevance of hypergeometric series
to binomial coefficient identities was first pointed out by
George "Andrews" in 1974 [|andrews-siam|, section~5].)\g
to appreciate the power of hypergeometric methods.

The convolution identities in Table |conv-table| do not have
hypergeometric equivalents, because their term ratios are rational
functions of~$k$ only when $t$~is an integer. Equations \eq(|t-e1|) and
\eq(|t-e2|) aren't hypergeometric even when $t=1$. But we can
take note of what \eq(|t-b1|) tells us when $t$ has small
integer values:
\begindisplay \openup4pt
&\hyp{\half r,\,\half r+\half,\,-n,\,-n-s}{r+1,\,-n-\half s,\,-n-\half s+\half}1
&={r+s+2n\choose n}\bigg/{s+2n\choose n}\,;\cr
&\def\third{{1\over3}}
\hyp{{1\over3}r,\,\third r+\third,\,\third r+{2\over3},\,
  -n,\,-n-\half s,\,-n-\half s-\half}{\half r+\half,\,\half r+1,\,
  -n-\third s,\,-n-\third s+\third,\,-n-\third s+{2\over3}}1\hidewidth\cr
&&={r+s+3n\choose n}\bigg/{s+3n\choose n}\,.\cr
\enddisplay
The first of these formulas gives the result of Problem 7 again, when the
quantities $(r,s,n)$ are replaced respectively by $(1,2n+1-m,-1-n)$.

Finally, the ``unexpected'' sum \eq(|half/2|) gives us an unexpected
"!unexpected sum"
 hypergeometric
identity that turns out to be quite instructive. Let's look at it
in slow motion. First we convert to an infinite sum,
\begindisplay \advance\abovedisplayskip-.6pt\advance\belowdisplayskip-.6pt
\sum_{k\le m}{m+k\choose k}2^{-k}=2^m\quad\iff\quad
\sum_{k\ge 0}{2m-k\choose m-k}2^k=2^{2m}\,.
\enddisplay
The term ratio from $(2m-k)!\,2^k\!/m!\,(m-k)!$
 is $2(k-m)/(k-2m)$, so
we have a hypergeometric identity with $z=2$:
\begindisplay \advance\abovedisplayskip-.6pt\advance\belowdisplayskip-.6pt
{2m\choose m}\hyp{1,\,-m}{-2m}2=2^{2m}\,,\qquad\hbox{integer $m\ge0$}.
\eqno
\enddisplay
But look at the lower parameter `$-2m$'. Negative
integers are verboten, so this identity is undefined!

\goodbreak
It's high time to look at such limiting cases carefully, as promised
earlier, because degenerate hypergeometrics can often be evaluated
by approaching them from nearby nondegenerate points. We must be
careful when we do this, because different results can be obtained
if we take limits in different ways. For example,
here are two limits that turn out to be quite different when one
of the upper parameters is increased by~$\epsilon$:
\begindisplay \let\\=\epsilon
\lim_{\\\to0}\hyp{-1+\\,\,-3\,}{-2+\\}1
&=\lim_{\\\to0}\textstyle\bigl(1+
 {(-1+\\)(-3)\over(-2+\\)\,1!}+
 {(-1+\\)(\\)(-3)(-2)\over(-2+\\)(-1+\\)\,2!}\cr
&\hskip70pt\textstyle+
 {(-1+\\)(\\)(1+\\)(-3)(-2)(-1)\over(-2+\\)(-1+\\)(\\)\,3!}\bigr)\cr
\noalign{\smallskip}
&=\textstyle 1-{3\over2}+0+\half=0\,;\cr
\noalign{\smallskip}
\lim_{\\\to0}\hyp{-1,-3}{-2+\\}1
&=\lim_{\\\to0}\textstyle\bigl(1+
 {(-1)(-3)\over(-2+\\)\,1!}+0+0\bigr)\cr
&=\textstyle 1-{3\over2}+0+0=-\half\,.\cr
\enddisplay
Similarly, we have defined ${-1\mskip2mu\choose-1\mskip2mu}=0=\lim_{\epsilon\to0}
 {-1+\epsilon\choose-1}$; this is not the same as
$\lim_{\epsilon\to0}{-1+\epsilon\choose-1+\epsilon}=1$. The
proper way to treat \thiseq\ as a limit is to realize that
the upper parameter $-m$ is being used to make all terms of
the series $\sum_{k\ge0}
{2m-k\choose m-k}2^k$ zero for $k>m$; this means that we want
to make the following more precise statement:
\begindisplay
{2m\choose m}\,\lim_{\epsilon\to0}\,\hyp{1,\,-m}{-2m+\epsilon}2=
 2^{2m}\,,\qquad\hbox{integer $m\ge0$}.
\eqno\eqref|hyp-half/2|
\enddisplay
Each term of this limit is well defined, because the denominator
factor $(-2m)\_^k$ does not become zero until $k>2m$. Therefore this
limit gives us exactly the sum \eq(|half/2|) we began with.

\begingroup \advance\parindent-7.5pt % subtitle is too wide!
\beginsection 5.6 Hypergeometric Transformations

\endgroup
It should be clear by now that a database of known hypergeometric
closed forms is a useful tool for doing sums of binomial coefficients.
We simply convert any given sum into its canonical hypergeometric form,
then look it up in the table. If it's there, fine, we've got the answer.
If not, we can add it to the database if the sum turns out to be
expressible in closed form.
We might also include entries in the table that say, ``This sum
does not have a simple closed form in general.'' For example, the sum
$\sum_{k\le m}{n\choose k}$ corresponds to the hypergeometric
\begindisplay
{n\choose m}\hyp{1,\,-m}{n-m+1\,}{-1\,}\,,\qquad\hbox{integers $n\ge m\ge0$};
\eqno
\enddisplay
this has a simple closed form only if $m$ is near $0$, $\half n$, or $n$.

But there's more to the story, since hypergeometric functions also
obey identities of their own. This means that every closed form for
hypergeometrics leads to additional closed forms
\g The hypergeometric database should really be a
\hbox{``knowledge base.\qback''}\g
and to additional entries in the database. For example, the identities
in exercises |increase-denominator| and~|omit-first-term| tell us
how to transform one hypergeometric into two others with similar
but different parameters. These can in turn be transformed again.

In 1797, J.\thinspace F. "Pfaff" [|pfaff|] discovered a surprising
% he published it also in a book {\sl Disquisitionibus...} (Helmstedt, 1797)
{\it"reflection law"},
\begindisplay
{1\over(1-z)^a}\hyp{a,b\,}c{{-z\over1-z}@}=\hyp{a,\,c-b}cz\,,
\eqno\eqref|hyp-refl|
\enddisplay
which is a transformation of another type.
This is a formal identity in power series, if the quantity $(-z)^k\!/(1-z)^{k+a}$
is replaced by the infinite series $(-z)^k\bigl(1+{k+a\choose1}z
+{k+a+1\choose2}z^2+\cdots\,\bigr)$ when the left-hand side is
expanded (see exercise~|prove-refl-trans|).
We can use this law to derive new formulas from the identities we
already know, when $z\ne1$.

For example, "Kummer's formula" \eq(|hyp-kummer|) can be combined with
the reflection law \thiseq\ if we choose the parameters so that both
identities apply:
\begindisplay \openup3pt
2^{-a}\hyp{a,\,1-a}{1{+}b{-}a}{1\over2}
&=\hyp{a,\,b}{1{+}b{-}a}{-1}\cr
&={(b/2)!\over b!}(b-a)\_{b/2}\,.
\eqno
\enddisplay
We can now set $a=-n$ and go back from this equation to a new identity in
binomial coefficients that we might need some day:
\begindisplay
\sum_{k\ge0}{(-n)\_^k\,(1{+}n)\_^k\over(1{+}b{+}n)\_^k}\,{2^{-k}\over k!\hypstrut}
&=\sum_k{n\choose k}\left(-1\over2\right)^{\!k}
 {n{+}k\choose k}\bigg/{n{+}b{+}k\choose k}\cr
&=2^{-n}{(b/2)!\,(b{+}n)!\over b!\,(b/2{+}n)!}\,,\quad\hbox{integer $n\ge0$}.
\eqno
\enddisplay
For example, when $n=3$ this identity says that
\begindisplay \openup3pt
&1-3\,{4\over2(4+b)}+3\,{4\cdot5\over4(4+b)(5+b)}
 -{4\cdot5\cdot6\over8(4+b)(5+b)(6+b)}\cr
&\hskip10em= {(b+3)(b+2)(b+1)\over(b+6)(b+4)(b+2)}\,.
\enddisplay
It's almost unbelievable, but true, for all $b$. (Except when
a factor in the denominator vanishes.)

This is fun; let's try again.
Maybe we'll find a formula that will really
astonish our friends. What does Pfaff's
reflection law tell us if we apply it to the strange form
\eq(|hyp-half/2|), where $z=2$? In this case we set $a=-m$, $\,b=1$,
and $c=-2m+\epsilon$, obtaining
\begindisplay \openup3pt
(-1)^m\lim_{\epsilon\to0}\hyp{-m,1}{-2m+\epsilon}2
&=\lim_{\epsilon\to0}\hyp{-m,-2m-1+\epsilon}{-2m+\epsilon}2\cr
&=\lim_{\epsilon\to0}
 \sum_{k\ge0}{(-m)\_^k\,(-2m-1+\epsilon)\_^k\over(-2m+\epsilon)\_^k\hypstrut}\,
 {2^k\over k!\hypstrut}\cr
&=\sum_{k\le m}{m\choose k}{(2m+1)\_k\over(2m)\_k}
 (-2)^k\,,
\enddisplay
because none of the limiting terms is close to zero. This leads to another
miraculous formula,
\g(Hysterical note: See exercise~|diff-limits| if you get a different
result.)\g
\begindisplay \tightplus
\sum_{k\le m}{m\choose k}{2m+1\over 2m+1-k}(-2)^k
&=(-1)^m2^{2m}\bigg/{2m\choose m}\cr
&=1\,\bigg/{-1/2\choose m}\,,\quad\hbox{integer $m\ge0$}.
\eqno\eqref|miracle-half/2|
\enddisplay
When $m=3$, for example, the sum is
\begindisplay
1-7+{84\over5}-14=-{16\over5}\,,
\enddisplay
and $-1/2@\choose3$ is indeed equal to $-{5\over16}$.

When we looked at our binomial coefficient identities and converted them
to hypergeometric form, we overlooked \eq(|partial-binomial|) because it
was a relation between two sums instead of a closed form. But now we
can regard \eq(|partial-binomial|) as an identity between hypergeometric
series.
If we differentiate it $n$ times with respect
to~$y$ and then replace $k$ by~$m-n-k$, we get
\begindisplay
&\sum_{k\ge0}{m+r\choose m-n-k}{n+k\choose n}x^{m-n-k}y^k\cr
&\qquad=
\sum_{k\ge0}{-r\choose m-n-k}{n+k\choose n}(-x)^{m-n-k}(x+y)^k\,.
\enddisplay
This yields the following hypergeometric transformation:
\begindisplay
\hyp{a,\,-n}cz={(a-c)\_n\over(-c)\_n}\hyp{a,\,-n}{1-n+a-c\,}{1-z\mskip2mu}\,,
\quad \tworestrictions{integer}{$n\ge0$.}
\eqno\eqref|hyp-gen-vdm|
\enddisplay
Notice that when $z=1$ this reduces to Vandermonde's convolution, \eq(|Fa-nc|).

Differentiation seems to be useful, if this example is any indication;
we also found it helpful in Chapter~2, when summing $x+2x^2+\cdots+nx^n$.
Let's see what happens when a general hypergeometric series is differentiated
with respect to~$z$:
\begindisplay \openup6pt
{d\over dz}\hyp{a_1,\ldots,a_m}{b_1,\ldots,b_n}z
&=\sum_{k\ge1}{a_1\_^k\ldots a_m\_^k\,z^{k-1}\over
  b_1\_^k\ldots b_n\_^k\,(k-1)!}\cr
&=\sum_{k+1\ge1}{a_1\_^{k+1}\ldots a_m\_^{k+1}\,z^k\over
  b_1\_^{k+1}\ldots b_n\_^{k+1}\,k!}\cr
&=\sum_{k\ge0}{a_1(a_1{+}1)\_^k\ldots a_m(a_m{+}1)\_^k\,z^k\over
  b_1(b_1{+}1)\_^k\ldots b_n(b_n{+}1)\_^k\,k!}\cr
&={a_1\ldots a_m\over b_1\ldots b_n}\hyp{a_1{+}1,\ldots,a_m{+}1}
 {b_1{+}1,\ldots,b_n{+}1}z\,.
\eqno\eqref|hyp/dz|
\enddisplay
The parameters move out and shift up.

It's also possible to use differentiation to tweak just one of the parameters
while holding the rest of them fixed. For this we use the operator
\g How do you pronounce $\vartheta$?\smallskip
(Dunno, but \TeX\ calls it `vartheta'.)\g
\begindisplay
\vartheta=z\,{d\over dz}\,,
\enddisplay
which acts on a function by differentiating it and then multiplying by~$z$.
This operator gives
\begindisplay \openup5pt
\vartheta\hyp{a_1,\ldots,a_m}{b_1,\ldots,b_n}z
&=z\sum_{k\ge1}{a_1\_^k\ldots a_m\_^k\,z^{k-1}\over
  b_1\_^k\ldots b_n\_^k\,(k-1)!}
&=\sum_{k\ge0}{k\,a_1\_^k\ldots a_m\_^k\,z^k\over
  b_1\_^k\ldots b_n\_^k\,k!}\,,\cr
\enddisplay
which by itself isn't too useful. But if we multiply $F$ by one of its
upper parameters, say $a_1$, and add it to $\vartheta F$, we get
\begindisplay \openup5pt \postdisplaypenalty=10000
(\vartheta+a_1)\hyp{a_1,\ldots,a_m}{b_1,\ldots,b_n}z
&=\sum_{k\ge0}{(k+a_1)a_1\_^k\ldots a_m\_^k\,z^k\over
  b_1\_^k\ldots b_n\_^k\,k!}\,,\cr
&=\sum_{k\ge0}{a_1(a_1{+}1)\_^k\,a_2\_^k\ldots a_m\_^k\,z^k\over
  b_1\_^k\ldots b_n\_^k\,k!}\cr
&=a_1\hyp{a_1{+}1,\,a_2,\,\ldots,\,a_m}{b_1,\,\ldots,\,b_n}z\,.
\enddisplay
Only one parameter has been shifted.

 A similar trick works with
lower parameters, but in this case things shift down instead of up:
\begindisplay \openup8pt
(\vartheta+b_1-1)\hyp{a_1,\ldots,a_m}{b_1,\ldots,b_n}z
&=\sum_{k\ge0}{(k+b_1-1)a_1\_^k\ldots a_m\_^k\,z^k\over
  b_1\_^k\ldots b_n\_^k\,k!}\,,\cr
&=\sum_{k\ge0}{(b_1-1)\,a_1\_^k\ldots a_m\_^k\,z^k\over
  (b_1-1)\_^k\,b_2\_^k\ldots b_n\_^k\,k!}\cr
&=(b_1{-}1)\hyp{a_1,\,\ldots,\,a_m}{b_1{-}1,\,b_2,\,\ldots,\,b_n}z\,.
\enddisplay

We can now combine all
\g Ever hear the one about the brothers who named their
cattle ranch Focus, because it's where the sons raise meat?\g
 these operations and make a mathematical ``"pun"'' by expressing
the same quantity in two different ways. Namely, we have
\begindisplay
(\vartheta+a_1)\ldots(\vartheta+a_m)F
=a_1\ldots a_m\,
 \hyp{a_1{+}1,\,\ldots,\,a_m{+}1}{b_1,\,\ldots,\,b_n}z\,,
\enddisplay
and
\begindisplay \openup3pt
&(\vartheta+b_1-1)\ldots(\vartheta+b_n-1)F\cr
&\phantom{(\vartheta+a_1)\ldots(\vartheta+a_m)F}
=(b_1{-}1)\ldots(b_n{-}1)
 \hyp{a_1,\,\ldots,\,a_m}{b_1{-}1,\,\ldots,\,b_n{-}1}z\,,\cr
\enddisplay
where $F=F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)$. And \eq(|hyp/dz|) tells us
that the top line is the derivative of the bottom line. Therefore the
general hypergeometric function $F$ satisfies the "differential equation"
\begindisplay
D(\vartheta+b_1-1)\ldots(\vartheta+b_n-1)F=
 (\vartheta+a_1)\ldots(\vartheta+a_m)F\,,
\eqno\eqref|hyp-diff-eq|
\enddisplay
where $D$ is the operator $d\over dz$.

\smallbreak
This cries out for an example. Let's find the differential equation satisfied
by the standard 2-over-1 hypergeometric series $F(z)=F(a,b;c;z)$. According to
\thiseq, we have
\begindisplay
D(\vartheta+c-1)F=(\vartheta+a)(\vartheta+b)F\,.
\enddisplay
What does this mean in ordinary notation? Well, $(\vartheta+c-1)F$ is
$zF'(z)+{(c-1)}F(z)$, and the derivative of this gives the left-hand side,
\begindisplay \postdisplaypenalty=-200
F'(z)+zF''(z)+(c-1)F'(z)\,.
\enddisplay
On the right-hand side we have
\begindisplay \openup3pt \tightplus
(\vartheta+a)\bigl(zF'(z)+bF(z)\bigr)
&=z{d\over dz}\bigl(zF'(z)+bF(z)\bigr)\,+\,a\bigl(zF'(z)+bF(z)\bigr)\cr
&=zF'(z)+z^2F''(z)+bzF'(z)+azF'(z)+abF(z)\,.\cr
\enddisplay
Equating the two sides tells us that
\begindisplay
z(1-z)F''(z)+\bigl(c-z(a+b+1)\bigr)F'(z)-abF(z)=0\,.
\eqno\eqref|gauss-diff-eq|
\enddisplay
This equation is equivalent to the factored form \eq(|hyp-diff-eq|).

Conversely, we can go back from the differential equation
to the power series. Let's assume that $F(z)=\sum_{k\ge0}t_kz^k$ is a
power series satisfying \eq(|hyp-diff-eq|).
A straight\-forward calculation shows that we must have
\begindisplay
{t_{k+1}\over t_k}={(k+a_1)\ldots(k+a_m)\over(k+b_1)\ldots(k+b_n)(k+1)}\,;
\enddisplay
hence $F(z)$ must be $t_0\,F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)$. We've proved
that the hypergeometric series \eq(|hyp-def|)
is the only formal power series that
satisfies the differential equation \eq(|hyp-diff-eq|) and has the
constant term~$1$.

It would be nice if hypergeometrics solved all the world's differential
equations, but they don't quite. The right-hand side of \eq(|hyp-diff-eq|)
always expands into a sum of terms of the form $\alpha_kz^kF^{(k)}(z)$,
where $F^{(k)}(z)$ is the $k$th derivative $D^kF(k)$; the left-hand side
always expands into a sum of terms of the form $\beta_kz^{k-1}F^{(k)}(z)$
with $k>0$. So the differential equation \eq(|hyp-diff-eq|)
always takes the special form
\begindisplay
z^{n-1}(\beta_n-z\alpha_n)F^{(n)}(z)+\cdots+(\beta_1-z\alpha_1)F'(z)
-\alpha_0F(z)=0\,.
\enddisplay
\looseness=-1
Equation \eq(|gauss-diff-eq|) illustrates this in the case $n=2$.
Conversely, we will prove in exercise~6.|small-vartheta|
 that any differential equation of
this form can be factored in terms of the $\vartheta$ operator, to give
an equation like \eq(|hyp-diff-eq|). So these are the differential equations
whose solutions are power series with rational term ratios.

Multiplying both sides of \eq(|hyp-diff-eq|) by~$z$
\g The function\par
$F(z)=(1-z)^r$\par
satisfies\par
$\vartheta F=z(\vartheta-r)F$.\par\vskip1.5pt
This gives another proof of the "binomial theorem".\g
dispenses with the $D$ operator and
gives us an instructive all-$\vartheta$ form,
\begindisplay
\vartheta(\vartheta+b_1-1)\ldots(\vartheta+b_n-1)F=
 z(\vartheta+a_1)\ldots(\vartheta+a_m)F\,.
\eqno\eqref|hyp-alt-diff-eq|
\enddisplay
The first factor $\vartheta=(\vartheta+1-1)$ on the left
corresponds to the $(k+1)$ in the term ratio \eq(|t-ratio|), which
corresponds to the $k!$ in
the denominator of the $k$th term in a general hypergeometric series.
The other factors $(\vartheta+b_j-1)$ correspond to the denominator
factor $(k+b_j)$, which corresponds to $b_j\_^k$ in \eq(|hyp-def|).
On the right, the $z$ corresponds to~$z^k$, and $(\vartheta+a_j)$ corresponds
to~$\smash{a_j\_^k}$.

One use of this differential theory is to find and prove new transformations.
For example, we can readily verify that both of the hypergeometrics
\begindisplay \advance\abovedisplayskip-1.6pt \advance\belowdisplayskip-1.6pt
\hyp{2a,\,2b}{a+b+\half\,}{z\,}\And\hyp{a,\,b}{a+b+\half\,}{\,4z(1-z)\,}
\enddisplay
satisfy the differential equation
\begindisplay \postdisplaypenalty=-400 \advance\abovedisplayskip-1pt
\textstyle z(1-z)F''(z)+(a+b+\half)(1-2z)F'(z)-4abF(z)=0\,;
\enddisplay
hence {\it "Gauss's identity"} [|gauss-hyp|, equation~102]
\begindisplay
\hyp{2a,\,2b}{a+b+\half\,}{z\,}=\hyp{a,\,b}{a+b+\half\,}{\,4z(1-z)\,}
\eqno\eqref|hyp4z-4z2|
\enddisplay
must be true. In particular,
\g(Caution: We can't use \eq(|hyp4z-4z2|) safely when $\vert z\vert>1/2$,
"!trap"
unless both sides are polynomials; see~exercise~|convergence-caution|.)\g
\begindisplay
\hyp{2a,\,2b}{a+b+\half\,}{\,{\half}\,}=\hyp{a,\,b}{a+b+\half\,}{\,1}\,,
\eqno\eqref|hyp-gauss-half|
\enddisplay
whenever both infinite sums converge.

Every new identity for hypergeometrics has consequences for binomial
coefficients, and this one is no exception. Let's consider the sum
\begindisplay
\sum_{k\le m}{m-k\choose n}{m+n+1\choose k}\left(-1\over2\right)^{\!k}\,,
\qquad\hbox{integers $m\ge n\ge0$}.
\enddisplay
The terms are nonzero for $0\le k\le m-n$, and with a little delicate
limit-taking as before we can express this sum as the hypergeometric
\begindisplay
\lim_{\epsilon\to0}\,{m\choose n}\,
 \hyp{n-m,\,-n-m-1+\alpha\epsilon\,}{-m+\epsilon}{\,\half\,}\,.
\enddisplay
The value of $\alpha$ doesn't affect the limit, since the nonpositive
upper parameter $n-m$ cuts the sum off early. We can set $\alpha=2$,
so that \thiseq\ applies. The limit can now be evaluated because
the right-hand side is a special case of~\eq(|Fabc|).
The result can be expressed in simplified form,
\begindisplay
&\sum_{k\le m}{m-k\choose n}{m+n+1\choose k}\left(-1\over2\right)^{\!k}\cr
&\qquad=
{(m+n)/2\choose n}2^{n-m}@\[\hbox{$m+n$ is even}]\,,
\quad\tworestrictions{integers}{$m\ge n\ge0$,}
\eqno\eqref|gauss-miracle|
\enddisplay
as shown in exercise |explain-gauss-miracle|.
 For example, when $m=5$ and $n=2$
we get ${5\choose2}{8\choose0}-{4\choose2}{8\choose1}/2+{3\choose 2}{8\choose 2}/4
-{\2\choose2}{8\choose3}/8=10-24+21-7=0$; when $m=4$ and $n=2$, both
sides give $3\over4$.

We can also find cases where \eq(|hyp4z-4z2|) gives binomial sums when $z=-1$,
but these are really weird. If we set $a={1\over6}-{n\over3}$ and $b=-n$,
we get the monstrous formula
\begindisplay
\hyp{{1\over3}-{2\over3}n,\,-2n}{\,{2\over3}-{4\over3}n}{-1}=
\hyp{{1\over6}-{1\over3}n,\,-n}{{2\over3}-{4\over3}n}{-8}\,.
\enddisplay
These hypergeometrics are
nondegenerate polynomials when $n\not\=2$ \tmod3; and the parameters
have been cleverly chosen so that the
left-hand side can be evaluated by \eq(|hyp-kummer|).
We are therefore led to a truly mind-boggling result,
\begindisplay
&\sum_k{n\choose k}{{1\over3}n-{1\over6}\choose k}\,8^k\bigg/
 {{4\over3}n-{2\over3}\choose k}\cr
&\qquad={2n\choose n}\bigg/
 {{4\over3}n-{2\over3}\choose n}\,,\quad
 \hbox{integer $n\ge0$, \ $n\not\=2$ \tmod3.}
\eqno\eqref|useless|
\enddisplay
This is the most startling identity in binomial coefficients that
we've seen.
Small cases of the identity aren't even easy to check
by hand. (It turns out that both sides do give $81\over7$ when $n=3$.)
\g The only use of \thiseq\ is to demonstrate
the existence of incredibly "useless identities".\g
But the identity is completely useless, of course; surely it will
never arise in a practical problem.

So that's our hype for hypergeometrics. We've seen that
hypergeometric series provide a high-level way
to understand what's going on in binomial coefficient sums.
A great deal of additional information can be found in the
classic book by Bailey~[|bailey|] and its sequel by
Gasper and Rahman~[|gasper-rahman|].

\beginsection 5.7 Partial Hypergeometric Sums

Most of the sums we've evaluated in this chapter
range over all indices $k\ge0$, but sometimes we've been able to find a
closed form that works over a general range $a\le k<b$.
For example, we know from \eq(|bc-alt-sum|) that
\begindisplay
\sum_{k<m}{n\choose k}(-1)^k=(-1)^{m-1}{n-1\choose m-1}\,,
\qquad\hbox{integer $m$}.
\eqno\eqref|alt-sum<|
\enddisplay
The theory in Chapter 2 gives us a nice way to understand formulas
like this: If $f(k)=\Delta g(k)=g(k+1)-g(k)$, then we've agreed to write
$\sum f(k)\,\delta k=g(k)+C$, and
\begindisplay
\sum\nolimits_a^b f(k)\,\delta k=g(k)\,\big\vert_a^b=g(b)-g(a)\,.
\enddisplay
Furthermore, when $a$ and $b$ are integers with $a\le b$, we have
\begindisplay
\sum\nolimits_a^b f(k)\,\delta k=\sum_{a\le k<b}f(k)=g(b)-g(a)\,.
\enddisplay
Therefore identity \eq(|alt-sum<|) corresponds to the indefinite
summation formula
\begindisplay
\sum{n\choose k}(-1)^k\,\delta k=(-1)^{k-1}{n-1\choose k-1}+C\,,
\enddisplay
and to the difference formula
\begindisplay
\Delta\biggl((-1)^k {n\choose k}\biggr)=(-1)^{k+1}{n+1\choose k+1}\,.
\enddisplay

\goodbreak
It's easy to start with a function $g(k)$ and to compute $\Delta g(k)=f(k)$,
a function whose sum will be $g(k)+C$. But it's much harder to start with
$f(k)$ and to figure out its indefinite sum $\sum f(k)\,\delta k=g(k)+C$;
this function~$g$ might not have a simple form. For example, there is
apparently no simple form for $\sum{n\choose k}\,\delta k$; otherwise
we could evaluate sums like $\sum_{k\le n/3}{n\choose k}$, about which
we're clueless. Yet maybe there is a simple form for $\sum{n\choose k}\,
\delta k$ and we just haven't thought of it; how can we be sure?

In 1977, R.\kern2pt W. "Gosper" [|gosper|]
 discovered a beautiful way to find indefinite sums $\sum f(k)\,\delta k=
g(k)+C$ whenever
$f$ and~$g$ belong to a general class
"!Gosper's method" "!summation, indefinite" "!indefinite summation"
of functions called "hypergeometric terms". Let us write
\begindisplay
\hypk_k{a_1,\, \ldots,\, a_m}{b_1,\, \ldots,\, b_n}z=
 {a_1\_^k\ldots a_m\_^k \over b_1\_^k\ldots b_n\_^k}
	\,{z^k\over k!\hypstrut}
\eqno\eqref|hypk-def|
\enddisplay
for the $k$th term of the hypergeometric series $F(a_1,\ldots,a_m;
b_1,\ldots,b_n;z)$. We will regard
$F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)_k$ as a function of~$k$, not of~$z$.
In many cases it turns out that there are parameters
$c$, $A_1$, \dots,~$A_M$, $B_1$,~\dots,~$B_N$, and~$Z$ such that
\begindisplay
\sum\hypk_k{a_1,\, \ldots,\, a_m}{b_1,\, \ldots,\, b_n}z\,\delta k=
c\hypk_k{A_1,\, \ldots,\, A_M}{B_1,\, \ldots,\, B_N}Z+C\,,
\eqno
\enddisplay
given $a_1$, \dots, $a_m$, $b_1$, \dots, $b_n$, and $z$. We will say that
a given function
$F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)_k$ is {\it"summable in hypergeometric
terms"\/} if such constants 
$c$, $A_1$, \dots,~$A_M$, $B_1$,~\dots,~$B_N$,~$Z$ exist.
Gosper's algorithm either finds the unknown constants or proves that
no such constants exist.

In general, we say that $t(k)$ is a {\it"hypergeometric term"\/} if
$t(k+1)/t(k)$ is a rational function of~$k$, not identically zero.
This means, in essence, that $t(k)$
is a constant multiple of a term like \eq(|hypk-def|).
(A technicality arises, however, with respect to zeros, because we
want $t(k)$ to be meaningful when $k$ is negative and when one or
more of the $b$'s in \eq(|hypk-def|) is zero or a negative integer.
Strictly speaking, we get the
most general hypergeometric term by multiplying \eq(|hypk-def|) by a
nonzero constant times a power of~$0$, then cancelling zeros of the
numerator with zeros of the denominator. The examples in exercise~|ht-warmup|
help clarify this general rule.)

Suppose we want to find $\sum t(k)\,\delta k$, when $t(k)$ is a hypergeometric
term. Gosper's algorithm proceeds in two steps, each of which is fairly
straightforward. Step~1 is to express the term ratio
in the special form
\begindisplay
{t(k+1)\over t(k)}={p(k+1)\over p(k)}\,{q(k)\over r(k+1)}\,,
\eqno\eqref|tk-pqr|
\enddisplay
where $p$, $q$, and $r$ are polynomials subject to the following condition:
\g("Divisibility of polynomials" is analogous to divisibility of
integers. For example, $(k+\alpha)\divides q(k)$ means that the
quotient $q(k)/(k+\alpha)$ is a polynomial.\par It's easy to see that
$(k+\alpha)\divides q(k)$\par if and only if\par $q(-\alpha)=0$.)\g
\begindisplay
&(k+\alpha)\divides q(k)\And(k+\beta)\divides r(k)\cr
&\qquad\implies
\hbox{$\alpha-\beta$ is not a positive integer.}
\eqno\eqref|qr-condition|
\enddisplay
This condition is easy to achieve: We start by provisionally setting $p(k)=1$,
and we set $q(k)$ and $r(k+1)$ to the numerator and denominator of
the term ratio, factoring them into linear factors. For example, if
$t(k)$ has the form \eq(|hypk-def|), we start with the factorizations
$q(k)=(k+a_1)\ldots(k+a_m)z$ and $r(k)=(k+b_1-1)\ldots(k+b_n-1)k$. Then
we check if \thiseq\ is violated. If $q$ and~$r$ have factors
$(k+\alpha)$ and $(k+\beta)$ where $\alpha-\beta=N>0$, we divide them out
of $q$ and~$r$ and replace $p(k)$ by
\begindisplay
p(k)(k{+}\alpha{-}1)\_{N-1}=p(k)(k{+}\alpha{-}1)(k{+}\alpha{-}2)\ldots
(k{+}\beta{+}1)\,.
\eqno\eqref|grow-p|
\enddisplay
The new $p$, $q$, and $r$ still satisfy \eq(|tk-pqr|), and we can
repeat this process until \eq(|qr-condition|) holds. We'll see in a
moment why \eq(|qr-condition|) is important.

Step 2 of Gosper's algorithm is to finish the job\dash---to
find a hypergeometric term $T(k)$ such that
\begindisplay
t(k)=T(k+1)-T(k)\,,
\eqno\eqref|gosper-goal|
\enddisplay
whenever possible. But it's not obvious how to do this; we need to develop
some theory before we know how to proceed. Gosper noticed, after studying
a lot of special cases, that it is wise to write the unknown function $T(k)$
in the form
\begindisplay
T(k)={r(k)\,s(k)\,t(k)\over p(k)}\,,
\eqno\eqref|gosper-mystery|
\enddisplay
where $s(k)$ is a secret function that must be discovered somehow.
\g(Exercise~|explain-gosper-mystery| gives a clue about why we might want to
make this magic substitution.)\g
Plugging \thiseq\ into \eq(|gosper-goal|) and applying \eq(|tk-pqr|) gives
\begindisplay
t(k)&={r(k+1)@s(k+1)@t(k+1)\over p(k+1)}-{r(k)@s(k)@t(k)\over p(k)}\cr
&={q(k)@s(k+1)@t(k)\over p(k)}-{r(k)@s(k)@t(k)\over p(k)}\,;\cr
\enddisplay
so we need to have
\begindisplay
p(k)=q(k)@s(k+1)-r(k)@s(k)\,.
\eqno\eqref|pqrs-rec|
\enddisplay
If we can find $s(k)$ satisfying this fundamental recurrence relation,
we've found $\sum t(k)\,\delta k$. If we can't, there's no $T$.

We're assuming that $T(k)$ is a hypergeometric term, which means that
$T(k+1)/T(k)$ is a rational function of~$k$. Therefore,
by \eq(|gosper-mystery|) and~\eq(|gosper-goal|), $r(k)@s(k)/p(k)=
T(k)/\bigl(T(k+1)-T(k)\bigr)$ is a rational function of~$k$,
and $s(k)$ itself must be a quotient of polynomials:
\begindisplay
s(k)=f(k)/g(k)\,.
\enddisplay
But in fact we can prove that $s(k)$ is itself a polynomial.
For if $g(k)$ is not constant, and if $f(k)$ and $g(k)$ have no common factors,
let $N$ be the largest integer such that $(k+\beta)$ and $(k+\beta+N-1)$
both occur as factors of $g(k)$ for some complex number~$\beta$. The
value of $N$ is positive, since $N=1$ always satisfies this condition.
Equation \eq(|pqrs-rec|) can be rewritten
\begindisplay
p(k)@g(k{+}1)@g(k)=q(k)@f(k{+}1)@g(k)-r(k)@g(k{+}1)@f(k)\,,
\enddisplay
and if we set $k=-\beta$ and $k=-\beta-N$ we get
\begindisplay
r(-\beta)@g(1{-}\beta)@f(-\beta)=0=q(-\beta{-}N)@f(1{-}\beta{-}N)@g(-\beta{-}N)\,.
\enddisplay
Now $f(-\beta)\ne0$ and $f(1-\beta-N)\ne0$, because $f$ and $g$ have no
common roots. Also $g(1-\beta)\ne0$ and $g(-\beta-N)\ne0$, because $g(k)$
would otherwise contain the factor $(k+\beta-1)$ or $(k+\beta+N)$,
contrary to the maximality of~$N$. Therefore
\begindisplay \postdisplaypenalty=10000
r(-\beta)=q(-\beta-N)=0\,.
\enddisplay
But this contradicts condition \eq(|qr-condition|). Hence $s(k)$
\g I see: Gosper came up with condition \eq(|qr-condition|) in order to
make this proof go through.\g
must be a polynomial.

Our task now boils down to finding a polynomial
$s(k)$ that satisfies \eq(|pqrs-rec|), when $p(k)$, $q(k)$, and $r(k)$
are given polynomials, or proving that no such polynomial exists.
It's easy to do this when $s(k)$ has
any particular degree~$d$, since we can write
\begindisplay
s(k)=\alpha_dk^d+\alpha_{d-1}k^{d-1}+\cdots+\alpha_0\,,\qquad \alpha_d\ne 0
\eqno
\enddisplay
for unknown coefficients $(\alpha_d,\ldots,\alpha_0)$ and plug this
expression into
the fundamental recurrence \eq(|pqrs-rec|). The polynomial $s(k)$ will satisfy
 the recurrence
if and only if the $\alpha$'s satisfy the linear equations that result
when we equate coefficients of each power of~$k$ in
\eq(|pqrs-rec|).

But how can we determine the degree of~$s$? It turns out that
there actually are at most
two possibilities. We can rewrite \eq(|pqrs-rec|) in the form
\begindisplay
&2p(k)=Q(k)\bigl(s(k+1)+s(k)\bigr)+R(k)\bigl(s(k+1)-s(k)\bigr)\,,
\eqno\eqref|pQRs-rec|\cr
&\hbox{where}\quad Q(k)=q(k)-r(k)\And R(k)=q(k)+r(k)\,.
\enddisplay
If $s(k)$ has degree $d$, then the sum $s(k+1)+s(k)=2\alpha_d k^d+\cdots\,$
also has degree~$d$,
while the difference
$s(k+1)-s(k)=\Delta s(k)=d\alpha_d k^{d-1}+\cdots\,$ has degree
$d-1$. (The zero polynomial can be assumed to have degree~$-1$.)
Let's write $\deg(P)$ for the degree of a polynomial~$P$.
If $\deg(Q)\ge\deg(R)$, then the degree of the right-hand side
of \thiseq\ is $\deg(Q)+d$, so we must have $d=\deg(p)-\deg(Q)$.
On the other hand if $\deg(Q)<\deg(R)=d'$, we can write $Q(k)=
\lambda' k^{d'-1}+\cdots\,$ and $R(k)=\lambda@ k^{d'}+\cdots\,$ where
$\lambda\ne0$; the right-hand side of~\thiseq\ has the form
\begindisplay
(2\lambda' \alpha_d+\lambda@ d\,\alpha_d)k^{d+d'-1}+\cdots\,.
\enddisplay
Ergo, two possibilities: Either $2\lambda'+\lambda@ d\ne0$, and $d=\deg(p)
-\deg(R)+1$; or $2\lambda'+\lambda@d=0$, and $d>\deg(p)-\deg(R)+1$. The
second case needs to be examined only if $-2\lambda'/\lambda$ is an
integer~$d$ greater than $\deg(p)-\deg(R)+1$.

OK, we now have enough facts to perform Step 2 of Gosper's two-step algorithm:
By trying at most two values of~$d$, we can discover $s(k)$, whenever
equation \eq(|pqrs-rec|) has a polynomial solution.
If $s(k)$ exists, we can plug it into \eq(|gosper-mystery|) and we have
our~$T$. If it doesn't, we've proved that $t(k)$ is not summable in
hypergeometric terms.

Time for an example: Let's try the partial sum \eq(|alt-sum<|). Gosper's
method should be able to deduce the value of
\begindisplay
\sum{n\choose k}(-1)^k\,\delta k
\enddisplay
for any fixed~$n$, so we seek the sum of
\begindisplay
t(k)={n\choose k}(-1)^k={n!\,(-1)^k\over k!\,(n-k)!}\,.
\enddisplay
Step 1 is to put the term ratio into the required form \eq(|tk-pqr|);
we have
\begindisplay
{t(k+1)\over t(k)}={k-n\over k+1}={p(k+1)\,q(k)\over p(k)\,r(k+1)}
\enddisplay
so we simply take $p(k)=1$, $q(k)=k-n$, and $r(k)=k$.
\g Why isn't it\par $r(k)=k+1$?\par Oh, I see.\g
This choice of $p$, $q$, and $r$ satisfies
\eq(|qr-condition|), unless $n$~is a negative integer; let's suppose
it isn't.

Now we do Step 2.
According to \eq(|pQRs-rec|), we should consider the
polynomials $Q(k)=-n$ and $R(k)=2k-n$. Since $R$ has larger degree than~$Q$,
we need to look at two cases. Either $d=\deg(p)-\deg(R)+1$, which is~$0$;
or $d=-2\lambda'/\lambda$ where $\lambda'=-n$ and $\lambda=2$, hence $d=n$.
The first case is nicer, because it doesn't require $n$ to be a positive
integer, so let's try it first; we'll need to try the other possibility for~$d$
only if the first case fails. Assuming that $d=0$, the value of
$s(k)$ is simply $\alpha_0$, and equation \eq(|pqrs-rec|) reduces to
\begindisplay
1=(k-n)\alpha_0-k\alpha_0\,.
\enddisplay
Hence we choose $\alpha_0=-1/n$. This satisfies the equation and
gives
\begindisplay \openup3pt
T(k)&={r(k)\,s(k)\,t(k)\over p(k)}\cr
&=k\cdot\left(-1\over n\right)\cdot{n\choose k}(-1)^k\cr
&={n-1\choose k-1}(-1)^{k-1}\,,\qquad\hbox{if $n\ne0$,}
\enddisplay
precisely the answer we were hoping to confirm.

If we apply the same method to find the indefinite sum $\sum{n\choose k}\,
\delta k$, without the $(-1)^k$, everything will be almost the same
except that $q(k)$ will be ${n-k}$; hence $Q(k)=n-2k$ will have greater
degree than $R(k)=n$, and we will conclude that $d$ has the
impossible value $\deg(p)-\deg(Q)=-1$. (The polynomial $s(k)$ cannot have
negative degree, because it cannot be zero.)
Therefore the function $n\choose k$ is not summable in hypergeometric terms.

However, once we have eliminated the impossible, whatever remains\dash---%
however improbable\dash---must be the truth % from Chapter 6, Sign of Four
 (according to S.~Holmes~[|holmes-four|]). When we defined $p$, $q$, and~$r$
in Step~1,
we decided to ignore the possibility that $n$ might be a negative integer.
What if it is? Let's set $n=-N$, where $N$ is positive.
Then the term ratio for $\sum{n\choose k}\,\delta k$ is
\begindisplay
{t(k+1)\over t(k)}={-(k+N)\over(k+1)}={p(k+1)\over p(k)}\,{q(k)\over r(k+1)}
\enddisplay
and it should be represented by $p(k)=(k+1)\_^{N-1}$, $q(k)=-1$,
$r(k)=1$, according to \eq(|grow-p|).
Step~2 of Gosper's algorithm now tells us to look for a polynomial
$s(k)$ of degree $d=N-1$; maybe there's hope after all.
For example, when $N=2$ recurrence \eq(|pqrs-rec|) says that we should solve
\begindisplay
k+1=-\bigl((k+1)\alpha_1+\alpha_0\bigr)-(k\alpha_1+\alpha_0)\,.
\enddisplay
Equating coefficients of $k$ and $1$ tells us that
\begindisplay
1=-\alpha_1-\alpha_1;\qquad 1=-\alpha_1-\alpha_0-\alpha_0;
\enddisplay
hence $s(k)=-\half k-{1\over4}$ is a solution, and
\begindisplay
T(k)={1\cdot\bigl(-\half k-{1\over4_{\mathstrut}}\bigr)\cdot{-2@\choose k}\over k+1}
=(-1)^{k-1}{2k+1\over4}\,.
\enddisplay
Can this be the desired sum? Yes, it checks out:
\g\noindent\llap{``}Excellent, "Holmes"!''\par
\noindent\llap{``}Elementary, my dear "Watson".''\g
\begindisplay
(-1)^k{2k+3\over4}-(-1)^{k-1}{2k+1\over4}=(-1)^k(k+1)
 ={-2\choose k}\,.
\enddisplay

Incidentally, we can write this summation formula in another form,
by attaching an upper limit:
\begindisplay \openup3pt
\sum_{k<m}{-2\choose k}&=(-1)^{k-1}{2k+1\over4}\,\Big\vert_0^m\cr
&={(-1)^{m-1}\over2}\biggl(m+{1-(-1)^m\over2}\biggr)\cr
&=(-1)^{m-1}\biggl\lceil{m\over2}\biggr\rceil\,,\qquad\hbox{integer $m\ge0$.}
\enddisplay
This representation
conceals the fact that $-2@\choose k$ is summable
in hypergeometric terms, because $\lceil m/2\rceil$ is not a
hypergeometric term. (See exercise~|ht-warmup|.)

A problem might arise in the denominator of \eq(|gosper-mystery|) if
$p(k)=0$ for some integer~$k$. Exercise |gosper-bonus| gives some insight
into what can be done in such situations.

Notice that we need not bother to
compile a catalog of indefinitely summable hypergeometric terms,
analogous to the database of definite hypergeometric sums mentioned earlier
in this chapter, because Gosper's algorithm provides a quick, uniform method
that works in all summable cases.

Marko "Petkov\v{s}ek" [|petk|] has found a nice way to generalize Gosper's
algorithm to more complicated inversion problems,
by showing how to determine all hypergeometric terms~$T(k)$
that satisfy the $l$th-order recurrence
\begindisplay
t(k)=p_l(k)T(k+l)+\cdots+p_1(k)T(k+1)+p_0(k)T(k)\,,
\eqno\eqref|petk-rec|
\enddisplay
given any hypergeometric term $t(k)$ and polynomials $p_l(k)$, \dots,
$p_1(k)$, $p_0(k)$.

\beginsection 5.8 Mechanical Summation

Gosper's algorithm, beautiful as it is, finds a closed form for only a few
of the binomial sums we meet in practice. But we need not stop there.
Doron "Zeilberger" [|creat|] showed how to extend "Gosper"'s algorithm
so that it becomes even more beautiful, making it succeed in vastly more
cases. With Zeilberger's extension we can handle summation over
all~$k$, not just partial sums, so we have an alternative to the
hypergeometric methods of Sections 5.5 and~5.6.
 Moreover, as with Gosper's original
method, the calculations can be done by computer, almost blindly; we
need not rely on cleverness and luck."!Gosper-Zeilberger algorithm"

The basic idea is to regard the term we want to sum as a function $t(n,k)$
of two variables $n$ and~$k$. (In Gosper's algorithm we wrote just $t(k)$.)
When $t(n,k)$ does not turn out to be indefinitely summable in hypergeometric
terms, with respect to~$k$\dash---and let's face it,
relatively few functions are\dash---Zeilberger
noticed that we can often modify $t(n,k)$ in order to obtain another term
that {\it is\/} indefinitely summable. For example, it often
turns out in practice that $\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)$
is indefinitely summable with respect to~$k$, for appropriate polynomials
$\beta_0(n)$ and $\beta_1(n)$. And when we carry out the sum with respect
to~$k$, we obtain a recurrence in~$n$ that solves our problem.

Let's start with a simple case in order to get familiar with this general
approach. Suppose we have forgotten the binomial theorem, and we want to
evaluate $\sum_k{n\choose k}z^k$. How could we discover the answer, without
clairvoyance or inspired guesswork? Earlier in this chapter,
\g Or without looking on page~|bc-tops|.\g
for example in Problem~3 of Section~5.2,  we learned how
to replace $n\choose k$ by ${n-1\choose k}+{n-1\choose k-1}$ and to fiddle
around with the result. But there's a more systematic way to proceed.

\def\that{{\hat t}}
\def\phat{{\hat p}}
\def\tbar{{\bar t}}
\def\pbar{{\bar p}}
Let $t(n,k)={n\choose k}z^k$ be the quantity we want to sum. Gosper's algorithm
tells us that we can't evaluate the partial sums $\sum_{k\le m}t(n,k)$ for
arbitrary~$n$ in hypergeometric terms,
except in the case $z=-1$. So let's consider a more general term
\begindisplay
\that(n,k)=\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)
\eqno
\enddisplay
instead. We'll look for values of $\beta_0(n)$ and $\beta_1(n)$ that make
Gosper's algorithm succeed. First we want to simplify \thiseq\ by
using the relation between $t(n+1,k)$ and $t(n,k)$ to eliminate $t(n+1,k)$
from the expression. Since
\begindisplay\openup3pt
{t(n+1,k)\over t(n,k)} &=
 {(n+1)!\,z^k\over(n+1-k)!\,k!}{(n-k)!\,k!\over n!\,z^k}\cr
&= {n+1\over n+1-k}\,,
\enddisplay
we have
\begindisplay
\that(n,k)=p(n,k){t(n,k)\over n+1-k}\,,
\enddisplay
where
\begindisplay
p(n,k)=(n+1-k)\beta_0(n)+(n+1)\beta_1(n)\,.
\enddisplay
We now apply Gosper's algorithm to $\that(n,k)$, with $n$ held fixed, first
writing
\begindisplay
{\that(n,k+1)\over\that(n,k)}=
{\phat(n,k+1)\over\phat(n,k)}{q(n,k)\over r(n,k+1)}
\eqno\eqref|zeil-pqr|
\enddisplay
as in \eq(|tk-pqr|). Gosper's method would find such a representation by
starting
with $\phat(n,k)=1$, but with Zeilberger's extension we are better off starting
with $\phat(n,k)=p(n,k)$. Notice that if we set
$\tbar(n,k)=\that(n,k)/p(n,k)$ and
$\pbar(n,k)=\phat(n,k)/p(n,k)$,
equation \thiseq\ is equivalent to
\begindisplay
{\tbar(n,k+1)\over\tbar(n,k)}=
{\pbar(n,k+1)\over\pbar(n,k)}{q(n,k)\over r(n,k+1)}\,.
\eqno\eqref|tbar-ratio|
\enddisplay
So we can find $\phat$, $q$ and~$r$ satisfying \eq(|zeil-pqr|) by
finding $\pbar$, $q$ and~$r$ satisfying \thiseq,
starting with $\pbar(n,k)=1$. This makes life easy,
because $\tbar(n,k)$ does not involve the unknown quantities $\beta_0(n)$
and $\beta_1(n)$ that appear in $\that(n,k)$. In our case $\tbar(n,k)=
t(n,k)/(n+1-k)=n!\,z^k\!/(n+1-k)!\,k!$, so we have
\begindisplay
{\tbar(n,k+1)\over\tbar(n,k)}=
{(n+1-k)@z\over k+1}\,;
\enddisplay
we may take $q(n,k)=(n+1-k)z$ and $r(n,k)=k$.
\g This time I remembered why $r(n,k)$ isn't $k+1$.\g
These polynomials in~$k$
are supposed to satisfy condition~\eq(|qr-condition|). If they don't, we're
supposed to remove factors from $q$ and $r$ and include corresponding
factors \eq(|grow-p|) in $\pbar(n,k)$; but
we should do this only when the quantity $\alpha-\beta$ in \eq(|qr-condition|)
is a positive integer constant, independent of~$n$, because we want our
calculations to be valid for arbitrary~$n$. (The formulas we derive will,
in fact, be valid even when $n$ and $k$ are not integers, using the
generalized factorials \eq(|f-def-lim|).)

Our first choices of $q$ and $r$ do satisfy \eq(|qr-condition|), in this sense,
so we can move right on to Step~2 of Gosper's
algorithm: We want to solve the analog of~\eq(|pqrs-rec|), using
\eq(|zeil-pqr|) in place of \eq(|tk-pqr|). So we want to solve
\begindisplay
\phat(n,k)=q(n,k)@s(n,k+1)-r(n,k)@s(n,k)
\eqno\eqref|zeil-pqrs-rec|
\enddisplay
for the secret polynomial
\begindisplay
s(n,k)=\alpha_d(n)@k^d+\alpha_{d-1}(n)@k^{d-1}+\cdots+\alpha_0(n)\,.
\eqno\eqref|zeil-s|
\enddisplay
(The coefficients of $s$ are considered to be functions of~$n$, not just
constants.) In our case equation \eq(|zeil-pqrs-rec|) is
\begindisplay
&(n+1-k)\beta_0(n)+(n+1)\beta_1(n)\cr
&\qquad=(n+1-k)@z@s(n,k+1)-k@s(n,k)\,,
\enddisplay
and we regard this as a polynomial equation in $k$ with coefficients that
are functions of~$n$. As before, we determine the degree~$d$ of~$s$ by
considering $Q(n,k)=q(n,k)-r(n,k)$ and $R(n,k)=q(n,k)+r(n,k)$. Since
\g The degree function $\deg(Q)$ refers here to the degree in~$k$,
treating~$n$ as constant.\g
$\deg(Q)=\deg(R)=1$
(assuming that $z\ne-1$), we have $d=\deg(\phat)-\deg(Q)=0$ and
$s(n,k)=\alpha_0(n)$ is independent of~$k$. Our equation becomes
\begindisplay
(n+1-k)\beta_0(n)+(n+1)\beta_1(n)=(n+1-k)@z@\alpha_0(n)-k@\alpha_0(n)\,;
\enddisplay
and by equating powers of $k$ we get the equivalent $k$-free equations
\begindisplay
\vbox{\halign{$\hfil#@$&$#\hfil$&
              ${}\hfil#@$&$#\hfil$&
              ${}\hfil#@$&$#\hfil$&$=0\,#\hfil$\cr
(n+1)&\beta_0(n)&+(n+1)&\beta_1(n)&-(n+1)@z&\alpha_0(n)&,\cr
-&\beta_0(n)&&&+(z+1)&\alpha_0(n)&.\cr}}
\enddisplay
Hence we have a solution to \eq(|zeil-pqrs-rec|) with
\begindisplay
\beta_0(n)=z+1\,,\qquad \beta_1(n)=-1\,,\qquad \alpha_0(n)=s(n,k)=1\,.
\enddisplay
(By chance, $n$ has dropped out.)

We have discovered, by a purely mechanical method, that the term
$\that(n,k)=(z+1)t(n,k)-t(n+1,k)$ is summable in hypergeometric terms.
In other words,
\begindisplay
\that(n,k)=T(n,k+1)-T(n,k)\,,
\eqno\eqref|gz-success|
\enddisplay
where $T(n,k)$ is a
hypergeometric term in~$k$. What is this $T(n,k)$? According to
\eq(|gosper-mystery|) and \eq(|tbar-ratio|), we have
\begindisplay
T(n,k)={r(n,k)@s(n,k)@\that(n,k)\over\phat(n,k)}=r(n,k)s(n,k)\tbar(n,k)\,,
\eqno
\enddisplay
because $\pbar(n,k)=1$. (Indeed, $\pbar(n,k)$ almost always turns out
to be~$1$ in practice.) Hence
\begindisplay
T(n,k)={k\over n+1-k}\,t(n,k)={k\over n+1-k}{n\choose k}z^k
 ={n\choose k-1}z^k.
\enddisplay
And sure enough, everything checks out\dash---equation \eq(|gz-success|)
is true:
\begindisplay
(z+1){n\choose k}z^k-{n+1\choose k}z^k=
{n\choose k}z^{k+1}-{n\choose k-1}z^k\,.
\enddisplay

But we don't actually need to know $T(n,k)$ precisely, because we are
going to sum $t(n,k)$ over all integers~$k$. All we need to know is that
$T(n,k)$ is nonzero for only finitely many values of~$k$, when $n$ is any given
nonnegative integer. Then the sum of $T(n,k+1)-T(n,k)$ over all~$k$ must
telescope to~$0$.

Let $S_n=\sum_k t(n,k)=\sum_k{n\choose k}z^k$; this is the sum we started with,
and we're now ready to compute it, because we now know a lot about $t(n,k)$.
The Gosper-Zeilberger procedure has deduced that
\begindisplay
\sum_k\bigl((z+1)t(n,k)
-t(n+1,k)\bigr)=0\,.
\enddisplay
But this sum is $(z+1)\sum_k t(n,k)-\sum_k t(n+1,k)
=(z+1)S_n-S_{n+1}$. Therefore we have
\g \vskip-30pt
In fact,\par\vskip1pt$\displaystyle\lim_{k\to\infty}T(n,k)=0$\par\vskip2pt
when $\vert z\vert<1$\par and $n$ is any\par
complex number. So \eq(|bin-rec|)
is true for all~$n$,\par and in particular $S_n=(z+1)^n$ when $n$ is a
negative integer.\g
\begindisplay
S_{n+1}=(z+1)S_n\,.
\eqno\eqref|bin-rec|
\enddisplay
Aha! This is a recurrence we know how to solve, provided that we know~$S_0$.
And obviously $S_0=1$. Hence we deduce that $S_n=(z+1)^n$, for all integers
$n\ge0$. QED.

Let's look back at this computation and summarize what we did, in a form that
will apply also to other summands $t(n,k)$. The Gosper-Zeilberger algorithm
can be formalated as follows, when $t(n,k)$ is given:
\smallskip
\item{0}Set $l:=0$. (We'll seek recurrences in $n$ of order~$l$.)
\item{1}Let $\that(n,k)=\beta_0(n)t(n,k)+\cdots+\beta_l(n)t(n+l,k)$, where
$\beta_0(n)$, \dots,~$\beta_l(n)$ are unknown functions. Use properties
of $t(n,k)$ to find a linear combination $p(n,k)$
of $\beta_0(n)$, \dots,~$\beta_l(n)$ with coefficients
that are polynomials in $n$ and~$k$, so that $\that(n,k)$ can be written
in the form $p(n,k)\tbar(n,k)$, where $\tbar(n,k)$ is a hypergeometric
term in~$k$. Find polynomials $\pbar(n,k)$, $q(n,k)$, $r(n,k)$ so that the
term ratio of $\tbar(n,k)$ is expressed in the form \eq(|tbar-ratio|),
where $q(n,k)$ and $r(n,k)$ satisfy Gosper's condition~\eq(|qr-condition|).
Set $\phat(n,k)=p(n,k)\pbar(n,k)$.
\item{2a}Set $d_Q:=\deg(q-r)$, $d_R:=\deg(q+r)$, and
\begindisplay
d:=\cases{\deg(\phat)-d_Q,&if $d_Q\ge d_R$;\cr
   \deg(\phat)-d_R+1,&if $d_Q<d_R$.\cr}
\enddisplay
\item{2b}If $d\ge0$, define $s(n,k)$ by \eq(|zeil-s|), and consider the
linear equations in $\alpha_0$, \dots,~$\alpha_d$, $\beta_0$, \dots~$\beta_l$
obtained by equating coefficients of powers of~$k$ in the fundamental
equation~\eq(|zeil-pqrs-rec|). If these equations have a solution with
$\beta_0$, \dots,~$\beta_l$ not all zero, go to Step~4. Otherwise, if
$d_Q<d_R$ and if $-2\lambda'/\lambda$ is an integer greater than~$d$,
where $\lambda$ is the coefficient of $k^{d_R}$ in $q+r$ and $\lambda'$ is
the coefficient of $k^{d_R-1}$ in $q-r$, set $d:=-2\lambda'/\lambda$ and
repeat Step~2b.
\item{3}(The term $\that(n,k)$ isn't hypergeometrically summable.)
Increase $l$ by~$1$ and go back to Step~1.
\item{4}(Success.) Set $T(n,k):=r(n,k)s(n,k)\tbar(n,k)/\pbar(n,k)$. The
algorithm has discovered that $\that(n,k)=T(n,k+1)-T(n,k)$.
\smallskip\noindent We'll prove later that this algorithm terminates
successfully whenever $t(n,k)$ belongs to a large class of terms
called proper terms.

The binomial theorem can be derived in many ways, so our first example of
the Gosper-Zeilberger approach was more instructive than impressive.
Let's tackle "Vandermonde's convolution" next. Can Gosper and Zeilberger
deduce algorithmically that $\sum_k{a\choose k}{b\choose n-k}$ has a
simple form? The algorithm starts with $l=0$, which essentially
reproduces Gosper's original algorithm, trying to see if ${a\choose k}
{b\choose n-k}$ is summable in hypergeometric terms. Surprise: That term
actually
does turn out to be summable, if $a+b$ is a specific nonnegative integer (see
exercise~|vdm-summable|). But we are interested in general values of
$a$ and~$b$, and the algorithm quickly discovers that the indefinite sum is
not a hypergeometric term in general. So $l$ is increased from $0$ to~$1$,
and the algorithm proceeds to try
$\that(n,k)=\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)$ instead.
The next step, as in our derivation of the binomial theorem, is to write
$\that(n,k)=p(n,k)@\tbar(n,k)$, where $p(n,k)$ is obtained by clearing
fractions in $t(n+1,k)/t(n,k)$. In this
case\dash---the reader should please work along on a piece of
scratch paper to check all these calculations\dash---they aren't as hard
as they look\dash---everything goes through in an analogous
fashion, but now with
\begindisplay
p(n,k)&=(n+1-k)\beta_0(n)+(b-n+k)\beta_1(n)=\phat(n,k)\,,\cr
\tbar(n,k)&=@t(n,k)/(n{+}1{-}k)
 =a!\,b!/(a{-}k)!\,k!\,(b{-}n{+}k)!\,(n{+}1{-}k)!\,,\cr
q(n,k)&=(n+1-k)(a-k)\,,\cr
r(n,k)&=(b-n+k)@k\,.\cr
\enddisplay
Step 2a finds $\deg(q-r)<\deg(q+r)$, and $d=\deg(\phat)-\deg(q+r)+1=0$,
so $s(n,k)$ is again independent of~$k$. Gosper's fundamental equation
\eq(|zeil-pqrs-rec|) is equivalent to two equations in three unknowns,
\g The crucial point is that the Gosper-Zeilberger method always leads to
equations that are linear in the unknown $\alpha$'s and $\beta$'s,
because the left side of \eq(|zeil-pqrs-rec|) is linear in the
$\beta$'s and the right side is linear in the $\alpha$'s.\g
\begindisplay
\vbox{\halign{$\hfil#@$&$#\hfil$&
              ${}\hfil#@$&$#\hfil$&
              ${}\hfil#@$&$#\hfil$&$=0\,#\hfil$\cr
(n+1)&\beta_0(n)&+(b-n)&\beta_1(n)&-(n+1)@a@&\alpha_0(n)&,\cr
-&\beta_0(n)&+&\beta_1(n)&+(a+b+1)&\alpha_0(n)&,\cr}}
\enddisplay
which have the solution
\begindisplay
\beta_0(n)=a+b-n\,,\qquad \beta_1(n)=-n-1\,,\qquad \alpha_0(n)=1\,.
\enddisplay
We conclude that $(a+b-n)t(n,k)-(n+1)t(n+1,k)$ is summable with respect
to~$k$; hence if
$S_n=\sum_k{a\choose k}{b\choose n-k}$ the recurrence
\begindisplay
S_{n+1}={a+b-n\over n+1}\,S_n
\enddisplay
holds; thus $S_n={a+b\choose n}$ since $S_0=1$. A piece of cake.

What about the "Saalsch\"utz"ian triple-binomial identity in
\eq(|bc-saalschutz|)? The proof of
\eq(|bc-saalschutz|) in exercise~|prove-saalschutz| is interesting,
but it requires inspiration. When we transform an art into a science, we aim
to replace inspiration by perspiration; so let's see if the Gosper-Zeilberger
approach to summation is able to discover and prove \eq(|bc-saalschutz|)
in a purely mechanical way. For convenience we make the substitutions
$m=b+d$, $n=a$, $r=a+b+c+d$, $s=a+b+c$, so that \eq(|bc-saalschutz|) takes
the more symmetrical form
\begindisplay
&\sum_k{(a+b+c+d+k)!\over (a-k)!\,(b-k)!\,(c+k)!\,(d+k)!\,k!}\cr
&\qquad={(a+b+c+d)!\,(a+b+c)!\,(a+b+d)!\over
 a!\,b!\,(a+c)!\,(a+d)!\,(b+c)!\,(b+d)!}\,.
\eqno\eqref|sym-saalschutz|
\enddisplay
To make the sum finite, we assume that either $a$ or $b$ is a nonnegative
integer.

Let $t(n,k)=(n+b+c+d+k)!/(n-k)!\,(b-k)!\,(c+k)!\,(d+k)!\,k!$ and
\g Deciding what parameter to call~$n$ is the only non-mechanical part.\g
$\that(n,k)=\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)$. Proceeding along a path that
is beginning to become well worn, we set
\begindisplay
p(n,k)&\advance\medmuskip-.3mu
 =\hbox{$(n+1-k)\beta_0(n)+(n+1+b+c+d+k)\beta_1(n)$}=\phat(n,k)\,,\cr
\tbar(n,k)&={t(n,k)\over n+1-k}
={(n+b+c+d+k)!\over(n+1-k)!\,(b-k)!\,(c+k)!\,(d+k)!\,k!}\,,\cr
q(n,k)&=(n+b+c+d+k+1)(n+1-k)(b-k)\,,\cr
r(n,k)&=(c+k)(d+k)k\,,\cr
\enddisplay
and we try to solve \eq(|zeil-pqrs-rec|) for $s(n,k)$. Again $\deg(q-r)<
\deg(q+r)$, but this time $\deg(\phat)-\deg(q+r)+1=-1$ so it looks like we're
stuck. However, Step 2b has an important second choice,
$d=-2\lambda'/\lambda$, for the degree of~$s$; we had
better try it now before we give up.
Here $R(n,k)=q(n,k)+r(n,k)=2k^3+\cdots\,$, so $\lambda=2$, while
the polynomial
$Q(n,k)=q(n,k)-r(n,k)$ almost miraculously turns out to have degree~$1$
\g Notice that $\lambda'$ is not the leading coefficient of~$Q$,
although~$\lambda$
is the leading coefficient of~$R$. The number~$\lambda'$ is the coefficient of
$k^{\deg(R)-1}$ in~$Q$.\g
in~$k$\dash---the coefficient of $k^2$ vanishes! Therefore $\lambda'=0$;
Gosper allows us to take $d=0$ and $s(n,k)=\alpha_0(n)$.

The equations to be solved are now
\begindisplay
&(n+1)\beta_0(n)+(n+1+b+c+d)\beta_1(n)\cr
\noalign{\vskip-1.5pt}
&\qquad{} -(n+1)(n+1+b+c+d)@b@\alpha_0(n)=0\,,\cr
&-\beta_0(n)+\beta_1(n)\cr
\noalign{\vskip-1.5pt}
&\qquad{} -\bigl((n+1)b-(n+1+b)(n+1+b+c+d)-cd\bigr)\alpha_0(n)=0\,;\cr
\enddisplay
and we find
\begindisplay
\beta_0(n)&=(n+1+b+c)(n+1+b+d)(n+1+b+c+d)\,,\cr
\beta_1(n)&=-(n+1)(n+1+c)(n+1+d)\,,\cr
\alpha_0(n)&=2n+2+b+c+d\,,\cr
\enddisplay
after only a modest amount of perspiration. The identity \eq(|sym-saalschutz|)
\g Perspiration flows, identity follows.\g
follows immediately.

A similar proof of \eq(|sym-saalschutz|) can be obtained if we work with
$n=d$ instead of $n=a$. (See exercise~|prove-saalschutz-again|.)
\par\goodbreak

The Gosper-Zeilberger approach helps us evaluate definite sums over a
restricted range as well as sums over all~$k$. For example, let's consider
\begindisplay
S_n(z)=\sum_{k=0}^n{n+k\choose k}z^k\,.
\eqno
\enddisplay
When $z=\half$ we obtained an ``"unexpected"'' result in \eq(|half/2|); would
Gosper and Zeilberger have expected it? Putting $t(n,k)={n+k\choose k}z^k$
leads us to
\begindisplay
p(n,k)&=(n+1)\beta_0(n)+(n+1+k)\beta_1(n)=\phat(n,k)\,,\cr
\tbar(n,k)&=@t(n,k)/(n+1)=(n+k)!\,z^k/@k!\,(n+1)!\,,\cr
q(n,k)&=(n+1+k)@z\,,\cr
r(n,k)&=k\,,\cr
\enddisplay
and $\deg(s)=\deg(\phat)-\deg(q-r)=0$. Equation \eq(|zeil-pqrs-rec|) is
solved by $\beta_0(n)=1$, $\beta_1(n)=z-1$, $s(n,k)=1$. Therefore we find
\begindisplay
t(n,k)+(z-1)t(n+1,k)=T(n,k+1)-T(n,k)\,,
\eqno
\enddisplay
where $T(n,k)=r(n,k)@s(n,k)@\that(n,k)/\phat(n,k)={n+k\choose k-1}z^k$. We
can now sum \thiseq\ for $0\le k\le n+1$, getting
\begindisplay \openup3pt
S_n(z)+t(n,n+1)+(z-1)@S_{n+1}(z)&=T(n,n+2)-T(n,0)\cr
&={2n+2\choose n+1}z^{n+2}\cr
&=2{2n+1\choose n}z^{n+2}\,.\cr
\enddisplay
But $t(n,n+1)={2n+1\choose n+1}z^{n+1}={2n+1\choose n}z^{n+1}$, so
\begindisplay
S_{n+1}(z)={1\over 1-z}\left(S_n(z)+(1-2z){2n+1\choose n}z^{n+1}\right).
\eqno
\enddisplay
We see immediately that the case $z=\half$ is special, and that $S_{n+1}(\half)
=2S_n(\half)$. Moreover, the recurrence \thiseq\ can be simplified by applying
the summation factor $(1-z)^{n+1}$ to both sides; this yields the general
identity
\begindisplay
(1-z)^n\sum_{k=0}^n{n+k\choose k}z^k=1+{1-2z\over 2-2z}\sum_{k=1}^n{2k\choose
k}\bigl(z(1-z)\bigr)^k\,,
\eqno
\enddisplay
which comparatively few people would have expected before Gosper and
Zeilberger came along. Now the production of such identities is routine.

How about the similar sum
\begindisplay
S_n(z)=\sum_{k=0}^n{n-k\choose k}z^k\,,
\eqno
\enddisplay
which we encountered in \eq(|bc-gen-fib|)? Flushed with confidence, we set
$t(n,k)={n-k\choose k}z^k$ and proceed to calculate
\begindisplay
p(n,k)&=(n+1-2k)\beta_0(n)+(n+1-k)\beta_1(n)=\phat(n,k)\,,\cr
\tbar(n,k)&=@t(n,k)/(n+1-2k)=(n-k)!\,z^k/@k!\,(n+1-2k)!\,,\cr
q(n,k)&=(n+1-2k)@(n-2k)@z\,,\cr
r(n,k)&=(n+1-k)@k\,.\cr
\enddisplay
But whoa\dash---there's no way to solve \eq(|zeil-pqrs-rec|), if we assume
\g $S_n(-{1\over4})$ equals\par\vskip2pt $(n+1)/2^n$.\g
that $z\ne-{1\over4}$, because the degree of $s$ would have to be
$\deg(\phat)-\deg(q-r)=-1$.

No problem. We simply add another parameter $\beta_2(n)$ and try
$\that(n,k)=\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)+\beta_2(n)t(n+2,k)$
instead:
\begindisplay
p(n,k)&=(n+1-2k)(n+2-2k)\beta_0(n)\cr
&\qquad{}+(n+1-k)(n+2-2k)\beta_1(n)\cr
&\qquad{}+(n+1-k)(n+2-k)\beta_2(n)=\phat(n,k)\,,\cr
\noalign{\goodbreak}
\tbar(n,k)&=@t(n,k)/(n{+}1{-}2k)(n{+}2{-}2k)=(n{-}k)!\,z^k
        /@k!\,(n{+}2{-}2k)!\,,\cr
q(n,k)&=(n+2-2k)(n+1-2k)@z\,,\cr
r(n,k)&=(n+1-k)@k\,.\cr
\enddisplay
Now we can try $s(n,k)=\alpha_0(n)$ and \eq(|zeil-pqrs-rec|) does have
a solution:
\begindisplay
\beta_0(n)=z\,,\qquad \beta_1(n)=1\,,\qquad \beta_2(n)=-1\,,\qquad \alpha_0(n)
=1\,.
\enddisplay
We have discovered that
\begindisplay
z@t(n,k)+t(n+1,k)-t(n+2,k)=T(n,k+1)-T(n,k)\,,
\enddisplay
where $T(n,k)$ equals $r(n,k)s(n,k)\that(n,k)/\phat(n,k)=(n+1-k)@k@\tbar(n,k)=
{n+1-k\choose k-1}z^k$. Summing from $k=0$ to $k=n$ gives
\begindisplay
&\textstyle zS_n(z)+\bigl(S_{n+1}(z)-{0\choose n+1}z^{n+1}\bigr)
-\bigl(S_{n+2}(z)-{0\choose n+2}z^{n+2}-{1\choose n+1}z^{n+1}\bigr)\cr
&\qquad=T(n,n+1)-T(n,0)\,.
\enddisplay
And ${1\choose n+1}={0\choose n}z^{n+1}=T(n,n+1)$ for all $n\ge0$, so we obtain
\begindisplay
S_{n+2}(z)=S_{n+1}(z)+z@S_n(z)\,,\qquad \hbox{$n\ge0$.}
\eqno
\enddisplay
We will study the solution of such recurrences in Chapters 6 and 7; the
methods of those chapters lead directly from \thiseq\ to the closed form
\eq(|bc-gen-fib|), when $S_0(z)=S_1(z)=1$.

One more example\dash---a famous one\dash---will complete the picture. The
French mathematician Roger "Ap\'ery" solved a long-standing problem in
1978 when he proved that the number "!zeta function" $\zeta(3)=
1+2^{-3}+3^{-3}+4^{-3}+\cdots\,$ is "irrational" [|apery|]. One of the main
components of his proof involved the binomial sums
\begindisplay
A_n=\sum_k\,{n\choose k}^2\,{n+k\choose k}^2,
\eqno\eqref|apery-sum|
\enddisplay
for which he announced a recurrence that other mathematicians were unable
to verify at the time. (The numbers $A_n$ have since become known as
"Ap\'ery numbers"; we have $A_0=1$, $A_1=5$, $A_2=73$, $A_3=1445$,
$A_4=33001$.) Finally [|vdP|] Don "Zagier" and Henri "Cohen" found a proof
of Ap\'ery's claim, and their proof for this special (but difficult) sum
was one of the key clues that ultimately led "Zeilberger" to discover the
general approach we are discussing.

By now, in fact, we have seen enough examples to make the sum in \thiseq\
almost trivial. Putting $t(n,k)={n\choose k}{}^2{n+k\choose k}{}^2$ and
\g (First we try doing without $\beta_2$, but that attempt quickly
peters out.)\g
$\that(n,k)=\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)+\beta_2(n)t(n+2,k)$, we
try to solve \eq(|zeil-pqrs-rec|) with
\begindisplay
p(n,k)&=(n+1-k)^2(n+2-k)^2\beta_0(n)\cr
&\qquad{}+(n+1+k)^2(n+2-k)^2\beta_1(n)\cr
&\qquad{}+(n+1+k)^2(n+2+k)^2\beta_2(n)=\phat(n,k)\,,\cr
\noalign{\goodbreak}
\tbar(n,k)&=@t(n,k)/(n{+}1{-}k)^2(n{+}2{-}k)^2=(n{+}k)!^2
  /@k!^4@(n{+}2{-}k)!^2\,,\cr
q(n,k)&=(n+1+k)^2(n+2-k)^2\,,\cr
r(n,k)&=k^4\,.\cr
\enddisplay
(We don't worry about the fact that $q$ has the factor $(k+n+1)$ while
$r$ has the factor~$k$; this does not violate \eq(|qr-condition|), because
we are regarding $n$ as a variable parameter, not a fixed integer.)
Since $q(n,k)-r(n,k)=-2k^3+\cdots\,$, we are allowed to set $\deg(s)=
-2\lambda'/\lambda=2$, so we take
\begindisplay
s(n,k)=\alpha_2(n)@k^2+\alpha_1(n)@k+\alpha_0(n)\,.
\enddisplay
With this choice of $s$, the recurrence
\eq(|zeil-pqrs-rec|) boils down to five equations
in the six unknown quantities $\beta_0(n)$, $\beta_1(n)$, $\beta_2(n)$,
$\alpha_0(n)$, $\alpha_1(n)$, $\alpha_2(n)$. For example,
the equation arising from the coefficients of $k^0$ simplifies to
\begindisplay
\beta_0+\beta_1+\beta_2-\alpha_0-\alpha_1-\alpha_2=0\,;
\enddisplay
the equation arising from the coefficients of $k^4$ is
\begindisplay
\beta_0+\beta_1+\beta_2+\alpha_1+(6+6n+2n^2)\alpha_2=0\,.
\enddisplay
The other three equations are more complicated. But the main point is that
these linear equations\dash---like all the equations that arise when we come
to this stage of the Gosper-Zeilberger algorithm\dash---are
{\it "homogeneous"\/} (their right-hand sides are~$0$).
So they always have a nonzero solution when the number of unknowns exceeds
the number of equations. A solution, in our case, turns out to be
\begindisplay
\beta_0(n)&=(n+1)^3\,,\cr
\beta_1(n)&=-(2n+3)(17n^2+51n+39)\,,\cr
\beta_2(n)&=(n+2)^3\,,\cr
\alpha_0(n)&=-16(n+1)(n+2)(2n+3)\,,\cr
\alpha_1(n)&=-12(2n+3)\,,\cr
\alpha_2(n)&=8(2n+3)\,.\cr
\enddisplay
Consequently
\begindisplay
&(n+1)^3t(n,k)-(2n+3)(17n^2+51n+39)t(n+1,k)\cr
&\qquad\qquad{}+(n+2)^3t(n+2,k)=T(n,k+1)-T(n,k)\,,
\enddisplay
where $T(n,k)=k^4s(n,k)\tbar(n,k)=(2n+3)(8k^2-12k-16(n+1)(n+2))\*
(n+k)!^2\!/(k-1)!^4(n+2-k)!^2$. Summing on $k$ gives Ap\'ery's
\g\noindent\llap{``}Professor "Littlewood", when he makes use of an
algebraic identity, always saves himself the trouble of proving it;
he maintains that an identity, if true, can be verified in a few lines
by anybody obtuse enough to feel the need of verification. My object
in the following pages is to confute this assertion.''\par
\hfill\dash---F.\thinspace J. "Dyson"~[|dyson|]\g
once-incredible recurrence,
\begindisplay
(n+1)^3A_n+(n+2)^3A_{n+2}=(2n+3)(17n^2{+}51n{+}39)A_{n+1}\,.
\eqno\eqref|apery-rec|
\enddisplay

Does the Gosper-Zeilberger method work with all the sums we've encountered
in this chapter? No. It doesn't apply when $t(n,k)$ is the summand
${n\choose k}(k+1)^{k-1}(n-k+1)^{n-k-1}$ in \eq(|t-e2|), because the term ratio
$t(n,k+1)/t(n,k)$
is not a rational function of~$k$. It also fails to handle cases like
$t(n,k)={n\choose k}n^k$, because the other term ratio
$t(n+1,k)/t(n,k)$ is not a rational
function of~$k$. (We can do that one, however, by summing ${n\choose k}z^k$
and then setting $z=n$.) And it fails on a comparatively simple summand like
$t(n,k)=1/(nk+1)$, even though both $t(n,k+1)/t(n,k)$ and $t(n+1,k)/t(n,k)$ are
rational functions of $n$ and~$k$; see exercise |not-holonomic|.

But the Gosper-Zeilberger algorithm is guaranteed
to succeed in an enormous number of cases, namely whenever the
summand $t(n,k)$ is a so-called {\it"proper term"}\dash---a term that
can be written in the form
\begindisplay\advance\medmuskip-2mu
t(n,k)=f(n,k){(a_1n+a'_1k+a''_1)!\,\ldots(a_pn+a'_pk+a''_p)!\over
              (b_1n+b'_1k+b''_1)!\,\ldots(b_qn+b'_qk+b''_q)!}\,w^nz^k\,.
\eqno\eqref|pt-def|
\enddisplay
Here $f(n,k)$ is a polynomial in $n$ and $k$; the coefficients $a_1$, $a'_1$,
\dots, $a_p$,~$a'_p$, $b_1$, $b'_1$, \dots, $b_q$, $b'_q$ are specific
integer constants;
the parameters $w$ and $z$ are nonzero;
and the other quantities $a''_1$, \dots,~$a''_p$,
$b''_1$, \dots,~$b''_q$ are arbitrary complex numbers.
We will prove that whenever $t(n,k)$
is a proper term, there exist polynomials $\beta_0(n)$, \dots,~$\beta_l(n)$,
not all zero, and a proper term~$T(n,k)$, such that
\g What happens if $t(n,k)$ is independent of~$n$?\g
\begindisplay
\beta_0(n)t(n,k)+\cdots+\beta_l(n)t(n+l,k)=T(n,k+1)-T(n,k)\,.
\eqno\eqref|pt-goal|
\enddisplay
The following proof is due to "Wilf" and "Zeilberger" [|wilf-zeil|].

Let $N$ be the operator that increases $n$ by $1$, and let $K$ be the operator
that increases $k$ by~$1$, so that, for example, $N^2K^3t(n,k)=t(n+2,k+3)$.
We will study "linear difference operators" in $N$, $K$, and~$n$, namely
operator polynomials of the form
\begindisplay
H(N,K,n)=\sum_{i=0}^I\sum_{j=0}^J\alpha_{i,j}(n)N^iK^j\,,
\eqno\eqref|ldo|
\enddisplay
where each $\alpha_{i,j}(n)$ is a polynomial in~$n$.
Our first observation is that, if $t(n,k)$ is any proper term and $H(N,K,n)$
is any linear difference operator, then $H(N,K,n)t(n,k)$ is a proper term.
Suppose $t$ and~$H$ are given respectively by \eq(|pt-def|) and \eq(|ldo|);
then we define a ``"base term"''
\begindisplay
\tbar(n,k)_{I,J}={\prod_{i=1_{\mathstrut}}^p
  \bigl(a_in+a'_ik+a_iI@\[a_i<0]+a'_iJ@\[a'_i<0]+a''_i\bigr)!\over
      \prod_{i=1}^{q^{\mathstrut}}
  \bigl(b_in+b'_ik+b_iI@\[b_i>0]+b'_iJ@\[b'_i>0]+b''_i\bigr)!}\,w^nz^k\,.
\enddisplay
For example, if $t(n,k)$ is ${n-2k\choose k}=(n-2k)!/
k!\,(n-3k)!$, the base term corresponding to a linear difference
operator of degrees $I$ and~$J$ is $\tbar(n,k)_{I,J}=(n-2k-2J)!/(k+J)!\,
(n-3k+I)!$. The point is that $\alpha_{i,j}(n)N^iK^jt(n,k)$ is equal to
$\tbar(n,k)_{I,J}$ times a polynomial in $n$ and~$k$, whenever $0\le i\le I$
and $0\le j\le J$. A finite sum of polynomials is a polynomial, so
$H(N,K,n)t(n,k)$ has the required form~\eq(|pt-def|).

The next step is to show that whenever $t(n,k)$ is a proper term, there is
always a nonzero linear difference operator $H(N,K,n)$ such that
\begindisplay
H(N,K,n)@t(n,k)=0\,.
\enddisplay
If $0\le i\le I$ and $0\le j\le J$, the shifted term
$N^iK^jt(n,k)$ is $\tbar(n,k)_{I,J}$ times a polynomial in $n$ and~$k$ that
has degree at most
\begindisplay
D_{I,J}&=\deg(f)+\vert a_1\vert@I+\vert a'_1\vert@@J+\cdots
               +\vert a_p\vert@I+\vert a'_p\vert@@J\cr
     &\hskip6em{}+\vert b_1\vert@I+\vert b'_1\vert@@J+\cdots
               +\vert b_q\vert@I+\vert b'_q\vert@@J
\enddisplay
in the variable~$k$. Hence the desired $H$ exists if we can solve $D_{I,J}+1$
homo\-geneous linear equations in the $(I+1)(J+1)$ variables $\alpha_{i,j}(n)$,
with coefficients that are polynomials in~$n$. All we need to do is choose
$I$ and~$J$ large enough that $(I+1)(J+1)>D_{I,J}+1$. For example, we can
take $I=2A'+1$ and $J=2A+\deg(f)$, where
%$A=\vert a_1\vert+\cdots+\vert a_p\vert+\vert b_1\vert+\cdots+\vert b_q\vert$
%and $A'=\vert a'_1\vert+\cdots+\vert a'_p\vert+\vert b'_1\vert+\cdots+
%\vert b'_q\vert$.
%  I'm displaying this just to avoid an underfull page...
\begindisplay
A&=\vert a_1\vert+\cdots+\vert a_p\vert
 +\vert b_1\vert+\cdots+\vert b_q\vert\,;\cr
A'&=\vert a'_1\vert+\cdots+\vert a'_p\vert+
 \vert b'_1\vert+\cdots+\vert b'_q\vert\,.\cr
\enddisplay

The last step in the proof is to go from the equation $H(N,K,n)@t(n,k)=0$ to
a solution of \eq(|pt-goal|). Let $H$ be chosen so that $J$~is minimized,
i.e., so that $H$ has the smallest possible degree in~$K$. We can write
\g The trick here is based on regarding $H$ as a polynomial in~$K$ and then
replacing $K$ by $\Delta+1$.\g
\begindisplay
H(N,K,n)=H(N,1,n)-(K-1)G(N,K,n)
\enddisplay
for some linear difference operator $G(N,K,n)$. Let $H(N,1,n)=\beta_0(n)
+\beta_1(n)N+\cdots+\beta_l(n)N^l$ and $T(n,k)=G(N,K,n)@t(n,k)$. Then
$T(n,k)$ is a proper term, and \eq(|pt-goal|) holds.

The proof is almost complete; we still have to verify that $H(N,1,n)$ is
not simply the zero operator. If it is, then $T(n,k)$ is independent of~$k$.
So there are polynomials $\beta_0(n)$ and $\beta_1(n)$ such that
$\bigl(\beta_0(n)+\beta_1(n)N\bigr)T(n,k)=0$. But then
$\bigl(\beta_0(n)+\beta_1(n)N\bigr)G(N,K,n)$ is a nonzero linear difference
operator of degree~$J-1$ that annihilates~$t(n,k)$; this contradicts
the minimality of~$J$, and our proof of \eq(|pt-goal|) is complete.

Once we know that \eq(|pt-goal|) holds, for some proper term $T$,
we can be sure that
Gosper's algorithm will succeed in finding $T$ (or $T$ plus a constant).
Although we proved Gosper's algorithm only for the case of hypergeometric
terms $t(k)$ in a single variable~$k$, our proof can be extended to
the two-variable case, as follows: There are infinitely many complex
numbers $n$ for which condition \eq(|qr-condition|) holds when $q(n,k)$ and
$r(n,k)$ are completely factored as polynomials in $k$, and for which the
calculations of $d$ in Step~2 agree with the calculations of Gosper's
one-variable algorithm. For all such~$n$, our
previous proof shows that
a suitable polynomial $s(n,k)$ in~$k$ exists; therefore a suitable
polynomial $s(n,k)$ in~$n$ and~$k$ exists; QED.

We have proved that the Gosper-Zeilberger algorithm will discover a solution
to \eq(|pt-goal|), for some~$l$, where $l$ is as small as possible.
That solution gives us a recurrence
in~$n$ for evaluating the sum over~$k$ of any proper term $t(n,k)$, provided
that $t(n,k)$ is nonzero for only finitely many~$k$. And the
roles of $n$ and~$k$ can, of course, be reversed, because the definition
of proper term in \eq(|pt-def|) is symmetrical in $n$ and~$k$.

Exercises |prove-saalschutz-again|--|apery-mn|
provide additional examples of the Gosper-Zeilberger
algorithm, illustrating some of its versatility. "Wilf" and "Zeilberger"~%
[|wilf-zeil|] have significantly extended these results to methods that
handle generalized binomial coefficients and multiple indices of summation.

\beginexercises

\subhead \kern-.05em Warmups

\ex:
What is $11^{4\,}$? Why is this number
easy to compute, for a person who knows binomial coefficients?
\answer $(11)_r^4=(14641)_r$, in any number system of radix $r\ge7$,
\g What's $11^4$ in radix~$11$?\g
because of the binomial theorem.
\source{"Forcadel" [|forcadel|].}

\ex:
For which value(s) of $k$ is $n\choose k$ a maximum,
when $n$ is a given positive integer?
Prove your answer.
\answer The ratio ${n\choose k+1}\big/
{n\choose k}=(n-k)/(k+1)$ is $\le1$ when $k\ge\lfloor n/2\rfloor$
and $\ge1$ when $k<\lceil n/2\rceil$, so the maximum occurs when
$k=\lfloor n/2\rfloor$ and $k=\lceil n/2\rceil$.

\ex:
Prove the "hexagon property",
\begindisplay
{n-1\choose k-1}{n\choose k+1}{n+1\choose k}
={n-1\choose k}{n+1\choose k+1}{n\choose k-1}\,.
\enddisplay
\answer Expand into factorials. Both products are equal to $f(n)
/f(n-k)@f(k)$,
where $f(n)=(n+1)!\,n!\,(n-1)!$.
\source{"Long" and "Hoggatt" [|long-hoggatt|].}

\ex:\exref|-1-choose-k|%
Evaluate $-1\choose k$ by negating (actually un-negating) its upper index.
\answer ${-1\choose k}=(-1)^k{k+1-1\choose k}=(-1)^k{k\choose k}
=(-1)^k\[k\ge0]$.

\ex:
Let $p$ be prime. Show that ${p\choose k}\bmod p=0$ for $0<k<p$.
What does this imply about the binomial coefficients $p-1\choose k$?
\answer If $0<k<p$, there's a $p$ in the numerator of $p\choose k$
with nothing to cancel it in the denominator. Since ${p\choose k}
={p-1\choose k}+{p-1\choose k-1}$, we must have ${p-1\choose k}\=
(-1)^k$ \tmod p, for $0\le k< p$.
\source{1983 in-class final.}

\ex:\exref|fix-symm-error|%
Fix up the text's derivation in Problem~6, Section 5.2,
\g A case of\par mistaken identity.\g
 by correctly applying symmetry.
\answer The crucial step (after second down) should be
\begindisplay
&{1\over n+1} \sum_k {n+k \choose k} {n+1 \choose k+1} (-1)^k\cr
&\qquad= {1\over n+1} \sum_{k\ge0}{n+k \choose n} {n+1 \choose k+1} (-1)^k\cr
&\qquad= {1\over n+1}\sum_k{n+k \choose n} {n+1 \choose k+1} (-1)^k\cr
&\qquad\qquad -{1\over n+1}{n-1\choose n}{n+1\choose0}(-1)^{-1}\,.
\enddisplay
The original derivation forgot to include this extra term, which is $\[n=0]$.

\ex:
Is \eq(|half-fact|) true also when $k<0$?
\answer Yes, because $r\_{-k}=(-1)^k\!/(-r-1)\_k$. We also have
\begindisplay
\textstyle r\_^k(r+\half)\_^k=(2r)\_^{2k}\!/2^{2k}\,.
\enddisplay

\ex:
Evaluate
\begindisplay
\sum_k{n\choose k}(-1)^k(1-k/n)^n\,.
\enddisplay
What is the approximate value
of this sum, when $n$ is very large? \Hint: The sum is $\Delta^{n\,}f(0)$ for
some function~$f$.
\answer $f(k)=(k/n-1)^n$ is a polynomial of degree $n$ whose leading
coefficient is $n^{-n}$. By~\equ(5.|nth-diff|), the sum is $n!/n^n$. When $n$~is large,
Stirling's approximation says that this is approximately $\sqrt{2\pi n}/e^n$.
(This is quite different from ${(1-1/e)}$, which is what we get
if we use the approximation $(1-k/n)^n\sim e^{-k}$, valid for
fixed~$k$ as $n\to\infty$.)

\ex:
Show that the generalized exponentials of \eq(|t-series-def|) obey
the law
\begindisplay
\Escr_t(z)=\Escr(tz)^{1/t}\,,\qquad\hbox{if $t\ne 0$},
\enddisplay
where $\Escr(z)$ is an abbreviation for $\Escr_1(z)$.
\answer $\Escr_t(z)^t=\sum_{k\ge0}t(tk+t)^{k-1}z^k\!/k!
=\sum_{k\ge0}(k+1)^{k-1}(tz)^k\!/k!=\Escr_1(tz)$, by \equ(5.|t-series-power|).

\ex:
Show that $-2\bigl(\ln(1-z)+z\bigr)/z^2$ is a hypergeometric function.
\answer $\sum_{k\ge0}2z^k\!/(k+2)=F(2,1;3;z)$, since $t_{k+1}/t_k=(k+2)z/(k+3)$.

\ex:\exref|sine-arcsine|%
Express the two functions
\begindisplay
\sin z&=z-{z^3\over3!}+{z^5\over5!}-{z^7\over7!}+\cdots\cr
\noalign{\nobreak\smallskip}
\mathop{\rm arcsin}z&=z+{1\cdt z^3\over2\cdt3}+{1\cdt3\cdt z^5\over2\cdt4\cdt5}
	+{1\cdt3\cdt5\cdt z^7\over2\cdt4\cdt6\cdt7}+\cdots\cr
\enddisplay
in terms of hypergeometric series.
\answer The first is Besselian and the second is Gaussian:
\g But not Imbesselian.\kern-1pt\g
\begindisplay
z^{-1}\sin z&=\textstyle
 \sum_{k\ge0}(-1)^kz^{2k}\!/(2k+1)!=F(1;1,{3\over2};-z^2\!/4)\,;\cr
z^{-1}\mathop{\rm arcsin}z&=\textstyle
 \sum_{k\ge0}z^{2k}(\half)\_^k\!/(2k+1)k!=F(\half,\half;{3\over2};z^2)\,.\cr
\enddisplay

\ex:\exref|ht-warmup|
Which of the following functions of $k$ is a hypergeometric term,
as defined in Section 5.7? Explain why or why not.
\itemitem{a}$n^k$.
\itemitem{b}$k^n$.
\itemitem{c}$\bigl(k!+(k+1)!\bigr)/2$.
\itemitem{d}$H_k$, that is, ${1\over1}+{1\over2}+\cdots+{1\over k}$.
\itemitem{e}$1/{n\choose k}$.
\itemitem{f}$t(k)@T(k)$, when $t$ and $T$ are hypergeometric terms.
\g(Here $t$ and $T$ aren't necessarily related as in \eq(|gosper-goal|).)\g
\itemitem{g}$t(k)+T(k)$, when $t$ and $T$ are hypergeometric terms.
\itemitem{h}$t(n-k)$, when $t$ is a hypergeometric term.
\itemitem{i}$a\,t(k)+b\,t(k{+}1)+c\,t(k{+}2)$,
 when $t$ is a hypergeometric term.
\itemitem{j}$\lceil k/2\rceil$.
\itemitem{k}$k\,\[k>0]$.
\answer (a) Yes, if $n\ne0$, since the term ratio is $n$.
(b)~Yes, when $n$ is an integer; the term ratio is $(k+1)^n\!/k^n$. Notice
that we get this term from \equ(5.|hypk-def|) by setting $m=n+1$,
$a_1=\cdots=a_m=1$, $b_1=\cdots=b_n=0$, $z=1$, and multiplying by $0^n$.
"!hypergeometric term"
\g Each value of a hypergeometric term $t(k)$ can be written
$0^{e(k)}v(k)$, where $e(k)$ is an integer and $v(k)\ne0$. Suppose the
term ratio $t(k+1)/t(k)$ is $p(k)/q(k)$, and that $p$ and $q$ have been
completely factored over the complex numbers. Then, for each~$k$,
$e(k+1)$ is $e(k)$ plus the number of zero factors of $p(k)$ minus the
number of zero factors of $q(k)$, and $v(k+1)$ is $v(k)$ times the product
of the nonzero factors of $p(k)$ divided by the product of the nonzero
factors of~$q(k)$.\g
(c)~Yes, the term ratio is $(k+1)(k+3)/(k+2)$. (d)~No, the term ratio is
$1+1/(k+1)H_k$; and $H_k\sim \ln k$ isn't a rational function.
(e)~Yes, the reciprocal of any hypergeometric term is a hypergeometric term.
The fact that $t(k)=\infty$ when $k<0$ or $k>n$ does not exclude $t(k)$
from hypergeometric termhood.
(f)~Of course.
(g)~Not when, say, $t(k)=2^k$ and $T(k)=1$.
(h)~Yes; the term ratio $t(n-1-k)/t(n-1-(k+1))$ is a rational function (the
reciprocal of the term ratio for $t$, with $k$ replaced by $n-1-k$), for
arbitrary~$n$.
(i)~Yes; the term ratio can be written
\begindisplay
{a\,t(k{+}1)/t(k)+b\,t(k{+}2)/t(k)+c\,t(k{+}3)/t(k)\over
a+b\,t(k{+}1)/t(k)+c\,t(k{+}2)/t(k)}\,,
\enddisplay
and $t(k+m)/t(k)=\bigl(t(k+m)/t(k+m-1)\bigr)\ldots\bigl(t(k+1)/t(k)\bigr)$
is a rational function of~$k$.
(j)~No. Whenever two rational functions $p_1(k)/q_1(k)$ and $p_2(k)/q_2(k)$ are
equal for infinitely many~$k$, they are equal for all~$k$, because
$p_1(k)q_2(k)=q_1(k)p_2(k)$ is a polynomial identity. Therefore the term
ratio $\lceil(k+1)/2\rceil/\lceil k/2\rceil$ would have to equal~$1$ if
it were a rational function.
(k)~No. The term ratio would have to be $(k+1)/k$, since it is $(k+1)/k$ for
all $k>0$; but then $t(-1)$ can be
zero only if $t(0)$ is a multiple of $0^2$, while $t(1)$ can be~$1$ only
if $t(0)=0^1$.

\subhead Basics

\ex:\exref|hyperfactorial-def|%
Find relations between the "superfactorial" function $P_n=\prod_{k=1}^n k!$
of exercise 4.|superfactorial-factors|, the "hyperfactorial" function
"!Pascal's triangle, row products"
$Q_n=\prod_{k=1}^n k^k$, and the product $R_n=\prod_{k=0}^n{n\choose k}$.
\answer $R_n=n!^{n+1}\!/P_n^2=Q_n/P_n=Q_n^2\!/n!^{n+1}$.
\source{1975 midterm.}

\ex:
Prove identity \eq(|bc-prod4|) by negating the upper index in
Vandermonde's convolution \eq(|bc-prod1|). Then show that another
negation yields \eq(|bc-prod5|).
\answer The first factor in \equ(5.|bc-prod4|) is ${l-k\choose l-k-m}$
when $k\le l$, so it's $(-1)^{l-k-m}\*{-m-1\choose l-k-m}$. The
sum for $k\le l$ is the sum over all~$k$, since $m\ge0$. (The condition
$n\ge0$ isn't really needed, although $k$ must assume
negative values if $n<0$.)\par To go from \equ(5.|bc-prod4|) to \equ(5.|bc-prod5|),
first replace $s$ by $-1-n-q$.
\source{[|knuth1|, exercise 1.2.6--20].}

\ex:
What is $\sum_k{n\choose k}^{\!3}(-1)^{k\,}$? \Hint: See \eq(|bc-dixon|).
\answer If $n$ is odd, the sum is zero, since we can replace $k$
by~$n-k$. If $n=2m$, the sum is $(-1)^m(3m)!/m!^3$, by \equ(5.|bc-dixon|)
with $a=b=c=m$.
\source{"Dixon" [|dixon|].}

\ex:
Evaluate the sum
\begindisplay
\sum_k{2a\choose a+k}{2b\choose b+k}{2c\choose c+k}(-1)^k
\enddisplay
when $a,b,c$ are nonnegative integers.
\answer This is just $(2a)!\,(2b)!\,(2c)!/(a+b)!\,(b+c)!\,(c+a)!$ times
\equ(5.|bc-dixon|), if we write the summands in terms of factorials.

\ex:
Find a simple relation between $2n-1/2@\choose n$ and $2n-1/2@\choose 2n$.
\answer The formulas ${2n-1/2@\choose n}={4n\choose 2n}/2^{2n}$ and
${2n-1/2@\choose 2n}={4n\choose 2n}/2^{4n}$ yield
${2n-1/2@\choose n}=2^{2n}{2n-1/2@\choose 2n}$.

\ex:
Find an alternative form analogous to \equ(5.|half-bc|) for the product
\begindisplay
{r\choose k}{r-1/3\choose k}{r-2/3\choose k}\,.
\enddisplay
\answer ${3r\choose 3k}{3k\choose k,k,k}/3^{3k}$.

\ex:
Show that the generalized binomials of \eq(|t-series-def|) obey the law
\begindisplay
\Bscr_t(z)=\Bscr_{1-t}(-z)^{-1}\,.
\enddisplay
\answer $\Bscr_{1-t}(-z)^{-1}=\sum_{k\ge0}{k-tk-1\choose k}\bigl(-1/(k-tk-1)\bigr)
(-z)^k$, by \equ(5.|t-series-power|), and this is
$\sum_{k\ge0}{tk\choose k}\bigl(1/(tk-k+1)\bigr)z^k=\Bscr_t(z)$.

\ex:
Define a ``generalized "bloopergeometric series"'' by the formula
\begindisplay \mathcode`F=\mathcode`G
\hyp{a_1,\, \ldots,\, a_m}{b_1,\, \ldots,\, b_n}z
	= \sum_{k \geq 0}
 {a_1\_k\ldots a_m\_k \over b_1\_k\ldots b_n\_k}
	\,{z^k\over k!\vphantom{b_1\_k}} \,,
\enddisplay
using falling powers instead of the rising ones
in \eq(|hyp-def|). Explain how
$G$~is related to~$F$.
\answer It equals $F(-a_1,\ldots,-a_m;-b_1,\ldots,-b_n;(-1)^{m+n}z)$;
see exercise~2.|rising-and-falling|.

\ex:\exref|factorial-def|%
Show that "Euler"'s definition of "factorial"s is consistent with the ordinary
definition, by showing that the limit in \eq(|f-def-lim|)
is $1\big/\bigl((m-1)\ldots(1)\bigr)$ when $z=m$ is a positive integer.
\answer $\lim_{n\to\infty}(n+m)\_m/n^m=1$.
\source{"Euler" [|euler-factorial|].}

\ex:\exref|factorial-dup|%
Use \eq(|f-def-lim|) to prove the {\it factorial "duplication formula"\/}:
\g By the way,\smallskip $(-\half)!=\sqrt\pi$.\g
\begindisplay
\textstyle x!\,(x-\half)!=(2x)!\,(-\half)!/2^{2x}\,.
\enddisplay
\answer Multiplying and dividing instances of \equ(5.|f-def-lim|) gives
\begindisplay \openup3pt
{(-1/2)!\over x!\,(x-1/2)!}
 &=\lim_{n\to\infty}{n+x\choose n}{n+x-1/2\,\choose n}n^{-2x}\bigg/
{n-1/2\,\choose n}\cr
 &=\lim_{n\to\infty}{2n+2x\choose 2n}n^{-2x}\,,
\enddisplay
by \equ(5.|half-fact|) and \equ(5.|n-1/2-bc|). Also
\begindisplay
1/(2x)!=\lim_{n\to\infty}{2n+2x\choose 2n}(2n)^{-2x}\,.
\enddisplay
Hence, etc. The "Gamma function" equivalent, incidentally, is
\begindisplay
\textstyle \Gamma(x)\,\Gamma(x+\half)=\Gamma(2x)\,\Gamma(\half)/2^{2x-1}\,.
\enddisplay

\ex:
What is the value of $F(-n,1;;1)$?
\answer $(-1)^n n\?\,$, see \equ(5.|subfactorial-sum|).

\ex:
Find $\sum_k{n\choose m+k}{m+k\choose2k}4^k$ by using hypergeometric
series.
\answer This sum is ${n\choose m}\hyp{m-n,-m}{1/2}1={2n\choose2m}$,
by \equ(5.|half-bc|) and \equ(5.|Fa-nc|).

\ex:\exref|increase-denominator|%
Show that
\begindisplay \openup3pt\advance\abovedisplayskip-3pt
&(a_1-b_1)\hyp{a_1,\,a_2,\,\ldots,\,a_m}{b_1{+}1,\,b_2,\,\ldots,\,b_n}z\cr
&\qquad=a_1\hyp{a_1{+}1,\,a_2,\,\ldots,\,a_m}{b_1{+}1,\,b_2,\,\ldots,\,b_n}z
-b_1\hyp{a_1,\,a_2,\,\ldots,\,a_m}{b_1,\,b_2,\,\ldots,\,b_n}z\,.
\enddisplay
Find a similar relation between the hypergeometrics
\begindisplay\openup5pt\advance\abovedisplayskip-4pt
&\hyp{a_1,\,a_2,\,a_3,\,\ldots,\,a_m}{b_1,\,\ldots,\,b_n}z\,,\cr
&\hyp{a_1+1,\,a_2,\,a_3,\,\ldots,\,a_m}{b_1,\,\ldots,\,b_n}z\,,
\qquad\hbox{and}\cr
&\hyp{a_1,\,a_2+1,\,a_3,\,\ldots,\,a_m}{b_1,\,\ldots,\,b_n}z\,.
\enddisplay
\answer This is equivalent to the easily proved identity
\begindisplay
(a-b){a\_^k\over(b+1)\_^k\hypstrut}
=a\,{(a+1)\_^k\over(b+1)\_^k\hypstrut}-b\,{a\_^k\over b\_^k\hypstrut}
\enddisplay
as well as to the operator formula $a-b=(\vartheta+a)-(\vartheta+b)$.\par
Similarly, we have
\begindisplay \openup6pt\advance\abovedisplayskip-9pt
&(a_1-a_2)\hyp{a_1,a_2,a_3,\,\ldots,\,a_m}{b_1,\,\ldots,\,b_n}z\cr
&\ \!=a_1\hyp{a_1{+}1,a_2,a_3,\ldots,a_m}{b_1,\,\ldots,\,b_n}z
-a_2\hyp{a_1,a_2{+}1,a_3,\ldots,a_m}{b_1,\,\ldots,\,b_n}z\,,
\enddisplay
because $a_1-a_2=(a_1+k)-(a_2+k)$. If $a_1-b_1$ is a nonnegative integer~$d$,
this second identity allows us to express
$F(a_1,\ldots,a_m;b_1,\ldots,b_n;z)$ as a linear combination of
$F(a_2+j,a_3,\ldots,a_m;b_2,\ldots,b_n;z)$ for $0\le j\le d$, thereby eliminating
an upper parameter and a lower parameter. Thus, for example,
we get closed forms for $F(a,b;a-1;z)$, $F(a,b;a-2;z)$, etc.\par
"Gauss" [|gauss-hyp|, \S7]
derived analogous relations
between $F(a,b;c;z)$ and any two ``"contiguous"''
hypergeometrics in which a parameter has been changed by $\pm1$.
"Rainville" [|rainville|] generalized this to cases with more parameters.
\source{"Gauss" [|gauss-hyp|, \S7].}

\ex:\exref|omit-first-term|%
Express the function $G(z)$ in the formula
\begindisplay
\hyp{a_1,\, \ldots,\, a_m}{b_1,\, \ldots,\, b_n}z=1+G(z)
\enddisplay
as a multiple of a hypergeometric series.
\answer If the term ratio in the original hypergeometric series is
$t_{k+1}/t_k=r(k)$, the term ratio in the new one is $t_{k+2}/t_{k+1}=r(k+1)$.
Hence
\begindisplay
\hyp{a_1,\, \ldots,\, a_m}{b_1,\, \ldots,\, b_n}z=1+
{a_1\ldots a_m\,z\over b_1\ldots b_n}\hyp{a_1+1,\ldots,a_m+1,1}
 {b_1+1,\ldots,b_n+1,2}z\,.
\enddisplay

\ex:
Prove that
\begindisplay \advance\abovedisplayskip-7pt \openup7pt
&\hyp{a_1,\,a_1+\half,\,\ldots,\,a_m,\,a_m+\half}
 {b_1,\,b_1+\half,\,\ldots,\,b_n,\,b_n+\half,\,\half\,}{(2^{m-n-1}z)^2}\cr
&\qquad=\half\biggl(\hyp{2a_1,\ldots,2a_m}{2b_1,\ldots,2b_n}z+
      \hyp{2a_1,\ldots,2a_m}{2b_1,\ldots,2b_n}{-z}\biggr)\,.
\enddisplay
\answer This is the sum of the even terms of $F(2a_1,\ldots,2a_m;
2b_1,\ldots,2b_m;z)$. We have $(2a)\_^{2k+2}\!/(2a)\_^{2k}=4(k+a)(k+a+\half)$,
etc.

\ex:
Prove {\it "Euler's identity"}
\begindisplay
\hyp{a,b}cz=(1-z)^{c-a-b}\hyp{c-a,\,c-b}cz
\enddisplay
by applying Pfaff's reflection law \eq(|hyp-refl|) twice.
\answer\g Equating coefficients of $z^n$ gives the "Pfaff"-"Saalsch\"utz"
formula \equ(5.|hyp-saalschutz|).\g
 $\hyp{a,\,b}cz=(1-z)^{-a}\hyp{a,\,c-b}c{{-z\over1-z}}
=(1-z)^{-a}\hyp{c-b,\,a}c{{-z\over1-z}}=({1-z})^{c-a-b}\hyp{c-a,\,c-b}cz$.
("Euler" proved the identity by showing that both sides satisfy the
same differential equation. The reflection law is often attributed
to Euler, but it does not seem to appear in his published papers.)
\source{"Euler" [|euler-hyp|].}

\ex:
Show that confluent hypergeometrics satisfy
\begindisplay
e^z\hyp ab{-z\,}=\hyp{b-a}b{z\,}\,.
\enddisplay
\answer The coefficients of $z^n$ are equal, by Vandermonde's convolution.
("Kummer"'s original proof was different: He considered $\lim_{m\to\infty}
F(m,b-a;b;z/m)$ in the reflection law \equ(5.|hyp-refl|).)
\source{"Kummer" [|kummer|, eq.~26.4].}

\ex:
What hypergeometric series $F$ satisfies $zF'(z)+F(z)=1/(1-z)$?
\answer Differentiate again to get $z(1-z)F''(z)+(2-3z)F'(z)-F(z)=0$.
Therefore $F(z)=F(1,1;2;z)$ by \equ(5.|gauss-diff-eq|).

\ex:
Show that if $f(k)$ is any function summable in hypergeometric terms,
then $f$ itself is a hypergeometric term. For example,
if $\sum f(k)\,\delta k=cF(A_1,\ldots,A_M;B_1,\ldots,B_N;Z)_k+C$,
then there are constants
$a_1$, \dots,~$a_m$, $b_1$,~\dots,~$b_n$, and~$z$ such that $f(k)$ is a
multiple of \eq(|hypk-def|).
\answer The condition $f(k)=T(k+1)-T(k)$ implies that $f(k+1)/f(k)=
\bigl(T(k+2)/T(k+1)-1\bigr)\big/\bigl(1-T(k)/T(k+1)\bigr)$ is a rational
function of~$k$.
\source{"Gosper" [|gosper|].}

\ex:
Find $\sum k^2\,\delta k$ by "Gosper's method".
\answer When summing a polynomial in $k$, Gosper's method reduces to the
``method of undetermined coefficients.\qback'' We have $q(k)=r(k)=1$,
and we try
to solve $p(k)=s(k+1)-s(k)$. The method suggests letting $s(k)$
be a polynomial whose degree is $d=\deg(p)+1$.

\ex:
Use Gosper's method to find $\sum\delta k/(k^2-1)$.
\answer The solution to $k=(k-1)@s(k+1)-(k+1)@s(k)$ is $s(k)=-k+\half$;
hence the answer is $(1-2k)/2k(k-1)+C$.

\ex:
Show that a partial hypergeometric sum can always be represented as a
limit of ordinary hypergeometrics:
\begindisplay
\sum_{k\le c}\hypk_k{a_1,\,\ldots,\,a_m}{b_1,\,\ldots,\,b_n}z
=\lim_{\epsilon\to0}\hyp{-c,\,a_1,\,\ldots,\,a_m}%
 {\epsilon-c,\,b_1,\,\ldots,\,b_n}z\,,
\enddisplay
when $c$ is a nonnegative integer. (See \eq(|hypk-def|.)
Use this idea to evaluate $\sum_{k\le m}{n\choose k}(-1)^k$.
\answer The limiting relation holds because all terms for $k>c$ vanish,
and $\epsilon-c$ cancels with $-c$ in the limit of the other terms. Therefore
the second partial sum is $\lim_{\epsilon\to0}F(-m,-n;\epsilon-m;1)=
\lim_{\epsilon\to0}(\epsilon+n-m)\_^m\!/(\epsilon-m)\_^m=(-1)^m{n-1\choose m}$.
\source{"Bailey" [|bailey|, \S10.4].}

\subhead Homework exercises

\ex:
The notation $\sum_{k\le n}{n\choose k}2^{k-n}$ is ambiguous without context.
Evaluate it
\smallskip
\itemitem{a}as a sum on $k$;
\itemitem{b}as a sum on $n$.
\answer (a) $2^{-n}3^n\[n\ge0]$.
(b) $(1-\half)^{-k-1}\[k\ge0]=2^{k+1}\[k\ge0]$.

\ex:\exref|bc-div-p|%
Let $p^k$ be the largest power of the prime $p$ that divides
"!exactly divides"
${m+n\choose m}$, when $m$ and~$n$ are nonnegative integers.
Prove that $k$ is the number of carries that occur when
$m$ is added to~$n$ in the radix~$p$ number system.
\Hint: Exercise 4.|epsilon-nu| helps here.
\answer The sum of the digits of $m+n$ is the sum of the digits of~$m$
plus the sum of the digits of~$n$, minus $p-1$ times the number of carries,
because each carry decreases the digit sum by $p-1$. [See~[|kw-carry|] for
extensions of this result to "generalized binomial coefficients".]
\source{"Kummer" [|kummer-carry|, p.~116].}

\ex:\exref|binomial-to-factorial|%
Show that an analog of the binomial theorem holds for factorial powers.
That is, prove the identities
\begindisplay \openup3pt
(x+y)\_n=\sum_k{n\choose k}x\_k@\,y\_{n-k}\,,\cr
(x+y)\_^n=\sum_k{n\choose k}x\_^k\,y\_^{n-k}\,,\cr
\enddisplay
for all nonnegative integers $n$.
\answer Dividing the first identity by $n!$ yields ${x+y\choose n}=
\sum_k{x\choose k}{y\choose n-k}$, Vandermonde's convolution. The second
identity follows, for example, from the formula $x\_^k=(-1)^k(-x)\_k$
if we negate both $x$ and~$y$.
\source{"Vandermonde" [|vandermonde|].}

\ex:
Show that all nonnegative integers $n$ can be represented uniquely
in the form $n={a\choose1}+{b\choose2}+{c\choose3}$ where $a$, $b$,
and~$c$ are integers with $0\le a<b<c$. (This is called the
{\it"binomial number system"}.)
\answer Choose $c$ as large as possible such that ${c\choose3}\le n$.
Then $0\le n-{c\choose 3}<{c+1\choose3}-{c\choose3}={c\choose 2}$;
replace $n$ by $n-{c\choose 3}$ and continue in the same fashion.
Conversely, any such representation is obtained in this way. (We can
do the same thing with
\begindisplay
n={a_1\choose 1}+{a_2\choose 2}+\cdots+{a_m\choose m},\qquad
\hbox{$0\le a_1<a_2<\cdots<a_m$}
\enddisplay
for any fixed $m$.)
\source{[|knuth1|, exercise 1.2.6--16].}

\ex:\exref|ax+by-expansion|%
Show that if $xy=ax+by$ then
\begindisplay
x^ny^n=\sum_{k=1}^n{2n-1-k\choose n-1}
(a^nb^{n-k}x^k+a^{n-k}b^ny^k)
\enddisplay
for all $n>0$. Find a similar
formula for the more general product $x^my^n$.
(These formulas give useful "partial fraction" expansions, for example
when $x=1/(z-c)$ and $y=1/(z-d)$.)
\answer $x^my^n=\sum_{k=1}^m{m+n-1-k\choose n-1}a^nb^{m-k}x^k
+\sum_{k=1}^n{m+n-1-k\choose m-1}a^{n-k}b^my^k$
for all $mn>0$, by induction on $m+n$.

\ex:
Find a closed form for
\begindisplay
\sum_{j=1}^m(-1)^{j+1}{r\choose j}\sum_{k=1}^n{-j+rk+s\choose m-j},
 \qquad\hbox{integers $m,n\ge0$}.
\enddisplay
\answer $(-1)^{m+1}\sum_{k=1}^n\sum_{j=1}^m{r\choose j}{m-rk-s-1\choose m-j}=
(-1)^m\sum_{k=1}^n\bigl({m-rk-s-1\choose m}-\break
{m-r(k-1)-s-1\,\choose m}\bigr)^{\mathstrut}
=(-1)^m\bigl({m-rn-s-1\choose m}-{m-s-1\choose m}\bigr)=
{rn+s\choose m}-{s\choose m}$.
\source{"R\o dseth" [|roedseth|].}

\ex:
Evaluate $\sum_k{n\choose k}k!/(n+1+k)!$ when $n$ is a nonnegative integer.
\answer $\sum_{k\ge0}n!/(n-k)!\,(n+k+1)!
 =\bigl(n!/(2n+1)!\bigr)\sum_{k>n}
{2n+1\choose k}$, which is $2^{2n}n!/(2n+1)!$.

\ex:\exref|bc-alt-recip|%
Find the indefinite sum $\sum\bigl((-1)^x\!\big/{n\choose x}\bigr)\,\delta x$,
and use it to compute the sum $\sum_{k=0}^n(-1)^k\!\big/{n\choose k}$
in closed form."!binomial coefficient, reciprocal of"
\answer We treat $n$ as an indeterminate real variable. Gosper's method
with $q(k)=k+1$ and $r(k)=k-1-n$ has the solution $s(k)=1/(n+2)$;
hence the desired indefinite sum is $(-1)^{x-1}{n+1\over n+2}/{n+1\choose x}$.
And
\begindisplay
\sum_{k=0}^n(-1)^k\Big/{n\choose k}=(-1)^{x-1}\,{n+1\over n+2}\bigg/
\!{n{+}1\choose x}\biggr\vert_0^{n+1}\!\!=2\,{n+1\over n+2}\,\[\hbox{$n$ even}]\,.
\enddisplay
This exercise, incidentally, implies the formula
\begindisplay
{1\over\displaystyle n{n-1\choose k}^{\mathstrut}}=
{1\over\displaystyle(n+1){n\choose k+1}^{\mathstrut}}+
{1\over\displaystyle(n+1){n\choose k}^{\mathstrut}}\,,
\enddisplay
"!dual binomial coefficients"
a ``dual'' to the basic recurrence \equ(5.|bc-addition|).

\ex:\exref|prove-saalschutz|%
Prove the triple-binomial identity \eq(|bc-saalschutz|). \Hint:
First replace $r+k\choose m+n$ by $\sum_j{r\choose m+n-j}{k\choose j}$.
\answer After the hinted first step we can apply \equ(5.|bc-tc|) and
sum on~$k$. Then \equ(5.|bc-tc|) applies again and Vandermonde's
convolution finishes the job. (A combinatorial proof of this identity
has been given by "Andrews" [|andrews-saalschutz|]. There's a quick
way to go from this identity to a proof of
\equ(5.|bc-dixon|), explained in [|knuth1|, exercise 1.2.6--62].)
\source{"Pfaff" [|pfaff|]; [|knuth1|,~exercise 1.2.6--31].}

\ex:
Use identity \eq(|bc-quad|) to find closed forms for the double sums
\begindisplay
&\sum_{j,k}(-1)^{j+k}{j+k\choose j}{a\choose j}{b\choose k}{m+n-j-k\choose m-j}
\And\cr
&\sum_{j,k\ge0}(-1)^{j+k}{a\choose j}{m\choose j}{b\choose k}{n\choose k}\bigg/
 {m+n\choose j+k}\,,
\enddisplay
given integers $m\ge a\ge0$ and $n\ge b\ge0$.
\answer Cancellation of factorials shows that
\begindisplay
{m\choose j}{n\choose k}{m+n\choose m}={m+n-j-k\choose m-j}{j+k\choose j}
 {m+n\choose j+k}\,,
\enddisplay
so the second sum is $1/{m+n\choose m}$ times the first.
And the first is just the special case $l=0$, $n=b$, $r=a$, $s=m+n-b$ of
\equ(5.|bc-quad|), so it is ${a+b\choose a}{m+n-a-b\choose n-a}$.

\ex:
Find a closed form for $\sum_{k\le n}{2k\choose k}4^{-k}$.
\answer According to \equ(5.|bc-sum-both|), $\sum_{k\le n}{k-1/2@\choose k}=
{n+1/2@\choose n}$. If this form of the answer isn't ``closed'' enough, we can
apply \equ(5.|half-bc|) and get $(2n+1){2n\choose n}4^{-n}$.

\ex:
Evaluate the following sum in closed form, when $n$ is a positive integer:
\begindisplay
\sum_k{2k-1\choose k}{4n-2k-1\choose 2n-k}{(-1)^{k-1}\over
 (2k-1)(4n-2k-1)}\,.
\enddisplay
\Hint: Generating functions win again.
\answer By \equ(5.|cat+-|), this convolution is the negative of the
coefficient of $z^{2n}$
in $\Bscr_{-1}(z)@\Bscr_{-1}(-z)$. Now $(2\Bscr_{-1}(z)-1)(2\Bscr_{-1}(-z)-1)
=\sqrt{@1-16z^{2\mathstrut}}$;
hence $\Bscr_{-1}(z)@\Bscr_{-1}(-z)={1\over4}\sqrt{@1-16z^{2\mathstrut}}
+\half \Bscr_{-1}(z)+\half \Bscr_{-1}(-z)-{1\over4}$.
By the binomial theorem,
\begindisplay
(1-16z^2)^{1/2}=\sum_n{1/2\choose n}(-16)^nz^{2n}=-\sum_n{2n\choose n}
 {4^nz^{2n}\over 2n-1}\,,
\enddisplay
so the answer is ${2n\choose n}4^{n-1}\!/(2n-1)+{4n-1\choose 2n}/(4n-1)$.

\ex:
The sum 
\begindisplay
\sum_k{rk+s\choose k}{rn-rk-s\choose n-k}
\enddisplay
is a polynomial in
$r$ and~$s$. Show that it doesn't depend on $s$.
\answer It's the coefficient of $z^n$ in $\bigl(\Bscr_r(z)^s\!/Q_r(z)\bigr)
\g\setbox0=\vbox{\halign{\span\grafctr\cr
The boxed\cr sentence\cr on the\cr other side\cr of this page\cr
is true.\cr}}"!self-reference"
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
\bigl(\Bscr_r(z)^{-s}\!/Q_r(z)\bigr)=Q_r(z)^{-2}$, where $Q_r(z)=
1-r+r@\Bscr_r(z)^{-1}$, by \equ(5.|t-series-mod-power|).

\ex:
The identity $\sum_{k\le n}{n+k\choose n}2^{-k}=2^n$ can be combined with
the formula
$\sum_{k\ge0}{n+k\choose n}z^k=1/(1-z)^{n+1}$ to yield
\begindisplay
\sum_{k>n}{n+k\choose n}2^{-k}=2^n\,.
\enddisplay
What is the hypergeometric form of the latter identity?
\answer $F(2n+2,1;n+2;\half)=2^{2n+1}\!/{2n+1\choose n+1}$, a special
case of \equ(5.|hyp-gauss-half|).
\source{Ranjan "Roy".*}

\ex:
Use the hypergeometric method to evaluate
\begindisplay
\sum_k(-1)^k{x\choose k}{x+n-k\choose n-k}{y\over y+n-k}\,.
\enddisplay
\answer Saalsch\"utz's identity \equ(5.|hyp-saalschutz|) yields
\begindisplay
{x+n\choose n}{y\over y+n}\,\hyp{-x,\,-n,\,-n-y}{-x-n,\,1-n-y}1=
{(y-x)\_^n\over(y+1)\_^n}\,.
\enddisplay
\source{"Roy" [|roy|, eq.~3.13].}

\ex:\exref|prove-refl-trans|%
Prove Pfaff's "reflection law" \eq(|hyp-refl|) by comparing the
coefficients of $z^n$ on both sides of the equation.
\answer The left-hand side is
\begindisplay \openup3pt
&\sum_{k\ge0}{a\_^k\,b\_^k\over c\_^k}{(-z)^k\over k!\vphantom{c\_^k}}
 \sum_{m\ge0}{k+a+m-1\choose m}z^m\cr
&\qquad=\sum_{n\ge0}z^n\sum_{k\ge0}
 {a\_^k\,b\_^k\over c\_^k\,k!}(-1)^k{n+a-1\choose n-k}
\enddisplay
and the coefficient of $z^n$ is
\begindisplay
{n+a-1\choose n}\hyp{a,b,-n}{c,a}1{a\_^n\over n!}={(c-b)\_^n\over c\_^n}
\enddisplay
by Vandermonde's convolution \equ(5.|Fabc|).

\ex:\exref|diff-limits|%
The derivation of \eq(|miracle-half/2|) shows that
\begindisplay
\textstyle\lim_{\epsilon\to0}
F(-m,-2m-1+\epsilon;-2m+\epsilon;2)=1\big/{-1/2@\choose m}\,.
\enddisplay
In this exercise
we will see that slightly different limiting processes lead to
distinctly different answers for the "degenerate hypergeometric series"
$F(-m,-2m-1;-2m;2)$.
\itemitem{a}Show that $\lim_{\epsilon\to0}F(-m+\epsilon,-2m-1;-2m+2\epsilon;2)
=0$, by using Pfaff's reflection law
to prove the identity $F(a,-2m-1;2a;2)=0$ for all integers $m\ge0$.
\itemitem{b}What is $\lim_{\epsilon\to0}F(-m+\epsilon,-2m-1;-2m+\epsilon;2)$?
\answer (a) Reflection gives $F(a,-n;2a;2)=(-1)^nF(a,-n;2a;2)$.
(Incidentally, this formula implies the remarkable identity
$\Delta^{2m+1\,}f(0)=0$, when $f(n)=2^nx\_n/(2x)\_n$.)
\par\goodbreak(b) The term-by-term limit is
$\sum_{0\le k\le m}{m\choose k}{2m+1\over 2m+1-k}(-2)^k$
plus an additional term for $k=2m-1$. The additional term is
\begindisplay \openup3pt
&{(-m)\ldots(-1)\,(1)\ldots(m)\,(-2m+1)\ldots(-1)\,2^{2m+1}
 \over(-2m)\ldots(-1)\,(2m-1)!}\cr
&\qquad=(-1)^{m+1}{m!\,m!\,2^{2m+1}\over(2m)!}={-2\over{-1/2@\choose m}}\,;
\enddisplay
hence, by \equ(5.|miracle-half/2|), this limit is $-1\big/{-1/2@\choose m}$,
the negative of what we had.

\ex:\exref|hyp-backwards|%
Prove that if $N$ is a nonnegative integer,
\begindisplay
&b_1\_^N\ldots b_n\_^N\hyp{a_1,\ldots,a_m,-N}{b_1,\ldots,b_n}z\cr
&\quad=a_1\_^N\ldots a_m\_^N(-z)^N\hyp{1{-}b_1{-}N,\ldots,1{-}b_n{-}N,-N}
{1{-}a_1{-}N,\ldots,1{-}a_m{-}N}{{(-1)^{m+n}\over z}}\,.
\enddisplay
\answer The terms of both series are zero for $k>N$. This identity
corresponds to replacing $k$ by $N-k$. Notice that
\begindisplay \openup2pt
a\_^N&=a\_^{N-k}\,(a+N-k)\_^k\cr
&=a\_^{N-k}\,(a+N-1)\_k=a\_^{N-k}\,(1-a-N)\_^k(-1)^k\,.
\enddisplay

\ex:\exref|convergence-caution|%
If we put $b=-\half$ and $z=1$ in Gauss's identity \eq(|hyp4z-4z2|),
the left side reduces to~$-1$ while the right side is $+1$. Why doesn't
this prove that $-1=+1$?
\answer When $b=-\half$, the left side of \equ(5.|hyp4z-4z2|) is $1-2z$
\g\setbox0=\vbox{\halign{\span\grafctr\cr
The boxed\cr sentence\cr on the\cr other side\cr of this page\cr
is false.\cr}}
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
and the right side is $(1-4z+4z^2)^{1/2}$, independent of~$a$. The right side
is the "formal power series" "!convergence"
\begindisplay
1+{1/2\choose1}4z(z-1)+{1/2\choose2}16z^2(z-1)^2+\cdots\,,
\enddisplay
which can be expanded and rearranged to give $1-2z+0z^2+0z^3+\cdots\,$;
but the rearrangement involves "divergent series" in its intermediate steps
when $z=1$, so it is not legitimate.
\source{"Gauss" [|gauss-hyp|]; Richard "Askey".*}

\ex:\exref|explain-gauss-miracle|%
Explain how the right-hand side of \eq(|gauss-miracle|) was obtained.
\answer If $m+n$ is odd, say $2N-1$, we want to show that
\begindisplay
\lim_{\epsilon\to0}\hyp{N-m-\half,\,-N+\epsilon}{-m+\epsilon}1=0\,.
\enddisplay
Equation \equ(5.|Fabc|) applies, since $-m+\epsilon>-m-\half+\epsilon$,
and the denominator factor $\Gamma(c-b)=\Gamma(N-m)$ is infinite since
$N\le m$; the other factors are finite. Otherwise $m+n$ is even;
setting $n=m-2N$ we have
\begindisplay
\lim_{\epsilon\to0}\hyp{-N,\,N-m-\half+\epsilon}{-m+\epsilon}1=
{(N-1/2)\_N\over m@\_N\hypstrut}
\enddisplay
by \equ(5.|Fa-nc|). The remaining job is to show that
\begindisplay
{m\choose m-2N}{(N-1/2)!\over(-1/2)!}{(m-N)!\over m!}={m-N\choose m-2N}2^{-2N}\,,
\enddisplay
and this is the case $x=N$ of exercise |factorial-dup|.

\ex:\exref|explain-gosper-mystery|%
If the hypergeometric terms
$t(k)=F(a_1,\ldots,a_m;\,b_1,\ldots,b_n;\,z)_k$ and\break
$T(k)=F(A_1,\ldots,A_M;B_1,\ldots,B_N;Z)_k$ satisfy
$t(k)=c\bigl(T(k+1)-T(k)\bigr)$ for all~$k\ge0$, show that $z=Z$ and
$m-n=M-N$.
\answer Let $Q(k)=(k+A_1)\ldots(k+A_M)Z$ and $R(k)=(k+B_1)\ldots(k+B_N)$.
Then $t(k+1)/t(k)=P(k)@Q(k-1)/P(k-1)@R(k)$,
 where $P(k)=Q(k)-R(k)$ is a nonzero polynomial.

\ex:
Find a general formula for $\sum{-3\choose k}\,\delta k$ using Gosper's method.
Show that $(-1)^{k-1}\bigl\lfloor{k+1\over2}\bigr\rfloor
\bigl\lfloor{k+2\over2}\bigr\rfloor$ is also a solution.
\answer The solution to $-(k+1)(k+2)=s(k+1)+s(k)$ is $s(k)=-\half k^2-k-{1\over4}$;
hence $\sum{-3\choose k}\,\delta k={1\over8}(-1)^{k-1}(2k^2+4k+1)+C$.
Also
\begindisplay \openup3pt
&(-1)^{k-1}\left\lfloor k+1\over2\right\rfloor
\left\lfloor k+2\over2\right\rfloor\cr
&\qquad={(-1)^{k-1}\over4}\left(k+1
 -{1{+}(-1)^k\over2}\right)\left(k+2-{1{-}(-1)^k\over2}\right)\cr
&\qquad={(-1)^{k-1}\over8}(2k^2+4k+1)+{1\over8}\,.
\enddisplay

\ex:
Use Gosper's method to find a constant $\theta$ such that
\begindisplay
\sum{n\choose k}z^k(k+\theta)\,\delta k
\enddisplay
is summable in hypergeometric terms.
\answer We have $t(k+1)/t(k)=(k-n)(k+1+\theta)(-z)/(k+1)(k+\theta)$.
Therefore we let $p(k)=k+\theta$, $q(k)=(k-n)(-z)$, $r(k)=k$.
The secret function $s(k)$ must be a constant $\alpha_0$, and we
have
\begindisplay
k+\theta=\bigl(-z(k-n)-k)\,\alpha_0\,;
\enddisplay
hence $\alpha_0=-1/(1+z)$ and $\theta=-nz/(1+z)$. The sum is
\begindisplay
\sum{n\choose k}z^k\biggl(k-{nz\over1+z}\biggr)\,\delta k=
-\,{n\over1+z}{n-1\choose k-1}z^k+C\,.
\enddisplay
(The special case $z=1$ was mentioned in \equ(5.|bc-partial-k|).)

\ex:\exref|samplesort-recurrence|%
If $m$ and $n$ are integers with $0\le m\le n$, let
\begindisplay
T_{m,n}=\sum_{0\le k<n}{k\choose m}{1\over n-k}\,.
\enddisplay
Find a relation between $T_{m,n}$ and $T_{m-1,n-1}$, then solve your
recurrence by applying a summation factor.
\answer If $m>0$ we can replace $k\choose m$ by ${k\over m}{k-1\choose m-1}$
and derive the formula $T_{m,n}={n\over m}T_{m-1,n-1}-{1\over m}{n-1\choose m}$.
The summation factor ${n\choose m}{}^{\!-1}$ is therefore appropriate:
\begindisplay
{T_{m,n}\over{n\choose m}}=
{T_{m-1,n-1}\over{n-1\choose m-1}}-{1\over m}+{1\over n}\,.
\enddisplay
We can unfold this to get
\begindisplay
{T_{m,n}\over{n\choose m}}=
T_{0,n-m}-H_m+H_n-H_{n-m}\,.
\enddisplay
Finally $T_{0,n-m}=H_{n-m}$, so $T_{m,n}={n\choose m}(H_n-H_m)$.
(It's also possible to derive this result by using generating functions;
see Example~2 in Section 7.5.)
\source{"Frazer" and "McKellar" [|samplesort|].}

\subhead Exam problems

\ex:
Find a closed form for
\begindisplay
\sum_{k\ge1}{n\choose \lfloor\log_m k\rfloor}
\enddisplay
when $m$ and $n$ are positive integers.
\answer $\sum_{j\ge0,\,k\ge1}{n\choose j}\bigi[j=\lfloor\log_m k\rfloor\bigr]
=\sum_{j\ge0,\,k\ge1}{n\choose j}\[m^j\le k<m^{j+1}]$, which is
$\sum_{j\ge0}{n\choose j}\*(m^{j+1}
-m^j)=(m-1)\sum_{j\ge0}{n\choose j}m^j=(m-1)(m+1)^n$.
\source{Stanford Computer Science Compre\-hensive Exam, Winter 1987.}

\ex:
Use Stirling's approximation \equ(4.|stirling-approx|)
to estimate $m+n\choose n$ when $m$ and $n$ are both large.
What does your formula reduce to when $m=n$?
\answer ${2n\choose n}\approx 4^n\!/\!\sqrt{\pi n}$ is the case $m=n$ of
\begindisplay
{m+n\choose n}\approx\sqrt{{1\over2\pi}\Bigl({1\over m}+{1\over n}\Bigr)}
 \Bigl(1+{m\over n}\Bigr)^{\!n}
 \Bigl(1+{n\over m}\Bigr)^{\!m}\,.
\enddisplay
\source{[|knuth1|, exercise 1.2.6--41].}

\ex:\exref|lucas-bc|%
Prove that when $p$ is prime, we have
\begindisplay
{n\choose m}\={\lfloor n/p\rfloor\choose\lfloor m/p\rfloor}
 {n\bmod p\choose m\bmod p}\ \pmod p\,,
\enddisplay
for all nonnegative integers $m$ and $n$.
\answer Let $\lfloor n/p\rfloor=q$ and $n\bmod p=r$. The polynomial
identity $(x+1)^p\=x^p+1$ \tmod p implies that
\begindisplay
(x+1)^{pq+r}\=(x+1)^r(x^p+1)^q\ \pmod p\,.
\enddisplay
The coefficient of $x^m$ on the left is ${n\choose m}$. On the right
it's $\sum_k{r\choose m-pk}{q\choose k}$, which is just ${r\choose m\,\bmod\,p}
{q\choose\lfloor m/p\rfloor}$ because $0\le r<p$.
\source{"Lucas" [|lucas-bc-mod|].}

\ex:
Assuming that $p$ is prime and that $m$ and $n$ are positive integers,
determine the value of ${np\choose mp}\bmod p^2$. \Hint: You may wish to
use the following generalization of "Vandermonde's convolution":
\begindisplay
\sum_{k_1+k_2+\cdots+k_m=n}{r_1\choose k_1}{r_2\choose k_2}\ldots
 {r_m\choose k_m}={r_1+r_2+\cdots+r_m\choose n}\,.
\enddisplay
\answer ${np\choose mp}=\sum_{k_1+\cdots+k_n=mp}{p\choose k_1}\ldots
{p\choose k_n}\={n\choose m}$ \tmod{p^2}, because
all terms of the sum are multiples of $p^2$ except for the $n\choose m$ terms
in which exactly $m$ of the $k$'s are equal to~$p$.
("Stanley" [|stanley|, exercise 1.6(d)] shows that the congruence actually holds
modulo~$p^3$ when $p>3$.)
\source{1971 midterm.}

\ex:
Find a closed form for
\begindisplay
\sum_{k=0}^n(-4)^k{n+k\choose 2k}\,,
\enddisplay
given an integer $n\ge0$.\par\goodbreak
\answer This is $S_n=\sum_{k=0}^n(-4)^k{n+k\choose n-k}=
\sum_{k=0}^n(-4)^{n-k}{2n-k\choose k}$. The denominator of
\equ(5.|bc-gen-fib|) is zero when $z=-1/4$, so we can't simply
plug into that formula. The recurrence $S_n=-2S_{n-1}-S_{n-2}$ leads
to the solution $S_n=(-1)^n(2n+1)$.
\source{1974 midterm.}

\ex:
Evaluate \displaymath
 \sum_{k=0}^n{n\choose k}\bigg/\biggl\lceil{k+1\over2}\biggr\rceil\,$,
given an integer $n\ge0$.
\answer $\sum_{k\ge0}\bigl({n\choose2k}+{n\choose2k+1}\bigr)\big/(k+1)
=\sum_{k\ge0}{n+1\choose2k+1}/(k+1)$, which is
\begindisplay
{2\over n+2}\sum_{k\ge0}{n+2\choose 2k+2}={2^{n+2}-2\over n+2}\,.
\enddisplay
\source{1980 midterm.}

\ex:
Prove that
\begindisplay
\sum_k{n-1\choose k}n^{-k}(k+1)!=n\,.
\enddisplay
\answer Multiply both sides by $n^{n-1}$ and replace $k$ by $n-1-k$
to get
\begindisplay \openup3pt
\sum_k{n-1\choose k}n^k(n-k)!
&=(n-1)!\sum_{k=0}^{n-1}\bigl(n^{k+1}\!/k!-n^k\!/(k-1)!\bigr)\cr
&=(n-1)!\,n^n\!/(n-1)!\,.
\enddisplay
(The partial
sums can, in fact, be found by "Gosper's algorithm".) Alternatively,
${n\choose k}kn^{n-1-k}k!$ can be interpreted as the number of mappings
of $\{1,\ldots,n\}$ into itself with $f(1)$, \dots,~$f(k)$ distinct
but $f(k+1)\in\{f(1),\ldots,f(k)\}$; summing on~$k$ must give~$n^n$.
\source{1983 midterm.}

\ex:
Evaluate ``"Harry's double sum",\qback''
"!Harry, Matthew Arnold" % Matt A. Harry
\begindisplay
\sum_{0\le j\le k}{-1\choose @j-\lfloor\sqrt{\mathstrut@ k-j}\rfloor}
{j\choose m}{1\over 2^j}\,,\qquad\hbox{integer $m\ge0$},
\enddisplay
as a function of $m$. (The sum is over both $j$ and $k$.)
\answer This is a walk-the-garden-path problem where there's only
one ``obvious'' way to proceed at every step. First replace $k-j$ by $l$,
then replace $\lfloor\sqrt l\,\rfloor$~by $k$, getting
\begindisplay
\sum_{j,k\ge0}{-1\choose j-k}{j\choose m}{2k+1\over 2^j}\,.
\enddisplay
The infinite series converges because the terms for fixed~$j$ are
dominated by a polynomial in~$j$ divided by~$2^j$. Now sum over~$k$, getting
\begindisplay
\sum_{j\ge0}{j\choose m}{j+1\over2^j}\,.
\enddisplay
Absorb the $j+1$ and apply \equ(5.|neg-binomial2|) to get the answer, $4(m+1)$.
\source{1984 midterm.}

\ex:
Find a closed form for
\begindisplay
\sum_{k=0}^n{{k\choose2}\choose2}{2n-k\choose n}\,,\qquad\hbox{integer $n\ge0$}.
\enddisplay
\answer $3{2n+2@\choose n+5}$ by \equ(5.|bc-prod5|), because
\begindisplay
{{k\choose2}\choose2}=3{k+1\choose4}\,.
\enddisplay
\source{1976 midterm.}

\ex:
Find a closed form for
\begindisplay
\sum_k{n\choose k}\min(k,n-k)\,,\qquad\hbox{integer $n\ge0$.}
\enddisplay
\answer Using the fact that
\begindisplay
\sum_{k\le n/2}{n\choose k}=2^{n-1}+
{1\over2}{n\choose n/2}\[\hbox{$n$ is even}]\,,
\enddisplay
 we get
$n\bigl(2^{n-1}-{n-1\choose\lfloor n/2\rfloor}\bigr)$.
\source{1985 midterm.}

\ex:
Find a closed form for
\begindisplay
\min\twoconditions{k_1,\ldots,k_m^{\mathstrut}\ge0}{k_1+\cdots+k_m=n}
\,\sum_{j=1}^m{k_j\choose2}
\enddisplay
as a function of $m$ and $n$.
\answer Since ${k+1\choose2}+{l-1\choose2}\le{k\choose2}+{l\choose2}\iff
\g\setbox0=\vbox{\halign{\span\grafctr\cr
The boxed\cr sentence\cr on the\cr other side\cr of this page\cr
is not a\cr sentence.\cr}}
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
k<l$, the minimum occurs when the $k$'s are as equal as possible. Hence,
by the equipartition formula of Chapter~3, the minimum is
\begindisplay \openup3pt
&(n\bmod m){\lceil n/m\rceil\choose2}+\bigl(n-(n\bmod m)\bigr)
 {\lfloor n/m\rfloor\choose2}\cr
&\qquad=n{\lfloor n/m\rfloor\choose2}
 +(n\bmod m)\left\lfloor n\over m\right\rfloor\,.
\enddisplay
A similar result holds for any lower index in place of $2$.
\source{Lyle "Ramshaw", guest lecture in 1986.}

\ex:
Find a closed form for
\begindisplay
\sum_k{n\choose k}{2k\choose k}\left(-1\over2\right)^{\!k}\,,\qquad
\hbox{integer $n\ge0$.}
\enddisplay
\answer This is $F(-n,\half;1;2)$; but it's also $(-2)^{-n}{2n\choose n}
F(-n,-n;\half-n;\half)$ if we replace $k$ by $n-k$. Now
$F(-n,-n;\half-n;\half)=F(-{n\over2},-{n\over2};\half-n;1)$ by
Gauss's identity \equ(5.|hyp-gauss-half|). (Alternatively,
$F(-n,-n;\half-n;\half)=2^{-n}F(-n,\half;\half-n;-1)$ by the
reflection law \equ(5.|hyp-refl|), and Kummer's formula \equ(5.|hyp-kummer|)
relates this to \equ(5.|alt-convolution|).) The answer is $0$~when $n$ is
odd, $2^{-n}{n\choose n/2}$
when $n$~is even. (See [|greene-knuth|, \S1.2] for another derivation. This
sum arises in the study of a simple search algorithm [|jonassen-knuth|].)
\source{"Andrews" [|andrews-siam|, theorem 5.4].}

\ex:
Let
\begindisplay
S_n=\sum_{k\ge0}{n+k\choose m+2k}\,a_k\,,
\enddisplay
 where $m$ and~$n$ are
nonnegative integers, and let $A(z)=\sum_{k\ge0}a_kz^k$ be the generating
function for the sequence $\<a_0,a_1,a_2,\ldots\,\>$.
\par\nobreak\smallskip
\itemitem{a}Express the generating function $S(z)=\sum_{n\ge0}S_nz^n$
in terms of~$A(z)$.
\itemitem{b}Use this technique to solve Problem~7 in Section 5.2.
\answer (a) Observe that
\begindisplay
S(z)=\sum_{k\ge0}a_k{z^{m+k}\over(1-z)^{m+2k+1}}
={z^m\over(1-z)^{m+1}}A\bigl(z/(1-z)^2\bigr)\,.
\enddisplay
(b)~Here $A(z)=\sum_{k\ge0}{2k\choose k}(-z)^k\!/(k+1)
=\bigl(\sqrt{1^{\mathstrut}+4z}-1\bigr)/2z$,
 so we have $A\bigl(z/(1-z)^2\bigr)=1-z$.
Thus $S_n=[z^n]\,\bigl(z/(1-z)\bigr){}^m={n-1\choose n-m}$.
\source{"Wilf" [|wilfology|, exercise 4.16].}

\ex:\exref|frac-bc|%
Prove that, if $m$, $n$, and $k$ are integers and $n>0$,
"!sideways addition"
\begindisplay
{m/n\choose k}n^{2k-\nu(k)}\quad\hbox{is an integer},
\enddisplay
where $\nu(k)$ is the number of $1$'s in the binary representation of~$k$.
\answer The stated quantity is $m(m-n)\ldots\bigl(m-(k-1)n\bigr)
n^{k-\nu(k)}\!/k!$. Any prime divisor $p$ of $n$ divides the numerator at least
$k-\nu(k)$ times and divides the denominator at most $k-\nu(k)$ times,
since this is the number of times $2$~divides~$k!$.
A prime~$p$ that does not divide~$n$ must divide the product
 $m(m-n)\ldots\bigl(m-(k-1)n\bigr)$
at least as often as it divides $k!$, because
 ${m(m-n)\ldots\bigl(m-(p^r-1)n\bigr)}$
is a multiple of~$p^r$ for all $r\ge1$ and all~$m$.
\source{"Hermite" [|hermite-cours|].}

\ex:
Use the "repertoire method" to solve the recurrence
\begindisplay
X_0&=\alpha\,; \qquad X_1=\beta\,;\cr
X_n&=(n-1)(X_{n-1}+X_{n-2})\,,\qquad\hbox{for $n>1$}.
\enddisplay
\Hint: Both $n!$ and $n\?$ satisfy this recurrence.
\answer Plugging in $X_n=n!$ yields $\alpha=\beta=1$;
plugging in $X_n=n\?$ yields $\alpha=1$, $\beta=0$. Therefore the
general solution is $X_n=\alpha n\?+\beta(n!-n\?)$.

\ex:
This problem concerns a deviant version of Pascal's triangle in which the
sides consist of the numbers $1$, $2$, $3$, $4$, \dots\ instead of all $1$'s,
although the interior numbers still satisfy the addition formula:
\begindisplay \openup-2pt \def\preamble{&$\,\hfil##\hfil\,$}%
\def\\{\raise.3ex\hbox{\bf.}}
&&&&&1\cr
&&&&2&&2\cr
&&&3&&4&&3\cr
&&4&&7&&7&&4\cr
&5&&11&&14&&11&&5\cr
\\&&\\&&\\&&\\&&\\&&\\\cr
\enddisplay
If $\double(n\choose k)$
denotes the $k$th number in row~$n$, for $1\le k\le n$, we
have $\double(n\choose1)=\double(n\choose n)=n$, and
$\double(n\choose k)=\double(n-1\choose k)+\double(n-1\choose k-1)$
for $1<k<n$. Express the quantity $\double(n\choose k)$ in closed form.
\answer ${n+1\choose k}-{n-1\choose k-1}$, for $1\le k\le n$.
\source{1979 midterm.}

\ex:\exref|wraparound3|%
Find a relation between the functions
"!wraparound binomial coefficients"
\begindisplay \openup4pt
S_0(n)&=\sum_k{n\choose3k}\,,\cr
S_1(n)&=\sum_k{n\choose 3k+1}\,,\cr
S_2(n)&=\sum_k{n\choose3k+2}
\enddisplay
and the quantities $\lfloor2^n\!/3\rfloor$
and $\lceil2^n\!/3\rceil$.
\answer The recurrence $S_k(n+1)=S_k(n)+S_{(k-1)\,\bmod\,3}(n)$ makes it
possible to verify inductively that two of the $S$'s are equal and that
$S_{(-n)\,\bmod\,3}(n)$ differs from them by~$(-1)^n$.
These three values split their sum $S_0(n)+S_1(n)+S_2(n)=2^n$ as equally
as possible, so there must be $2^n\bmod3$ occurrences of $\lceil2^n\!/3\rceil$
and $3-(2^n\bmod3)$ occurrences of $\lfloor2^n\!/3\rfloor$.
\source{1971 midterm.}

\ex:
Solve the following recurrence for $n,k\ge0$:
\begindisplay \openup4pt
Q_{n,0}&=1\,;\qquad Q_{0,k}=\[k=0]\,;\cr
Q_{n,k}&=Q_{n-1,k}+Q_{n-1,k-1}+{n\choose k}\,,
 \qquad\hbox{for $n,k>0$}.
\enddisplay
\answer $Q_{n,k}=(n+1){n\choose k}-{n\choose k+1}$.
\source{[|knuth1|, exercise 1.2.6--59 (corrected)].}

\ex:
What is the value of
\begindisplay
\sum_{0\le k_1,\ldots,k_m\le n}\,\,\prod_{1\le j<m}{k_{j+1}\choose k_j}\,,
\qquad\hbox{if $m>1$?}
\enddisplay
\answer The terms are zero unless $k_1\le \cdots\le k_m$, when the product
\g\setbox0=\vbox{\halign{\span\grafctr\cr
The boxed\cr sentence\cr on the\cr other side\cr of this page\cr
is not boxed.\cr}}
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
is the multinomial coefficient
\begindisplay
{k_m\choose k_1,\,k_2-k_1,\,\ldots,\,k_m-k_{m-1}}\,.
\enddisplay
Therefore the sum over $k_1$, \dots, $k_{m-1}$ is $m^{k_m}$, and the
final sum over $k_m$ yields
$(m^{n+1}-1)/(m-1)$.
\source{1986 midterm.}

\ex:
Assuming that $m$ is a positive integer, find a closed form for
\begindisplay
\sum_{k=0}^{2m^2}{k\bmod m\choose(2k+1)\bmod(2m+1)}\,.
\enddisplay
\answer Extend the sum to $k=2m^2+m-1$; the new terms are
${1\choose4}+{\2\choose6}+\cdots+{m-1\choose2m}=0$.
Since $m\rp(2m+1)$, the pairs $\bigl(k\bmod m,k\bmod(2m+1)\bigr)$ are
distinct. Furthermore, the numbers $(2j+1)\bmod(2m+1)$
as $j$ varies from $0$ to $2m$ are the numbers
$0$,~$1$, \dots,~$2m$ in some order. Hence the sum is
\begindisplay
\sum\twoconditions{0\le k<m}{0\le j<2m+1}{k\choose\,j\,}
=\sum_{0\le k<m}2^k=2^m-1\,.
\enddisplay
\source{[|some-sum|].}

\ex:
\hangindent2\parindent \textindent{a}% \item combined with \itemitem
What is the greatest common divisor of ${2n\choose 1}$, $2n\choose 3$,
\dots,~$2n\choose 2n-1$? \Hint: Consider the sum of these $n$ numbers.
\itemitem{b}%
Show that the least common multiple of $n\choose0$, $n\choose1$,
\dots,~$n\choose n$ is equal to $L(n+1)/(n+1)$, where
$L(n)=\lcm(1,2,\ldots,n)$.
\answer (a) The sum is $2^{2n-1}$, so the gcd must be a power of~$2$.
If $n=2^kq$ where $q$~is odd, $2n\choose1$ is divisible by $2^{k+1}$
and not by~$2^{k+2}$. Each $2n\choose 2j+1$ is divisible by $2^{k+1}$
(see exercise~|bc-div-p|), so this must be the gcd.
(b)~If $p^r\le n+1<p^{r+1}$, we get the most radix~$p$ carries by adding
$k$ to $n-k$ when $k=p^r-1$. The number of carries in this case
is $r-\epsilon_p(n+1)$, and $r=\epsilon_p\bigl(L(n+1)\bigr)$.
\source{"Mendelsohn" [|mendelsohn|]; "Montgomery" [|montgomery|].}

\ex:
Prove that ${n\choose k}\le(en/k)^k$ for all integers $k,n\ge0$.
\g Handy to know.\g
\answer First prove by induction that $k!\ge(k/e)^k$.

\ex:\exref|bc-inequality|%
If $0<\theta<1$ and $0\le x\le1$, and if $l,m,n$ are nonnegative integers
with $m<n$, prove the inequality
\begindisplay
(-1)^{n-m-1}\sum_k{l\choose k}{m+\theta\choose n+k}x^k>0\,.
\enddisplay
\Hint: Consider taking the derivative with respect to~$x$.
\answer Let $f_{l,m,n}(x)$ be the left-hand side. It is sufficient
to show that we have
 $f_{l,m,n}(1)>0$ and that $f^{\,\prime}_{l,m,n}(x)<0$ for $0\le x\le1$.
The value of $f_{l,m,n}(1)$ is $(-1)^{n-m-1}{l+m+\theta\choose l+n}$
by \equ(5.|bc-prod2|), and this is positive because the binomial coefficient
has exactly $n-m-1$ negative factors. The inequality is true when $l=0$, for
the same reason. If $l>0$, we have $f^{\,\prime}_{l,m,n}(x)
=-l\,f_{l-1,m,n+1}(x)$, which is negative by induction.
\source{1986 final exam; [|knuth-bci|].}

\subhead Bonus problems

\ex:
Prove that Pascal's triangle has an even more surprising "hexagon property"
than the one cited in the text:
\begindisplay
\textstyle\gcd\Bigl({n-1\choose k-1},{n\choose k+1},{n+1\choose k}\Bigr)
=\gcd\Bigl({n-1\choose k},{n+1\choose k+1},{n\choose k-1}\Bigr)\,,
\enddisplay
if $\,0<k<n$. For example, $\gcd(56,36,210)=\gcd(28,120,126)=2$.
\answer Let $\epsilon_p(a)$ be the exponent by which the prime~$p$ divides~$a$,
and let $m=n-k$. The identity to be proved reduces to
\begindisplay
&\min\bigl(\epsilon_p(m){-}\epsilon_p(m{+}k),
 \epsilon_p(m{+}k{+}1){-}\epsilon_p(k{+}1),
 \epsilon_p(k){-}\epsilon_p(m{+}1)\bigr)\cr
&\ =
\min\bigl(\epsilon_p(k){-}\epsilon_p(m{+}k),
 \epsilon_p(m){-}\epsilon_p(k{+}1),
 \epsilon_p(m{+}k{+}1){-}\epsilon_p(m{+}1)\bigr)\,.
\enddisplay
For brevity let's write this as $\min(x_1,y_1,z_1)=\min(x_2,y_2,z_2)$.
Notice that $x_1+y_1+z_1=x_2+y_2+z_2$. The general relation
\begindisplay
\epsilon_p(a)<\epsilon_p(b)\quad\implies\quad
 \epsilon_p(a)=\epsilon_p\bigl(\vert a\pm b\vert\bigr)
\enddisplay
allows us to conclude that $x_1\ne x_2\implies\min(x_1,x_2)=0$; the
same holds also for $(y_1,y_2)$ and $(z_1,z_2)$. It's now a simple matter
to complete the proof.
\source{"Hillman" and "Hoggatt" [|hillman-hoggatt|].}

\ex:\exref|prove-bc-quad|%
Prove the amazing five-parameter double-sum identity \eq(|bc-quad|).
\answer (Solution by P. "Paule".) Let $r$ be a nonnegative integer.
The given sum is the coefficient of $x^ly^m$ in
\begindisplay
&\sum_{j,k}(-1)^{j+k}{(1+x)^{j+k}\over x^k}{r\choose j}{n\choose k}
(1+y)^{s+n-j-k}y^j\cr
&\qquad=\biggl(1-{(1+x)y\over 1+y}\biggr)^{\!r}
        \biggl(1-{1+x\over(1+y)x}\biggr)^{\!n}
        (1+y)^{s+n}\cr
\noalign{\vskip2pt}
&\qquad=(-1)^n(1-xy)^{n+r}(1+y)^{s-r}/x^n\,,\cr
\enddisplay
so it is clearly $(-1)^l{n+r\choose n+l}{s-r\choose m-n-l}$.
(See also exercise |reprove-bc-quad|.)

\ex:
Show that the second pair of convolution formulas, \eq(|t-series-mod-power|),
 follows from the first pair, \eq(|t-series-power|).
\Hint: Differentiate with respect to~$z$.
\answer Following the hint, we get
\begindisplay
z\Bscr_t(z)^{r-1}\Bscr_t'(z)=\sum_{k\ge0}{tk+r\choose k}{kz^k\over tk+r}\,,
\enddisplay
and a similar formula for $\Escr_t(z)$. Thus the formulas
$\bigl(zt\Bscr_t^{-1}(z)@\Bscr_t'(z)+1\bigr)@\Bscr_t(z)^r$ and
$\bigl(zt\Escr_t^{-1}(z)@\Escr_t'(z)+1\bigr)@\Escr_t(z)^r$ give the respective
right-hand sides of \equ(5.|t-series-mod-power|). We must therefore prove that
\begindisplay \openup2pt \advance\abovedisplayskip-2pt%
\advance\belowdisplayskip-2pt
\bigl(zt\Bscr_t^{-1}(z)@\Bscr_t'(z)+1\bigr)@\Bscr_t(z)^r
 &={1\over1-t+t\Bscr_t(z)^{-1}}\,,\cr
\bigl(zt\Escr_t^{-1}(z)@\Escr_t'(z)+1\bigr)@\Escr_t(z)^r
 &={1\over1-zt\Escr(z)^t}\,,\cr
\enddisplay
and these follow from \equ(5.|t-series-rec|).

\ex:
Prove that
\begindisplay
&\sum_{m=1}^n(-1)^m\!\!\sum_{1\le k_1<k_2<\cdots<k_m\le n}
{k_1^3+k_2^3+\cdots+k_m^3+2^n\choose n}\cr
&\hskip15em=(-1)^n n!^3-{2^n\choose n}\,.
\enddisplay
(The left side is a sum of $2^n-1$ terms.) \Hint: Much more is true.
\answer If $f(x)=a_nx^n+\cdots+a_1x+a_0$ is any polynomial of degree $\le n$,
we can prove inductively that
\begindisplay\tightplus
\sum_{0\le\epsilon_1,\ldots,\epsilon_n\le1}\!\!(-1)^{\epsilon_1+\cdots+\epsilon_n}
f(\epsilon_1x_1+\cdots+\epsilon_nx_n)=(-1)^nn!\,a_nx_1\ldots x_n\,.
\enddisplay
The stated identity is the special case where $a_n=1/n!$ and $x_k=k^3$.
\source{"Hsu" [|hsu-identity|].}

\ex:\exref|dyson|%
Let $a_1$, \dots, $a_n$ be nonnegative integers, and
 let $C(a_1,\ldots,a_n)$ be the coefficient of the
 constant term $z_1^0\ldots z_n^0$ when the $n(n-1)$ factors
\begindisplay
\prod\twoconditions{1\le i,j\le n}{i\ne j}\left(1-{z_i\over z_j}\right)^{a_i}
\enddisplay
are fully expanded into positive and negative powers of the complex variables
$z_1$, \dots, $z_n$.
\itemitem{a} Prove that $C(a_1,\ldots,a_n)$ equals the left-hand side of
\equ(5.|bc-dyson|).
\itemitem{b} Prove that if $z_1$, \dots, $z_n$ are distinct complex numbers,
then the polynomial
\begindisplay
f(z)=\sum_{k=1}^n\prod\twoconditions{1\le j\le n}{j\ne k}{z-z_j\over z_k-z_j}
\enddisplay
is identically equal to $1$.
\itemitem{c} Multiply the original product of $n(n-1)$ factors by $f(0)$ and
deduce that $C(a_1,a_2,\ldots,a_n)$ is equal to
\begindisplay
&C(a_1-1,a_2,\ldots,a_n)+C(a_1,a_2-1,\ldots,a_n)\cr
&\qquad+\cdots +C(a_1,a_2,\ldots,a_n-1)\,.
\enddisplay
(This recurrence defines multinomial coefficients, so $C(a_1,\ldots,a_n)$
must equal the right-hand side of \equ(5.|bc-dyson|).)
\answer (a) First expand with $n(n-1)$ index variables $l_{ij}$ for all
$i\ne j$. Setting $k_{ij}=l_{ij}-l_{ji}$ for $1\le i<j<n$ and using the
constraints $\sum_{i\ne j}(l_{ij}-l_{ji})=0$ for all $i<n$ allows us
to carry out the sums on $l_{jn}$ for $1\le j<n$ and then on $l_{ji}$ for
$1\le i<j<n$ by Vandermonde's convolution.
(b)~$f(z)-1$ is a polynomial of degree $<n$ that has $n$ roots, so it
must be zero.
(c)~Consider the constant terms in
\begindisplay
\prod\twoconditions{1\le i,j\le n}{i\ne j}\left(1-{z_i\over z_j}\right)^{a_i}
=\sum_{k=1}^n
 \prod\twoconditions{1\le i,j\le n}{i\ne j}\left(1-{z_i\over z_j}\right)^
  {a_i-[i=k]}.
\enddisplay
\source{"Good" [|good|].}

\ex:
Let $m$ be a positive integer and let $\zeta=e^{\pi i/m}$. Show that
\begindisplay
&\sum_{k\le n/m}{n-mk\choose k}z^{mk}\cr
&\qquad={\Bscr_{-m}(z^m)^{n+1}\over(1+m)\Bscr_{-m}(z^m)-m}\cr
&\qquad\qquad-\!\sum_{0\le j<m}\!
 {\bigl(\zeta^{2j+1}z\,\Bscr_{1+1/m}(\zeta^{2j+1}z)^{1/m}\bigr)^{n+1}
 \over(m+1)\Bscr_{1+1/m}(\zeta^{2j+1}z)^{-1}-1}\,.
\enddisplay
"!generalized binomial series"
(This reduces to \eq(|bc-gen-fib|) in the special case $m=1$.)
\answer The first term is $\sum_k{n-k\choose k}z^{mk}$, by
\equ(5.|t-series-mod-power|). The summands in the second term are
\begindisplay
&{1\over m}\sum_{k\ge0}{\,(n+1)/m+(1{+}1/m)k\,\choose k}(\zeta z)^{k+n+1}\cr
&\qquad={1\over m}\sum_{k>n}{(1{+}1/m)k-n-1\choose k-n-1}(\zeta z)^k\,.
\enddisplay
Since $\sum_{0\le j<m}(\zeta^{2j+1})^k=m(-1)^l\[k=ml]$, these terms
sum to
\begindisplay
&\sum_{k>n/m}{(1{+}1/m)mk-n-1\choose mk-n-1}(-z^m)^k\cr
&\qquad=\sum_{k>n/m}{(m{+}1)k-n-1\choose k}(-z^m)^k
       =\!\!\sum_{k>n/m}{n-mk\choose k}z^{mk}\,.
\enddisplay
Incidentally, the functions $\Bscr_m(z^m)$ and
$\zeta^{2j+1}z\,\Bscr_{1+1/m}(\zeta^{2j+1} z)^{1/m}$ are the $m+1$ complex
roots of the equation $w^{m+1}-w^m=z^m$.

\ex:\exref|stirling-gamma|%
Prove that the coefficients $s_k$ in \eq(|stirling-try2|) are equal to
\begindisplay
(-1)^k\int_0^\infty e^{-t}(1-e^{-t})^{k-1}\,{dt\over t}\,,
\enddisplay
for all $k>1$; hence $\vert s_k\vert<1/(k-1)$.
\answer Use the facts that $\int_0^\infty(e^{-t}-e^{-nt})\,dt/t=\ln n$
and $(1-e^{-t})/t\le1$. (We have ${x\choose k}=O(k^{-x-1})$ as
$k\to\infty$, by \equ(5.|f-def-lim|); so this bound implies that
Stirling's series $\sum_k s_k{x\choose k}$ converges
when $x>-1$. "Hermite" [|hermite|] showed that the sum is $\ln\Gamma(1+x)$.)
\source{"Hermite" [|hermite|].}

\ex:
Prove that \eq(|partial-binomial|) has an infinite counterpart,
\begindisplay \tightplus
\sum_{k>m}{m+r\choose k}x^ky^{m-k}=
 \sum_{k>m}{-r\choose k}(-x)^k(x+y)^{m-k}\,,\,\quad\hbox{integer $m$},
\enddisplay
if $\vert x\vert<\vert y\vert$ and $\vert x\vert<\vert x+y\vert$.
Differentiate this identity $n$ times with respect to~$y$ and express
it in terms of hypergeometrics; what relation do you get?
\answer Adding this to \equ(5.|partial-binomial|) gives $y^{-r}(x+y)^{m+r}$
on both sides, by the binomial theorem. Differentiation gives
\begindisplay
&\sum_{k>m}{m+r\choose k}{m-k\choose n}x^ky^{m-k-n}\cr
&\qquad= \sum_{k>m}{-r\choose k}{m-k\choose n}(-x)^k(x+y)^{m-k-n}\,,
\enddisplay
and we can replace $k$ by $k+m+1$ and apply \equ(5.|bc-switch|) to get
\begindisplay
&\sum_{k\ge0}{m+r\choose m+1+k}{-n-1\choose k}(-x)^{m+1+k}y^{-1-k-n}\cr
&\qquad=\sum_{k\ge0}{-r\choose m+1+k}{-n-1\choose k}x^{m+1+k}(x+y)^{-1-k-n}\,.
\enddisplay
In hypergeometric form, this reduces to
\begindisplay
\hyp{1-r,\,n+1}{m+2}{{-x\over y}}=\left(1+{x\over y}\right)^{\!\!-n-1}
\hyp{m+1+r,\,n+1}{m+2}{{x\over x+y}}\,,
\enddisplay
which is the special case $(a,b,c,z)=(n+1,m+1+r,m+2,-x/y)$ of
the reflection law \equ(5.|hyp-refl|).
(Thus \equ(5.|hyp-gen-vdm|) is related to reflection and to
the formula in exercise~|hyp-backwards|.)

\ex:
Problem 1 in Section 5.2
considers $\sum_{k\ge0}{r\choose k}\big/{s\choose k}$ when
$r$ and~$s$ are integers with $s\ge r\ge0$. What is the value of this
sum if $r$ and~$s$ aren't integers?
\answer If $r$ is a nonnegative integer, the sum is finite, and the
\g\setbox0=\vbox{\halign{\span\grafctr\cr
The boxed\cr sentence\cr on the\cr other side\cr of this page\cr
is self-\cr referential.\cr}}
\vskip-30pt
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
derivation in the text is valid as long as none of the terms of the
sum for $0\le k\le r$ has zero in the denominator. Otherwise the
sum is infinite, and the $k$th term ${k-r-1\choose k}\big/{k-s-1\choose k}$
is approximately $k^{s-r}(-s-1)!/(-r-1)!$ by \equ(5.|f-def-lim|).
So we need $r>s+1$ if the infinite series is going to converge.
(If $r$ and~$s$ are complex, the condition
is $\Re r>\Re s+1$, because $\vert k^z\vert
=k^{\Re z}$.) The sum is
\begindisplay
\hyp{-r,\,1}{-s}1={\Gamma(r-s-1)\Gamma(-s)\over\Gamma(r-s)\Gamma(-s-1)}
={s+1\over s+1-r}
\enddisplay
by \equ(5.|Fabc|); this is the same formula we found when $r$ and $s$ were
integers.

\ex:
Prove {\it "Whipple"'s identity},
\begindisplay \openup3pt
&\hyp{\half a,\,\half a+\half,\,1+a-b-c}{1+a-b,\,1+a-c}{{-4z\,\over(1-z)^2}}\cr
&\qquad=(1-z)^a\hyp{a,\,b,\,c}{1+a-b,1+a-c}z\,,
\enddisplay
by showing that both sides satisfy the same differential equation.
\answer (It's best to have computer help for this.)
Incidentally, when $c=(a+1)/2$, this reduces to an identity that's
equivalent to "Gauss's identity"
\equ(5.|hyp4z-4z2|), in view of Pfaff's reflection law.
For if $w=-z/(1-z)$ we have $4w(1-w)=-4z/(1-z)^2$, and
\begindisplay \openup3pt \postdisplaypenalty=-500
\hyp{\half a,\,\half a+\half-b}{1+a-b}{4w(1-w)}
&=\hyp{a,\,a+1-2b}{1+a-b}{{-z\over1-z}}\cr
&=(1-z)^a\hyp{a,\,b}{1+a-b}z\,.
\enddisplay
\source{"Whipple" [|whipple|].}

\ex:
Prove {\it "Clausen"'s product identities}
\begindisplay \openup4pt
&\hyp{a,\,b}{a+b+\half}z^2
&=\hyp{2a,\,a+b,\,2b}{2a+2b,\,a+b+\half}z\,;\cr
\noalign{\smallskip}
&\hyp{{1\over4}+a,\,{1\over4}+b}{1+a+b}z\,
 \hyp{{1\over4}-a,\,{1\over4}-b}{1-a-b}z\hidewidth\cr
&&=\hyp{\half,\,\half+a-b,\,\half-a+b}{1+a+b,\,1-a-b}z\,.
\enddisplay
What identities result when
the coefficients of $z^n$ on both sides of these formulas are equated?
\answer The identities can be proved, as Clausen proved them more than
150 years ago, by showing that both sides satisfy the same differential
equation. One way to write the resulting equations between coefficients
of $z^n$ is in terms of binomial coefficients:
\begindisplay \openup6pt
&\sum_k{\displaystyle{r\choose k}{s\choose k}{r\choose n-k}{s\choose n-k}\over
\displaystyle{r+s-1/2\choose k}{r+s-1/2\choose n-k}}
 ={\displaystyle {2r\choose n}{r+s\choose n}{2s\choose n}\over
 \displaystyle {2r+2s\choose n}{r+s-1/2\choose n}}\,;\cr
\noalign{\smallskip}
&\sum_k{\displaystyle{-1/4+r\choose k}{-1/4+s\choose k}
 {-1/4-r\choose n-k}{-1/4-s\choose n-k}
 \over\displaystyle{-1+r+s\choose k}{-1-r-s\choose n-k}}\cr
&\hskip5em={\displaystyle{-1/2\choose n}{-1/2+r-s\choose n}{-1/2-r+s\choose n}
 \over\displaystyle{-1+r+s\choose n}{-1-r-s\choose n}}\,.
\enddisplay
Another way is in terms of hypergeometrics:
\g\setbox0=\vbox{\halign{\span\grafctr\cr
The boxed\cr sentence\cr on the\cr other side\cr of this page\cr
is not self-\cr referential.\cr}}
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
\setmathsize{\hyp{a,b,\half-a-b-n,-n}{\half+a+b,1-a-n,1-b-n}1=
{(2a)\_^n\,(a+b)\_^n\,(2b)\_^n\over (2a+2b)\_^n\,a\_^n\,b\_^n}\,;}
\begindisplay \openup3pt
&\hyp{a,b,\half-a-b-n,-n}{\half+a+b,1-a-n,1-b-n}1=
{(2a)\_^n\,(a+b)\_^n\,(2b)\_^n\over (2a+2b)\_^n\,a\_^n\,b\_^n}\,;\cr
\noalign{\smallskip}
&\hyp{{1\over4}+a,{1\over4}+b,a+b-n,-n}{1+a+b,{3\over4}+a-n,{3\over4}+b-n}1\cr
&\mathsize{\hfill={(1/2)\_^n\,(1/2+a-b)\_^n\,(1/2-a+b)\_^n\over
  (1+a+b)\_^n\,(1/4-a)\_^n\,(1/4-b)\_^n}\,.}\cr
\enddisplay
\source{"Clausen" [|clausen1|],\thinspace[|clausen2|].}

\ex:
Show that the indefinite sum
\begindisplay
\sum\Biggl(\,\prod_{j=1}^{k-1}\bigl(f(j)+\alpha\bigr)\bigg/
 \prod_{j=1}^k f(j)\Biggr)\,\delta k
\enddisplay
has a (fairly) simple form, given any function~$f$ and any constant~$\alpha$.
\answer $\alpha^{-1}\prod_{j=1}^k\bigl(f(j)+\alpha\bigr)/f(j)$.
\source{"Gosper" [|gosper|].}

\ex:\exref|vdm-summable|%
Find $\sum{a\choose k}{-a\choose n-k}\,\delta k$.
\answer Gosper's algorithm finds the answer $-{a-1\choose k-1}{-a-1\choose
n-k}a/n+C$. Consequently, when $m\ge0$ is an integer, we have
\begindisplay
\sum{a\choose k}{m-a\choose n-k}\,\delta k=
\sum_j{m\choose j}{-a\over n-j}{a-1\choose k-1}{-a-1\choose n-j-k}+C\,.
\enddisplay

\ex:
What conditions in addition to \eq(|qr-condition|) will make the polynomials
$p$,~$q$,~$r$ of \eq(|tk-pqr|) uniquely determined?
\answer The leading coefficients of $p$ and $r$ should be unity, and $p$
should have no factors in common with $q$ or $r$. It is easy to fulfill
these additional conditions by shuffling factors around.\par
Now suppose $p(k+1)q(k)/p(k)r(k+1)=P(k+1)Q(k)/P(k)R(k+1)$, where
the polynomials $(p,q,r)$
and $(P,Q,R)$ both satisfy the new criteria. Let $p_0(k)=p(k)/g(k)$ and
$P_0(k)=P(k)/g(k)$, where $g(k)=\gcd\bigl(p(k),P(k)\bigr)$ is the product
of all common factors of $p$ and~$P$. Then
\begindisplay
p_0(k+1)@q(k)@P_0(k)@R(k+1)=p_0(k)@r(k+1)@P_0(k+1)@Q(k)\,.
\enddisplay
Suppose $p_0(k)\ne1$. Then there is a complex number $\alpha$ such that
$p_0(\alpha)=0$; this implies $q(\alpha)\ne0$, $r(\alpha)\ne0$, and
$P_0(\alpha)\ne0$. Hence we must have $p_0(\alpha+1)@R(\alpha+1)=0$ and
$p_0(\alpha-1)@Q(\alpha-1)=0$. Let $N$ be a positive integer such that
$p_0(\alpha+N)\ne0$ and $p_0(\alpha-N)\ne0$. Repeating the argument
$N$ times, we find
$R(\alpha+1)\ldots R(\alpha+N)=0=Q(\alpha-1)\ldots Q(\alpha-N)$,
contradicting \equ(5.|qr-condition|). Therefore $p_0(k)=1$. Similarly
$P_0(k)=1$, so $p(k)=P(k)$.
Now $q(\alpha)=0$ implies $r(\alpha+1)\ne0$, by \equ(5.|qr-condition|),
hence $q(k)\divides Q(k)$. Similarly $Q(k)\divides q(k)$, so $q(k)=Q(k)$
since they have the same leading coefficient. That leaves $r(k)=R(k)$.
\source{"Petkov\v{s}ek" [|petk|, Corollary 3.1].}

\ex:
Prove that if Gosper's algorithm finds no solution to \eq(|gosper-goal|),
given a hypergeometric term~$t(k)$, then there is no solution to the more
general equation
\begindisplay
t(k)=\bigl(T_1(k+1)+\cdots+T_m(k+1)\bigr)-\bigl(T_1(k)+\cdots+T_m(k)\bigr)\,,
\enddisplay
where $T_1(k)$, \dots, $T_m(k)$ are hypergeometric terms.
\answer \def\that{{\hat t}}\def\phat{{\hat p}}%
\def\tbar{{\bar t}}\def\pbar{{\bar p}}%
 If $r(k)$ is a nonzero rational function and $T(k)$ is a hypergeometric
term, then $r(k)T(k)$ is a hypergeometric term, which is called {\it
"similar"\/} to $T(k)$. (We allow $r(k)$ to be $\infty$ and $T(k)$ to
be $0$, or vice versa, for finitely many values of~$k$.)
In particular, $T(k+1)$ is always similar to~$T(k)$.
If $T_1(k)$ and $T_2(k)$ are similar hypergeometric terms, then $T_1(k)+T_2(k)$
is a hypergeometric term. If $T_1(k)$, \dots,~$T_m(k)$ are mutually
dissimilar, and $m>1$, then $T_1(k)+\cdots+T_m(k)$ cannot be zero for all
but finitely many~$k$.
For if it could, consider a counterexample for which
\g\setbox0=\vbox{\halign{\span\grafctr\cr
Burma-\cr Shave\cr}}
\hbox{\vrule\vtop{\hrule\kern8pt\hbox{\kern0pt\box0\kern0pt}\kern8pt\hrule}%
 \vrule}\g
$m$ is minimum, and let $r_j(k)=T_j(k+1)/T_j(k)$. Since $T_1(k)+\cdots+T_m(k)=
0$, we have $r_m(k)T_1(k)+\cdots+r_m(k)T_m(k)=0$ and $r_1(k)T_1(k)+\cdots+
r_m(k)T_m(k)=T_1(k+1)+\cdots+T_m(k+1)=0$;
 hence $\bigl(r_m(k)-r_1(k)\bigr)T_1(k)
+\cdots+\bigl(r_m(k)-r_{m-1}(k)\bigr)T_{m-1}(k)=0$.
We cannot have $r_m(k)-r_j(k)=0$, for any $j<m$,
since $T_j$ and $T_m$ are dissimilar. But $m$ was minimum, so this cannot
be a counterexample; it follows that $m=2$. But then $T_1(k)$ and $T_2(k)$
must be similar, since they are both zero for all but finitely many $k$.
\par
Now let $t(k)$ be any hypergeometric term with $t(k+1)/t(k)=r(k)$, and suppose
that $t(k)=\bigl(T_1(k+1)+\cdots+T_m(k+1)\bigr)-\bigl(T_1(k)+\cdots+
T_m(k)\bigr)$, where $m$ is minimal. Then $T_1$, \dots,~$T_m$ must be
mutually dissimilar. Let $r_j(k)$ be the rational function such that
\begindisplay
r(k)\bigl(T_j(k+1)-T_j(k)\bigr)-\bigl(T_j(k+2)-T_j(k+1)\bigr)=r_j(k)T_j(k)\,.
\enddisplay
Suppose $m>1$.
Since $0=r(k)t(k)-t(k+1)=r_1(k)T_1(k)+\cdots+r_m(k)T_m(k)$, we must
have $r_j(k)=0$ for all but at most one value of~$j$.
If $r_j(k)=0$, the function $\tbar(k)=T_j(k+1)-T_j(k)$ satisfies
$\tbar(k+1)/\tbar(k)=t(k+1)/t(k)$. So Gosper's algorithm will find a solution.
\source{"Petkov\v{s}ek" [|petk|, Corollary 5.1].}

\ex:\exref|gosper-bonus|%
Find all complex numbers $z$ such that $k!^2\big/\prod_{j=1}^k(j^2+jz+1)$ is
summable in hypergeometric terms.
\answer Suppose first that $z$ is not equal to $-d-1/d$ for any integer
$d>0$. Then in Gosper's algorithm we have $p(k)=1$, $q(k)=(k+1)^2$,
$r(k)=k^2+kz+1$. Since $\deg(Q)<\deg(R)$ and $\deg(p)-\deg(R)+1=-1$, the
only possibility is $z=d+2$ where $d$ is a nonnegative integer. Trying
$s(k)=\alpha_dk^d+\cdots+\alpha_0$ fails when $d=0$ but succeeds whenever
$d>0$. (The linear equations obtained by equating coefficients of $k^d$,
$k^{d-1}$, \dots,~$k^1$ in \equ(5.|pqrs-rec|) express $\alpha_{d-1}$,
\dots,~$\alpha_0$ as positive multiples of~$\alpha_d$, and the remaining
equation $1=\alpha_d+\cdots+\alpha_1$ then defines~$\alpha_d$.) For example,
when $z=3$ the indefinite sum is $(k+2)k!^2\big/\prod_{j=1}^{k-1}(j^2+3j+1)
+C$.\par
If $z=-d-1/d$, on the other hand, the stated terms $t(k)$ are infinite
for $k\ge d$. There are two reasonable ways to proceed: We can cancel the
zero in the denominator by redefining
\begindisplay\advance\abovedisplayskip-1pt\advance\belowdisplayskip-1pt
t(k)={k!^2\over\prod_{j=d+1}^k\bigl(j^2-j(d+1/d)+1\bigr)}=
{(d-1/d)!\,k!^2\over(k-1/d)!\,(k-d)!}\,,
\enddisplay
thereby making $t(k)=0$ for $0\le k<d$ and positive for $k\ge d$.
Then Gosper's algorithm gives $p(k)=k\_d$,
$q(k)=k+1$, $r(k)=k-1/d$, and we can solve \equ(5.|pqrs-rec|) for $s(k)$
because the coefficient of $k^j$ on the right is $(j+1+1/d)\alpha^j$ plus
multiples of $\{\alpha_{j+1},\ldots,\alpha_d\}$. For example, when $d=2$
the indefinite sum is $(3/2)!\,k!\,({2\over7}k^2-{26\over35}k+{32\over105})/
(k-3/2)!+C$.
\par
Alternatively, we can try to sum the original terms,
\g Look, any finite sequence is trivially summable, because we can find a
polynomial that matches $t(k)$ for $0\le k<d$.\g
but only in the range $0\le k<d$.
Then we can replace $p(k)=k\_d$ by
\begindisplay\advance\abovedisplayskip-2pt\advance\belowdisplayskip-2pt
p'(k)=\sum_{j=1}^d(-1)^{d-j}j{d\brack j}k^{j-1}\,.
\enddisplay
This is justified since \equ(5.|tk-pqr|) still holds for
$0\le k<d-1$; we have $p'(k)=\lim_{\epsilon\to0}\bigl((k+\epsilon)\_d
-k\_d\bigr)/\epsilon=\lim_{\epsilon\to0}(k+\epsilon)\_d/\epsilon$, so this
trick essentially cancels a $0$ from the numerator and denominator of
\equ(5.|tk-pqr|) as in "L'Hospital's rule".
 Gosper's method now yields an indefinite sum.

\ex:
What recurrence does the Gosper-Zeilberger method give for the sum
$S_n=\sum_k{n\choose2k}$?
\answer $nS_{n+1}=2nS_n$. (Beware: This gives no information about $S_1/S_0$.)
\source{Ira "Gessel".*}

\ex:\exref|prove-saalschutz-again|%
Use the "Gosper-Zeilberger" method to discover a closed form for
$\sum_k\kern-.9pt
 t(n,k)$ when $t(n,k)=(n+a+b+c+k)!/(n-k)!\,(c+k)!\,(b-k)!\,(a-k)!\,k!$,
assuming that $a$~is a nonnegative integer.
\answer Let $p(n,k)=(n+1+k)\beta_0(n)+(n+1+a+b+c+k)\beta_1(n)=\phat(n,k)$,
$\tbar(n,k)=t(n,k)/(n+1+k)$, $q(n,k)=(n+1+a+b+c+k)(a-k)(b-k)$,
$r(n,k)=(n+1+k)(c+k)k$. Then \equ(5.|zeil-pqrs-rec|) is solved by
$\beta_0(n)=(n+1+a+b+c)(n+1+a+b)$, $\beta_1(n)=-(n+1+a)(n+1+b)$,
$\alpha_0(n)=s(n,k)=-1$. We discover \equ(5.|sym-saalschutz|) by observing
that it is true when $n=-a$ and using induction on~$n$.

% Put this just before exercise 100!
\def\ex:{\par{\advance\medskipamount-1pt\medbr}%
  \advance\excount 1 \item{\kern-2pt\number\excount}}
\ex:
Find a recurrence relation for the sum
\begindisplay
S_n=\sum_{k=0}^n{1\over\displaystyle{n\choose k}}\,,
\enddisplay
and use the recurrence to find another formula for $S_n$.
\answer The Gosper-Zeilberger algorithm discovers easily that
\begindisplay\advance\belowdisplayskip-3pt
{n+2\over\displaystyle{n\choose k}}-
{2n+2\over\displaystyle{n+1\choose k}}=
{n-k\over\displaystyle{n\choose k+1}}-
{n+1-k\over\displaystyle{n\choose k}}\,,\qquad\hbox{$0\le k<n$}.
\enddisplay
Summing from $k=0$ to $n-1$ yields $(n+2)(S_n-1)-(2n+2)(S_{n+1}-1-{1\over n+1})
=-n$. Hence $(2n+2)S_{n+1}=(n+2)S_n+2n+2$. Applying a summation factor
now leads to the expression $S_n=(n+1)2^{-n-1}\sum_{k=1}^{n+1}2^k\!/k$.

\ex:\exref|legendre-recs|%
Find recurrence relations satisfied by the sums
\g Better use computer algebra for this one (and the next few).\g
\vskip3pt
\itemitem{a}$\displaystyle
S_{m,n}(z)=\sum_k{m\choose k}{n\choose k}z^k\,$;
\vskip3pt
\itemitem{b}$\displaystyle
S_n(z)=S_{n,n}(z)=\sum_k{n\choose k}^{\!2} z^k\,$.
\answer (a) If we hold $m$ fixed, the Gosper-Zeilberger algorithm discovers
that $(n+2)S_{m,n+2}(z)=(z-1)(n+1)S_{m,n}(z)+(2n+3-z(n-m+1))S_{m,n+1}(z)$.
We can also apply the method to the term
\begindisplay
\beta_0(m,n)t(m,n,k)+
\beta_1(m,n)t(m{+}1,n,k)+\beta_2(m,n)t(m,n{+}1,k)\,,
\enddisplay
in which case we get a
simpler recurrence,
\begindisplay
(m+1)S_{m+1,n}(z)-(n+1)S_{m,n+1}(z)=(1-z)(m-n)S_{m,n}(z)\,.
\enddisplay
(b)~Now we must work a little harder, with five equations in six unknowns.
The algorithm finds
\begindisplay\advance\abovedisplayskip-3pt\advance\belowdisplayskip-3pt
&(n+1)(z-1)^2{n\choose k}^2z^k-(2n+3)(z+1){n+1\choose k}^2z^k\cr
\noalign{\vskip-3pt}
&\qquad\qquad{}+(n+2){n+2\choose k}^2z^k=T(n,k+1)-T(n,k)\,,\cr
\noalign{\vskip3pt}
&T(n,k)={n+1\choose k-1}^2{s(n,k)\over n+1}\,z^k\,,\cr
&s(n,k)=(z{-}1)k^2-2((n{+}2)z{-}2n{-}3)k+(n{+}2)((n{+}2)z{-}4n{-}5)\,.\cr
\enddisplay
Therefore $(n+1)(z-1)^2S_n(z)-(2n+3)(z+1)S_{n+1}(z)+(n+2)S_{n+2}(z)=0$.
Incidentally, this recurrence holds also for negative $n$, and we have
$S_{-n-1}(z)=S_n(z)/(1-z)^{2n+1}$.\par
The sum $S_n(z)$ can be regarded as a modified form of the "Legendre
polynomial" $P_n(z)=\sum_k{n\choose k}^2(z-1)^{n-k}(z+1)^k\!/2^n$, since we
can write
% $S_n(z)=(1-z)^n P_n\bigl((1+z)/(1-z)\bigr)$. Similarly, $S_{m,n}(z)=
% (1-z)^n P_n^{(0,m-n)}\bigl((1+z)/(1-z)\bigr)$ is a modified "Jacobi
$S_n(z)=(1-z)^n P_n\bigl({1+z\over1-z}\bigr)$. Similarly, $S_{m,n}(z)=
(1-z)^n P_n^{(0,m-n)}\bigl({1+z\over1-z}\bigr)$ is a modified "Jacobi
polynomial".

\ex:
Use the "Gosper-Zeilberger procedure" to generalize the ``"useless"'' identity
\eq(|useless|): Find additional values of $a$, $b$, and $z$ such that
\begindisplay
\sum_k{n\choose k}{{1\over3}n-a\choose k}\,z^k\bigg/{{4\over3}n-b\choose k}
\enddisplay
has a simple closed form.
\answer The sum is $F(a-{1\over3}n,-n;b-{4\over3}n;-z)$, so we need not
consider the case $z=-1$. Let $n=3m$. We seek solutions to
\g How about $z=0$?\g
\equ(5.|zeil-pqrs-rec|) when
\begindisplay
p(m,k)&=(3m+3-k)\_3(m+1-k)\beta_0+(4m+4-b-k)\_4\beta_1\,,\cr
q(m,k)&=(3m+3-k)(m+1-a-k)z\,,\cr
r(m,k)&=k(4m+1-b-k)\,,\cr
s(m,k)&=\alpha_2k^2+\alpha_1k+\alpha_0\,.\cr
\enddisplay
The resulting five homogeneous equations have a nonzero solution
$(\alpha_0,\alpha_1,\alpha_2,\allowbreak
\beta_0,\beta_1)$ if and only if
the determinant of coefficients is zero; and this
determinant, a polynomial in~$m$, vanishes only in eight cases. One of
those cases is, of course, \equ(5.|useless|); but we can now evaluate the
sum for all nonnegative integers~$n$, not just $n\not\=2$ (mod~$3$):
\begindisplay
\sum_k{n\choose k}{{1\over3}n-{1\over6}\choose k}\,8^k\bigg/
 {{4\over3}n-{2\over3}\choose k}=[1,1,{\textstyle-\half}]{2n\choose n}\bigg/
 {{4\over3}n-{2\over3}\choose n}\,.
\enddisplay
Here the notation $[c_0,c_1,c_2]$ stands for the single value
$c_{n\,\bmod\,3}$.
Another case, $(a,b,z)=(\half,0,8)$, yields the identity
\begindisplay
\sum_k{n\choose k}{{1\over3}n-{1\over2}\choose k}\,8^k\bigg/
 {{4\over3}n\choose k}=[1,0,0]\,16^{n/3}{{2\over3}n\choose{1\over3}n}\bigg/
 {{4\over3}n\choose n}\,.
\enddisplay
(This sum, amazingly, is zero unless $n$ is a multiple of~$3$; and then the
identity can be written
\begindisplay
\sum_k{3m\choose k}{2m\choose2k}{2k\choose k}\,2^k\bigg/
{4m\choose k}{m\choose k}=16^m{(3m)!\,(2m)!\over(4m)!\,m!}\,,
\enddisplay
which might even be useful.) The remaining six cases generate even weirder
sums
\begindisplay
&\sum_k{n\choose k}{{1\over3}n-a\choose k}\,z^k\bigg/
 {{4\over3}n-b\choose k}\cr
&\qquad=[c_0,c_1,c_2]{\displaystyle
{{1\over3}n-a\choose\lfloor n/3\rfloor}{{1\over3}n-a'\choose\lfloor n/3\rfloor}
_{\mathstrut}x^{\lfloor n/3\rfloor}
\over\displaystyle{{4\over3}n-b\choose n}^{\mathstrut}
{{1\over3}n-b\choose\lfloor n/3\rfloor}
{{1\over3}n-b'\choose\lfloor n/3\rfloor}}
\enddisplay
where the respective values of $(a,b,z,c_0,c_1,c_2,a',b',x)$ are
\begindisplay
\vbox{\halign{&$(\hfil\textstyle{#}$&
               $,\,\hfil\textstyle{#}$&
               $,\hfil\textstyle{#}$&
               $,\hfil\textstyle{#}$&
               $,\hfil\textstyle{#}$&
               $,\hfil\textstyle{#}$&
               $,\,\hfil\textstyle{#}$&
               $,\hfil\textstyle{#}$&
               $,\hfil\textstyle{#})$&$\,#$\hfil\qquad\cr
7\over12&1\over3&8&1&-1&0&1\over4&0&64&;&
   1\over4&0&8&1&2&0&7\over12&1\over3&64&;\cr
5\over12&2\over3&8&1&0&-3&3\over4&0&64&;&
   1\over12&1\over3&8&1&3&0&3\over4&0&64&;\cr
1\over2&0&-4&1&2&0&1\over6&1\over3&-16&;&
   1\over6&2\over3&-4&1&0&-3&5\over6&0&-16&.\cr}}
\enddisplay
\source{H. S. "Wilf".*}

\ex:
Let $t(n,k)$ be the proper term \eq(|pt-def|). What are the degrees of
$\phat(n,k)$, $q(n,k)$, and $r(n,k)$ in terms of the variable~$k$, when
the Gosper-Zeilberger procedure is applied to $\that(n,k)=\beta_0(n)t(n,k)
+\cdots+\beta_l(n)t(n+l,k)$? (Ignore the rare, exceptional cases.)
\answer We assume that each $a'_i$ and $b'_i$ is nonzero, since the
corresponding factors would otherwise have no influence on the degrees
in~$k$. Let
$\that(n,k)=\phat(n,k)\tbar(n,k)$ where
\begindisplay
\tbar(n,k)={\prod_{i=1_{\mathstrut}}^p
          \bigl(a_in+a'_ik+a_il@\[a_i<0]+a''_i\bigr)!\over
      \prod_{i=1}^{q^{\mathstrut}}\bigl(b_in+b'_ik+b_il@\[b_i>0]+b''_i\bigr)!}
                  z^k\,.
\enddisplay
Then we have $\deg(\phat)=\deg(f)+\max\bigl(
  \sum_{i=1}^q b_i\[b_i>0]-\sum_{i=1}^p a_i\[a_i<0],\allowbreak
  \sum_{i=1}^p a_i\[a_i>0]-\sum_{i=1}^q b_i\[b_i<0]\bigr)
\ge\deg(f)+\half l@\bigl(\vert a_1\vert+\cdots+\vert a_p\vert
                      +\vert b_1\vert+\cdots+\vert b_q\vert\bigr)$, except in
unusual cases where cancellation occurs in the leading coefficient. And
$\deg(q)=\sum_{i=1}^p a'_i\[a'_i>0]-\sum_{i=1}^q b'_i\[b'_i<0]$,
$\deg(r)=\sum_{i=1}^q b'_i\[b'_i>0]-\sum_{i=1}^p a'_i\[a'_i<0]$,
again except in unusual cases.\par
(These estimates can be used to show directly that, as $l$ increases, the
degree of $\phat$ eventually becomes large enough to make a polynomial
$s(n,k)$ possible, and the number of unknown $\alpha_j$ and $\beta_j$
eventually becomes larger than
the number of homogeneous linear equations to be solved. So we obtain another
proof that the Gosper-Zeilberger algorithm succeeds, if we argue as in
the text that there must be a solution with $\beta_0(n)$, \dots,~$\beta_l(n)$
not all zero.)

\ex:
Use the Gosper-Zeilberger procedure to verify the remarkable identity
\begindisplay
\sum_k(-1)^k{r-s-k\choose k}{r-2k\choose n-k}{1\over r-n-k+1}
={s\choose n}{1\over r-2n+1}\,.
\enddisplay
Explain why the simplest recurrence for this sum is not found.
\answer Let $t(n,k)=(-1)^k(r-s-k)!\,(r-2k)!/\bigl((r-s-2k)!\,(r-n-k+1)!
\,(n-k)!\allowbreak\,k!\bigr)$.
Then $\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)$ is not summable in hypergeometric
terms, because $\deg(\phat)=1$, $\deg(q-r)=3$,
$\deg(q+r)=4$, $\lambda=-8$, $\lambda'=-4$;
but $\beta_0(n)t(n,k)+\beta_1(n)t(n+1,k)+\beta_2(n)t(n+2,k)$
is\dash---basically
because $\lambda'=0$ when $q(n,k)=-(r-s-2k)(r-s-2k-1)(n+2-k)(r-n-k+1)$ and
$r(k)=(r-s-k+1)(r-2k+2)(r-2k+1)@k$. The solution is
\begindisplay
\beta_0(n)&=(s-n)(r-n+1)(r-2n+1)\,,\cr
\beta_1(n)&=(rs-s^2-2rn+2n^2-2r+2n)(r-2n-1)\,,\cr
\beta_2(n)&=(s-r+n+1)(n+2)(r-2n-3)\,,\cr
\alpha_0(n)&=r-2n-1\,,\cr
\enddisplay
and we may conclude that $\beta_0(n)S_n+\beta_1(n)S_{n+1}+\beta_2(n)S_{n+2}=0$
when $S_n$ denotes the stated sum. This suffices to prove the identity by
induction, after verifying the cases $n=0$ and $n=1$.\par
\def\betabar{{\bar\beta}}
But $S_n$ also satisfies the simpler recurrence
$\betabar_0(n)S_n+\betabar_1(n)S_{n+1}=0$, where
$\betabar_0(n)=(s-n)(r-2n+1)$ and $\betabar_1(n)=-(n+1)(r-2n-1)$. Why didn't
the method discover this? Well, nobody ever said that such a recurrence
necessarily forces the terms $\betabar_0(n)t(n,k)+\betabar_1(n)t(n+1,k)$ to be
indefinitely summable. The surprising thing is that the Gosper-Zeilberger
method actually does find the simplest recurrence in so many other cases.\par
Notice that the second-order recurrence we found can be factored:
\begindisplay
&\beta_0(n)+\beta_1(n)@N+\beta_2(n)@N^2\cr
&\qquad=
\bigl((r-n+1)N+(r-s-n-1)\bigr)\;\bigl(\betabar_0(n)+\betabar_1(n)@N\bigr)\,,
\enddisplay
where $N$ is the shift operator in \equ(5.|ldo|).
\source{Volker "Strehl".*}

\ex:
Show that if $\omega=e^{2\pi i/3}$ we have
\begindisplay
\sum_{k+l+m=3n}{3n\choose k,l,m}^{\!2}\omega^{l-m}={4n\choose n,n,2n}\,,
\qquad\hbox{integer $n\ge0$.}
\enddisplay
\answer  Set $a=1$ and compare
the coefficients of $z^{3n}$ on both sides of
"Henrici"'s ``"friendly monster"'' identity,
\begindisplay
&f(a,z)\,f(a,\omega z)\,f(a,\omega^2z)\cr
&\quad=\hyp{\half a-{1\over4},\,\,\,\half a+{1\over4}}
{{1\over3}a,\,\,{1\over3}a{+}{1\over3},\,\,{1\over3}a{+}{2\over3},\,\,
 {2\over3}a{-}{1\over3},\,\, {2\over3}a,\,\,{2\over3}a{+}{1\over3},\,\,
  a\,\,}{\left(4z\over9\right)^{\!\!3}\,},
\enddisplay
where $f(a,z)=F(1;a,1;z)$. The identity can be proved by showing that both
sides satisfy the same differential equation.
\par
Peter "Paule" has found another interesting way to evaluate the sum:
\begindisplay
  \sum_{k,l}{N\choose k,l,N-k-l}^{\!2}&\omega^{k+2l}
=\sum_{k,l}{N\choose k-l,l,N-k}^{\!2}\omega^{k+l}\cr
&=\sum_{k,l}{N\choose k}^{\!2}{k\choose l}^{\!2}\omega^{k+l}\cr
&=\sum_k{N\choose k}^{\!2}\omega^{k}\,
        [z^k]\,\bigl((1+z)(\omega+z)\bigr)^k\cr
&=[z^0]\,\sum_k{N\choose k}^{\!2}
        \left(\omega(1+z)(\omega+z)\over z\right)^{\!k}\cr
&=[z^0]\,\sum_{k,j}{N\choose k}^{\!2}{k\choose j}
        \left({\omega(1+z)(\omega+z)\over z}-1\right)^{\!j}\cr
&=[z^0]\,\sum_{k,j}{N\choose k}{N-j\choose N-k}{N\choose j}
        \left((\omega z-1)^2\over\omega z\right)^{\!j}\cr
&=\sum_j{2N-j\choose N}{N\choose j}\,[z^j]\,(z-1)^{2j}\cr
&=\sum_j{2N-j\choose N}{N\choose j}{2j\choose j}(-1)^j\,,\cr
\enddisplay
using the binomial theorem, Vandermonde's convolution, and the fact that
$[z^0]g(az)=[z^0]g(z)$. We can now set $N=3n$ and apply the Gosper-Zeilberger
algorithm
to this sum~$S_n$, miraculously obtaining the first-order recurrence
$(n+1)^2S_{n+1}=4(4n+1)(4n+3)S_n$; the result follows by induction.\par
If $3n$ is replaced by $3n+1$ or $3n+2$, the stated sum is zero.
Indeed, $\sum_{k+l+m=N}t(k,l,m)\omega^{l-m}$ is always zero when
$N\bmod3\ne0$ and $t(k,l,m)=t(l,m,k)$.
\source{"Henrici" [|henrici-bieberbach|, p.~118].}

\ex:\exref|reprove-bc-quad|%
Prove the amazing identity \eq(|bc-quad|) by letting $t(r,j,k)$ be the
summand divided by the right-hand side, then showing that there are
functions $T(r,j,k)$ and $U(r,j,k)$ for which
\begindisplay
t(r+1,j,k)-t(r,j,k)&=T(r,j+1,k)-T(r,j,k)\cr
&\qquad{}+U(r,j,k+1)-U(r,j,k)\,.
\enddisplay
\answer (Solution by Shalosh B "Ekhad".) Let
\begindisplay
%T(r,j,k)&={(1{-}j{+}s{+}sk{+}n{-}jn{+}r{-}jr{-}kr{+}sr{+}nr)(j-l)@j\over
T(r,j,k)&={((1{+}n{+}s)(1{+}r)-(1{+}n{+}r)@j+(s{-}r)@k)(j{-}l)@j\over
(l-m+n-r+s)(n+r+1)(j-r-1)(j+k)}t(r,j,k)\,;\cr
U(r,j,k)&={(s+n+1)(k+l)@k\over(l-m+n-r+s)(n+r+1)(j+k)}t(r,j,k)\,.
\enddisplay
The stated equality is routinely verifiable, and \equ(5.|bc-quad|) follows
by summing with respect to $j$ and $k$. (We sum $T(r,j+1,k)-T(r,j,k)$ first
with respect to~$j$, then with respect to~$k$; we sum the other terms
$U(r,j,k+1)-U(r,j,k)$ first with respect to~$k$, then with respect to~$j$.)
\par Well, we also need to verify \equ(5.|bc-quad|) when $r=0$. In that
case it reduces via trinomial revision
to $\sum_k(-1)^k{n\choose n+l}{n+l\choose k+l}{s+n-k\choose m}
=(-1)^l{n\choose n+l}{s\choose m-n-l}$. We are assuming that $l$, $m$, and $n$
are integers and $n\ge0$. Both sides are clearly zero unless
$n+l\ge 0$. Otherwise we can replace $k$ by $n-k$ and use \equ(5.|bc-prod3|).

\ex:\exref|not-holonomic|%
Prove that $1/(nk+1)$ is not a "proper term".
\answer If it were proper, there would be a linear difference operator
\g Noticee that $1/nk$ is proper, since it's\par
$(n-1)!(k-1)!/$\par
$n!\,k!$. Also\par $1/(n^2-k^2)$ is proper. But\vskip2pt $1/(n^2+k^2)$ isn't.\g
that annihilates it. In other words, we would have a finite summation identity
\begindisplay
\sum_{i=0}^I\sum_{j=0}^J\alpha_{i,j}(n)\big/\bigl((n+i)(k+j)+1\bigr)=0\,,
\enddisplay
where the $\alpha$'s are polynomials in $n$, not all zero. Choose integers
$i$, $j$, and $n$ such that $n>1$ and $\alpha_{i,j}(n)\ne0$. Then when
$k=-1/(n+i)-j$, the $(i,j)$ term in the sum is infinite but the other
terms are finite.

\ex:\exref|apery-mn|%
Show that the Ap\'ery numbers $A_n$ of \eq(|apery-sum|) are the
diagonal elements $A_{n,n}$ of a matrix of numbers defined by
\begindisplay
A_{m,n}=\sum_{j,k}{m\choose j}^2\,{m\choose k}^2\,{2m+n-j-k\choose 2m}\,.
\enddisplay
Prove, in fact, that this matrix is symmetric, and that
\begindisplay
A_{m,n}
&=\sum_k{m+n-k\choose k}^2\,{m+n-2k\choose m-k}^2\cr
&=\sum_k{m\choose k}\,{n\choose k}\,{m+k\choose k}\,{n+k\choose k}\,.
\enddisplay
\answer Replace $k$ by $m-k$ in the double sum, then
use \equ(5.|bc-saalschutz|) to sum on~$k$, getting
\begindisplay
A_{m,n}=\sum_j{m\choose j}^2\,{m+n-j\choose m}^2\,;
\enddisplay
trinomial revision \equ(5.|bc-tc|) then yields one of the desired
formulas.\par
It appears to be difficult to find a direct proof that the two symmetrical
sums for $A_{m,n}$ are equal. We can, however, prove the equation
indirectly with the Gosper-Zeilberger
algorithm, by showing that both sums satisfy the recurrence
\begindisplay
(n+1)^3A_{m,n}-f(m,n)A_{m,n+1}+(n+2)^3A_{m,n+2}=0\,,
\enddisplay
where $f(m,n)=(2n+3)(n^2+3n+2m^2+2m+3)$.
Setting $t_1(n,k)={m\choose k}{n\choose k}{m+k\choose k}{n+k\choose k}$ and
$t_2(n,k)={m+n-k\choose k}^2{m+n-2k\choose m-k}^2$, we find
\begindisplay
&(n+1)^2t_j(n,k)-f(m,n)t_j(n+1,k)+(n+2)^2t_j(n+2,k)\cr
&\qquad=T_j(n,k+1)-T_j(n,k)\,,\cr
\enddisplay
where $T_1(n,k)=-2(2n+3)k^4t_1(n,k)/(n+1-k)(n+2-k)$ and
$T_2(n,k)=-\bigl((n+2)(4mn+n+3m^2+8m+2)-2(3mn+n+m^2+6m+2)k+{(2m+1)k^2}\bigr)
k^2(m+n+1-k)^2t_2(n,k)/(n+2-k)^2$. This proves the recurrence, so we
need only verify equality when $n=0$ and $n=1$.
(We could also have used the simpler recurrence
\begindisplay
m^3A_{m,n-1}-n^3A_{m-1,n}=(m-n)(m^2+n^2-mn)A_{m-1,n-1}\,,
\enddisplay
which can be discovered by the method of exercise |legendre-recs|.)\par
The fact that the first formula for $A_{m,n}$ equals the third implies a
remarkable identity between the
generating functions $\sum_{m,n}A_{m,n}w^mz^n$:
\begindisplay
\sum_k{w^kS_k(z)^2\over(1-z)^{2k+1}}=
\sum_k{2k\choose k}^2\!{w^k\over(1-w)^{2k+1}}{z^k\over(1-z)^{2k+1}}\,,
\enddisplay
where $S_k(z)=\sum_j{k\choose j}^2z^j$. It turns out, in fact, that
\begindisplay
\sum_k{w^k@S_k(x)@S_k(y)\over(1-x)^k(1-y)^k}=
\sum_k{2k\choose k}{w^k\over(1-w)^{2k+1}}{\sum_j{k\choose j}^2x^jy^{k-j}\over
(1-x)^k(1-y)^k}\,;
\enddisplay
this is a special case of an identity discovered by "Bailey" [|bailey-jacobi|].
\source{"Ap\'ery" [|apery|].}

\ex:
Prove that the "Ap\'ery numbers" \eq(|apery-sum|) satisfy
\begindisplay
A_n\=A_{\lfloor n/p\rfloor}A_{n\,\bmod\,p}\qquad\hbox{(mod~$p$)}
\enddisplay
for all primes~$p$ and all integers $n\ge0$.
\answer Let $X_n=\sum_k{n\choose k}^{a_0}{n+k\choose k}^{a_1}\ldots
{n+lk\choose k}^{a_l}x^k$ for any positive integers $a_0$, $a_1$, \dots,~$a_l$,
and any integer~$x$. Then if $0\le m<p$ we have
\begindisplay \openup3pt
X_{m+pn}&=\sum_{j=0}^{p-1}\sum_k{m+pn\choose j+pk}^{\!a_0}\ldots
{m+pn+l(j+pk)\choose j+pk}^{\!a_l}x^{j+pk}\,,\cr
X_mX_n&=\sum_{j=0}^{p-1}\sum_k{m\choose j}^{\!a_0}{n\choose k}^{\!a_0}\ldots
{m+l@j\choose j}^{\!a_l}{n+lk\choose k}^{\!a_l}x^{j+k}\,.
\enddisplay
And corresponding terms are congruent (mod $p$), because exercise |bc-div-p|
implies that they are multiples of~$p$ when $lj+m\ge p$, exercise |lucas-bc|
implies that the binomials are congruent when $lj+m<p$, and
\equ(4.|alt-fermat-theorem|) implies that $x^p\=x$.
\source{"Gessel" [|gessel-apery|].}

\subhead \kern-2ptResearch problems

\ex:
For what values of $n$ is ${2n\choose n}\=(-1)^n$ \tmod{(2n+1)}?
\answer The congruence surely holds if $2n+1$ is prime.
\g Ilan "Vardi" notes that the condition holds for
$2n+1=p^2$, where~$p$ is prime, if~and only~if\smallskip
$2^{p-1}\bmod{p^2}=1$.\smallskip
This yields two more examples:\smallskip
$n=(1093^2-1)/2$;\par
$n=(3511^2-1)/2$.\g
Steven "Skiena" has also
found the example $n=2953$, when $2n+1=3\cdt11\cdt179$.
\source{R. William "Gosper", Jr.*}

\ex:
Let $q(n)$ be the smallest odd prime factor of the
"middle binomial coefficient"~$2n\choose n$.
According to exercise |bc-div-p|,
the odd primes~$p$ that do {\it not\/} divide
$2n\choose n$ are those for which all
digits in $n$'s radix~$p$ representation are $(p-1)/2$ or less.
Computer experiments have shown that $q(n)\le11$ for $1<n<10^{10000}$,
except that $q(3160)=13$.
\smallskip
\itemitem{a} Is $q(n)\le11$ for all $n>3160$?
\itemitem{b} Is $q(n)=11$ for infinitely many $n$?
\smallskip
\par A "reward" of \$$7\cdot11\cdot13$ is offered for
a solution to either (a) or~(b).
\answer See [|ruzsa-et-al|] for partial results. The computer experiments
were done by V.\thinspace A. "Vyssotsky".
\source{[|erdos-graham|, p.~71].}

\ex:
Is $2n\choose n$ divisible either by $4$ or by $9$, for all $n>4$ except
$n=64$ and $n=256$?
\answer If $n$ is not a power of $2$, $2n\choose n$ is a multiple of~$4$
because of exercise~|bc-div-p|. Otherwise the stated phenomenon
was verified for $n\le2^{22000}$ by A.~"Granville" and O.~"Ramar\'e",
who also sharpened a theorem of S\'ark\"ozy [|sarkozy|]
by showing that $2n\choose n$ is divisible by the square of a prime for
all $n>2^{22000}$. This established a long-standing conjecture that
$2n\choose n$ is never "squarefree" when $n>4$.\par
The analogous conjectures for cubes are that $2n\choose n$ is divisible
by the cube of a prime for all $n>1056$, and by either $2^3$ or $3^3$ for
all $n>2^{29}+2^{23}$. This has been verified for all $n<2^{10000}$.
Paul "Erd\H os" conjectures that, in fact,
$\max_p \epsilon_p\bigl({2n\choose n}\bigr)$ tends to infinity as $n\to\infty$;
this might be true even if we restrict $p$ to the values $2$ and~$3$.
\source{[|erdos-graham|, p.~71].}

\ex:
If $t(n+1,k)/t(n,k)$ and $t(n,k+1)/t(n,k)$ are rational functions of $n$
and~$k$, and if there is a nonzero linear difference operator $H(N,K,n)$ such
that $H(N,K,n)@t(n,k)=0$, does it follow that $t(n,k)$ is a "proper term"?
\answer The theorem about generating functions in exercise 7.|d-finite-defined|
may help resolve this conjecture.
\source{"Wilf" and "Zeilberger" [|wilf-zeil|].}

\ex:
Let $m$ be a positive integer, and define the sequence $c_n^{(m)}$ by the
recurrence
\begindisplay
\sum_k{n\choose k}^{\!m}{n+k\choose k}^{\!m}=
\sum_k{n\choose k}{n+k\choose k}\,c_k^{(m)}\,.
\enddisplay
Are these numbers $c_n^{(m)}$ integers?
\answer "Strehl" [|strehl|] has shown that $c_n^{(2)}=\sum_k{n\choose k}^3
=\sum_k{n\choose k}^2{2k\choose n}$
is a so-called "Franel number"~[|franel|], and that $c_n^{(3)}=
\sum_k{n\choose k}{}^2{2k\choose k}{}^2{2k\choose n-k}$.
In another direction, H.\thinspace S. "Wilf" has shown that
$c_n^{(m)}$ is an integer for all~$m$ when $n\le9$.
\source{"Strehl" [|strehl|] credits A. "Schmidt".}

