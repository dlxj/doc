

# 多层全连接神经网络



上标表示层

总共有$K$ 层，第$0$ 层是输入层，第$K$ 层是输出层

第$1$ 到$K-1$ 层是隐层





$x^0_i$ 第$0$ 层输入向量的第$i$ 分量

$n_0$ 第$0$ 层总共有$n_0$ 个分量

$a^k_i$ 第$k$ 层第$i$ 个神经元的仿射值

$x^k_i$ 第$k$ 层第$i$ 个神经元的激活值

第$k$ 层有$n_k$ 个神经元，所以该层有$n_k$ 个输出




$$
\begin{bmatrix}
w^{1}_{1,1} & w^{1}_{1,2} & \cdots & w^{1}_{1,n}  \\
w^{1}_{2,1} & w^{1}_{2,2} & \cdots & w^{1}_{2,n} \\
\vdots & \vdots & \ddots & \vdots & \\
w^{1}_{n,1} & w^{1}_{n,2} & \cdots & w^{1}_{n,n} \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{0}_{1,1} & x^{0}_{1,2} & \cdots & x^{0}_{1,m}  \\
x^{0}_{2,1} & x^{0}_{2,2} & \cdots & x^{0}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{0}_{n,1} & x^{0}_{n,2} & \cdots & x^{0}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{1}_{1,1} & b^{1}_{1,2} & \cdots & b^{1}_{1,m}  \\
b^{1}_{2,1} & b^{1}_{2,2} & \cdots & b^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
b^{1}_{n,1} & b^{1}_{n,2} & \cdots & b^{1}_{n,m} \\
\end{bmatrix}

\\

= 

\begin{bmatrix}
a^{1}_{1,1} & a^{1}_{1,2} & \cdots & a^{1}_{1,m}  \\
a^{1}_{2,1} & a^{1}_{2,2} & \cdots & a^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
a^{1}_{n,1} & a^{1}_{n,2} & \cdots & a^{1}_{n,m} \\
\end{bmatrix}
$$
$(n \times n) (n \times m) \rightarrow (n \times m)$


$$
a^1_{i,j} \ , for \ i \in 1 \cdots n, \ j \in 1 \cdots m
$$
$a^1_{i,j}$ 总共$n*m$ 个函数


$$
a^1_{1,1} = w^{1}_{1,1} x^{0}_{1,1}  + w^{1}_{1,2} x^{0}_{2,1} + \cdots + w^{1}_{1,n} x^{0}_{n,1} + b^{1}_{1,1}
$$

$$
g(x) = \frac{1}{1 + e^{-x}}
$$

$$
g \bigg (
\begin{bmatrix}
a^{1}_{1,1} & a^{1}_{1,2} & \cdots & a^{1}_{1,m}  \\
a^{1}_{2,1} & a^{1}_{2,2} & \cdots & a^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
a^{1}_{n,1} & a^{1}_{n,2} & \cdots & a^{1}_{n,m} \\
\end{bmatrix}
\bigg )

= 

\begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}
$$

$$
\begin{bmatrix}
w^{2}_{1,1} & w^{2}_{1,2} & \cdots & w^{2}_{1,n}  \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{2}_{1,1} & b^{2}_{1,2} & \cdots & b^{2}_{1,m}  \\
\end{bmatrix}
\\
= 
\begin{bmatrix}
a^{2}_{1,1} & a^{2}_{1,2} & \cdots & a^{2}_{1,m}  \\
\end{bmatrix}
$$
$(1 \times n) (n \times m) \rightarrow (1 \times m)$


$$
a^{2}_{1,1} = w^{2}_{1,1} x^{1}_{1,1} + w^{2}_{1,2} x^{1}_{2,1} + \cdots + w^{2}_{1,n} x^{1}_{n,1} + b^{2}_{1,1}
$$

$$
g \bigg (
\begin{bmatrix}
a^{2}_{1,1} & a^{2}_{1,2} & \cdots & a^{2}_{1,m}  \\
\end{bmatrix}
\bigg )

= 

\begin{bmatrix}
h^{2}_{1,1} & h^{2}_{1,2} & \cdots & h^{2}_{1,m}  \\
\end{bmatrix}
$$

> h 是预测值，既输出


$$
(f \circ g)'(x) = f'(g(x))g'(x)
$$

$$
\frac{d}{d x} g(x) = \frac{d}{d x} (\frac{1}{1 + e^{-x}}) = g(x)(1 - g(x))
$$

$$
\frac{\partial}{\partial w_{1}} h^{2}_{1,1} = 
\frac{\partial}{\partial w_{1}} g(a^{2}_{1,1}) \\
= \frac{\partial}{\partial w_1} g( a^{2}_{1,1} ) \frac{\partial}{\partial w_1} a^{2}_{1,1} \\
= g( a^{2}_{1,1} ) ( 1 - g( a^{2}_{1,1} ) ) \frac{\partial}{\partial w_1} a^{2}_{1,1} \\
$$

$$
\mathcal{L} =  \frac{1}{2m} \sum^{m}_{i=1}(h^{2}_{1,i} - y_{i})^2 \\
= \frac{1}{m} \sum^{m}_{i=1} 

(h^{2}_{1,i} - y_{i}) \frac{\partial}{\partial w_{j}} h^{2}_{1,i}
$$




















## 前向传播



$f$ 激活映射，$g$ 仿射映射，$h$损失映射

> 仿射映射可以看作由若干个仿射函数组成
>
> 仿射函数是一个线性函数加上一个常量





第零层（输入层）输入值


$$
X^0 = \begin{bmatrix}
x^{0}_{1,1} & x^{0}_{1,2} & \cdots & x^{0}_{1,m}  \\
x^{0}_{2,1} & x^{0}_{2,2} & \cdots & x^{0}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{0}_{n,1} & x^{0}_{n,2} & \cdots & x^{0}_{n,m} \\
\end{bmatrix}
$$




第一层（隐层）仿射值


$$
g(W^1,X^0,B^1) = W^1 \cdot X^0 + B^1 =

\begin{bmatrix}
w^{1}_{1,1} & w^{1}_{1,2} & \cdots & w^{1}_{1,n}  \\
w^{1}_{2,1} & w^{1}_{2,2} & \cdots & w^{1}_{2,n} \\
\vdots & \vdots & \ddots & \vdots & \\
w^{1}_{n,1} & w^{1}_{n,2} & \cdots & w^{1}_{n,n} \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{0}_{1,1} & x^{0}_{1,2} & \cdots & x^{0}_{1,m}  \\
x^{0}_{2,1} & x^{0}_{2,2} & \cdots & x^{0}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{0}_{n,1} & x^{0}_{n,2} & \cdots & x^{0}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{1}_{1,1} & b^{1}_{1,2} & \cdots & b^{1}_{1,m}  \\
b^{1}_{2,1} & b^{1}_{2,2} & \cdots & b^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
b^{1}_{n,1} & b^{1}_{n,2} & \cdots & b^{1}_{n,m} \\
\end{bmatrix}

\\

= 

\begin{bmatrix}
a^{1}_{1,1} & a^{1}_{1,2} & \cdots & a^{1}_{1,m}  \\
a^{1}_{2,1} & a^{1}_{2,2} & \cdots & a^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
a^{1}_{n,1} & a^{1}_{n,2} & \cdots & a^{1}_{n,m} \\
\end{bmatrix}
$$

维度检查：$(n \times n) (n \times m) \rightarrow (n \times m)$





第一层（隐层）激活值
$$
f( g(W^1,X^0,B^1) ) = X^1 = \begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}
$$




第二层（输出层）仿射值


$$
g(W^2,X^1,B^2) = W^2 \cdot X^1 + B^2 =

\begin{bmatrix}
w^{2}_{1,1} & w^{2}_{1,2} & \cdots & w^{2}_{1,n}  \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{2}_{1,1} & b^{2}_{1,2} & \cdots & b^{2}_{1,m}  \\
\end{bmatrix}
\\
= 
\begin{bmatrix}
a^{2}_{1,1} & a^{2}_{1,2} & \cdots & a^{2}_{1,m}  \\
\end{bmatrix}
$$


维度检查：$(1 \times n) (n \times m) \rightarrow (1 \times m)$

 



第二层（输出层）激活值（输出值）


$$
f(g(W^2,X^1,B^2)) = X^2 = \begin{bmatrix}
x^{2}_{1,1} & x^{2}_{1,2} & \cdots & x^{2}_{1,m}  \\

\end{bmatrix}
$$




损失值（均方误差）
$$
\mathcal{L} = h(X^2, Y) =  \frac{1}{2m} \sum^{m}_{i=1}(x^2_{1,i} - y^{i})^2
$$




$$
\frac{\partial}{\partial w^2_{1,j}} \mathcal{L} = \frac{1}{m} \sum^{m}_{i=1} (x^2_{1,i} - y^i) \frac{\partial}{\partial w^2_{1,j}} x^2_{1,i}
$$













## 反向传播



若将损失值$\mathcal{L}$ 视为某个激活值的函数，则它是若干个映射的复合

> 将这个激活值视为$\mathcal{L}$ 的唯一变量，其它所有量都认为是常量
>
> 这是为了求偏导



例如第一层的某个激活值：


$$
\mathcal{L}(x^1_{i,j}) = h( f ( g( x^1_{i,j} ) ) )
$$


根据链式法则，$\mathcal{L}$ 对$x^k_{i,j}$ （这里的$k = 1$）的雅可比（导数）是这三个映射在相应位置的雅可比之积：


$$
\frac{\partial \mathcal{L}}{\partial x^k_{i,j}} = A_h \cdot A_f \cdot A_g
$$

$$
A_h = \begin{pmatrix}
\frac{\partial \mathcal{L}}{\partial x^{k+1}_{1,1}} & \cdots & \frac{\partial \mathcal{L}}{\partial x^{k+1}_{1,m}}
\end{pmatrix}
$$

$$
A_f = \begin{pmatrix}
\frac{\partial x^{k+1}_{1,1}}{\partial a^{k+1}_{1,1}} & \cdots & \frac{\partial x^{k+1}_{1,m}}{\partial a^{k+1}_{1,m}}
\end{pmatrix}
$$

$$
A_g = \begin{pmatrix}
\frac{\partial a^{k+1}_{1,1}}{x^k_{1,1}}
\end{pmatrix}
$$









预测时将 $w$和$b$ 视为常量，将 $x$ 视为变量；训练时则将$x$ 视为常量，将 $w$ 和$b$ 视为变量。



多元函数的梯度是列向量



坚持认为是权重$W$ 对输入$X$ 进行了线性变换







训练阶段将 $w$ 和$b$ 视为变量，将$x$ 视为常量



映射


$$
g(w^1, b^1) = a^1
$$

$$
w^1 \cdot x^0 + b^1 = a^1
$$




$$
\begin{bmatrix}
w^{1}_{1,1} & w^{1}_{1,2} & \cdots & w^{1}_{1,n}  \\
w^{1}_{2,1} & w^{1}_{2,2} & \cdots & w^{1}_{2,n} \\
\vdots & \vdots & \ddots & \vdots & \\
w^{1}_{n,1} & w^{1}_{n,2} & \cdots & w^{1}_{n,n} \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{0}_{1,1} & x^{0}_{1,2} & \cdots & x^{0}_{1,m}  \\
x^{0}_{2,1} & x^{0}_{2,2} & \cdots & x^{0}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{0}_{n,1} & x^{0}_{n,2} & \cdots & x^{0}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{1}_{1,1} & b^{1}_{1,2} & \cdots & b^{1}_{1,m}  \\
b^{1}_{2,1} & b^{1}_{2,2} & \cdots & b^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
b^{1}_{n,1} & b^{1}_{n,2} & \cdots & b^{1}_{n,m} \\
\end{bmatrix}

\\

= 

\begin{bmatrix}
a^{1}_{1,1} & a^{1}_{1,2} & \cdots & a^{1}_{1,m}  \\
a^{1}_{2,1} & a^{1}_{2,2} & \cdots & a^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
a^{1}_{n,1} & a^{1}_{n,2} & \cdots & a^{1}_{n,m} \\
\end{bmatrix}

=
a^1
$$


维度检查：$(n \times n) (n \times m) \rightarrow (n \times m)$

> $m$ 个$n$ 维向量，每一个$n$ 维向量都被变换了$n$ 次
>
> > 或着说$m$ 个神经网络的输入，每一个输入有$n$ 种属性（例如影响房子价格的属性有面积、朝向、楼层等）
>
> $n$ 次线性变换结果构成新的$n$ 维向量



$(n \times m) \rightarrow (n \times m)$ 的映射可以看成由$n * m$ 个$(n \times 1) \rightarrow (1 \times 1)$ 函数组成


$$
a^1_{i,j} = w^1_{i,*} \cdot x^0_{*,j} = w^1_{i,1}  x^0_{1,j} + w^1_{i,2}  x^0_{2,j} + \cdots + w^1_{i,n}  x^0_{n,j} + b^1_{i,j}
$$

> 下标 $i,*$  ，表示这是一个行向量，$i$ 指明了它是矩阵的第几行
>
> 下标 $*,j$  ，表示这是一个列向量，$j$ 指明了它是矩阵的第几列





训练时则将$x$ 视为常量，将 $w$ 和$b$ 视为变量

> 目标是出损失值$\mathcal{L}$ 对权重$w$ 的偏导，这个偏导会指导我们如何调整$w$ 的值从而减小损失值$\mathcal{L}$

预测时将 $w$和$b$ 视为常量，将 $x$ 视为变量



$(n \times 1) \rightarrow (1 \times 1)$ 仿射函数的梯度

> 训练阶段这个仿射函数的自变量是$w$ 和$b$
>
> 这是一个$n$ 元函数，所以有$n$ 个偏导数
>
> $n$ 个偏导数组成一个列向量，这就是梯度




$$
\nabla_{w} a^1_{i,j} = \begin{pmatrix} 
\frac{\partial a^1_{i,j}}{w^1_{i,1}}  \\
\vdots \\
\frac{\partial a^1_{i,j}}{w^1_{i,n}}  \\
\end{pmatrix} 
= \begin{pmatrix}
x^0_{1,j} \\
\vdots \\
x^0_{n,j} \\
\end{pmatrix}
$$


$$
\nabla_{b} a^1_{i,j} =  
\frac{\partial a^1_{i,j}}{\partial b^1_{i,j}} 
= 1
$$



激活



第一层某个权重影响第一层映射，


$$
i \in 1 \cdots n \\ 
j \in 1 \cdots m \\
s \in 1 \cdots n \\
$$

$$
\frac{\partial a^1_{i,j}}{w^1_{i,s}} = x^0_{s,j} \\
$$






$$
\cdot \frac{\partial a^1_{i,j}}{w^1_{i,s}}
$$




$(n \times m) \rightarrow (n \times m)$ 映射可以看成由$n * m$ 个$(1 \times 1) \rightarrow (1 \times 1)$ 的函数组成 


$$
a^1
=
\begin{bmatrix}
a^{1}_{1,1} & a^{1}_{1,2} & \cdots & a^{1}_{1,m}  \\
a^{1}_{2,1} & a^{1}_{2,2} & \cdots & a^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
a^{1}_{n,1} & a^{1}_{n,2} & \cdots & a^{1}_{n,m} \\
\end{bmatrix}
$$

$$
f(a^1) = x^1 =  \begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}
$$

$$
f(a^1_{i,j}) = x^1_{i,j}
$$

$$
\nabla_{a} x^1_{i,j} = \frac{\partial x^1_{i,j}}{\partial a^1_{i,j}}
$$


映射


$$
g(w^2, b^2) = w^2  \cdot x^1 + b^2 = a^2
$$

$$
\begin{bmatrix}
w^{2}_{1,1} & w^{2}_{1,2} & \cdots & w^{2}_{1,n}  \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{2}_{1,1} & b^{2}_{1,2} & \cdots & b^{2}_{1,m}  \\
\end{bmatrix}
\\
= 
\begin{bmatrix}
a^{2}_{1,1} & a^{2}_{1,2} & \cdots & a^{2}_{1,m}  \\
\end{bmatrix}
$$



激活
$$
f(a^2) = x^2 = \begin{bmatrix}
x^{2}_{1,1} & x^{2}_{1,2} & \cdots & x^{2}_{1,m}  \\
\end{bmatrix}
$$


损失值（均方误差）
$$
\mathcal{L} = h(x^2, y) =  \frac{1}{2m} \sum^{m}_{i=1}(x^2_{1,i} - y^{i})^2
$$



$$
\frac{\partial \mathcal{L}}{\partial {w^1_{i,j}}} = \frac{\partial \mathcal{L}}{\partial {x^2_{1,j}}}
$$









