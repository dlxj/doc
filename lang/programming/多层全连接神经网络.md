

# 多层全连接神经网络



上标表示层

总共有$K$ 层，第$0$ 层是输入层，第$K$ 层是输出层

第$1$ 到$K-1$ 层是隐层





$x^0_i$ 第$0$ 层输入向量的第$i$ 分量

$n_0$ 第$0$ 层总共有$n_0$ 个分量

$a^k_i$ 第$k$ 层第$i$ 个神经元的仿射值

$x^k_i$ 第$k$ 层第$i$ 个神经元的激活值

第$k$ 层有$n_k$ 个神经元，所以该层有$n_k$ 个输出





## 前向传播



$f$ 激活映射，$g$ 仿射映射，$h$损失映射

> 仿射映射可以看作由若干个仿射函数组成
>
> 仿射函数是一个线性函数加上一个常量





第零层（输入层）输入值


$$
X^0 = \begin{bmatrix}
x^{0}_{1,1} & x^{0}_{1,2} & \cdots & x^{0}_{1,m}  \\
x^{0}_{2,1} & x^{0}_{2,2} & \cdots & x^{0}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{0}_{n,1} & x^{0}_{n,2} & \cdots & x^{0}_{n,m} \\
\end{bmatrix}
$$




第一层（隐层）仿射值


$$
g(W^1,X^0,B^1) = W^1 \cdot X^0 + B^1 =

\begin{bmatrix}
w^{1}_{1,1} & w^{1}_{1,2} & \cdots & w^{1}_{1,n}  \\
w^{1}_{2,1} & w^{1}_{2,2} & \cdots & w^{1}_{2,n} \\
\vdots & \vdots & \ddots & \vdots & \\
w^{1}_{n,1} & w^{1}_{n,2} & \cdots & w^{1}_{n,n} \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{0}_{1,1} & x^{0}_{1,2} & \cdots & x^{0}_{1,m}  \\
x^{0}_{2,1} & x^{0}_{2,2} & \cdots & x^{0}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{0}_{n,1} & x^{0}_{n,2} & \cdots & x^{0}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{1}_{1,1} & b^{1}_{1,2} & \cdots & b^{1}_{1,m}  \\
b^{1}_{2,1} & b^{1}_{2,2} & \cdots & b^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
b^{1}_{n,1} & b^{1}_{n,2} & \cdots & b^{1}_{n,m} \\
\end{bmatrix}

\\

= 

\begin{bmatrix}
a^{1}_{1,1} & a^{1}_{1,2} & \cdots & a^{1}_{1,m}  \\
a^{1}_{2,1} & a^{1}_{2,2} & \cdots & a^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
a^{1}_{n,1} & a^{1}_{n,2} & \cdots & a^{1}_{n,m} \\
\end{bmatrix}
$$

维度检查：$(n \times n) (n \times m) \rightarrow (n \times m)$





第一层（隐层）激活值
$$
f( g(W^1,X^0,B^1) ) = X^1 = \begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}
$$




第二层（输出层）仿射值


$$
g(W^2,X^1,B^2) = W^2 \cdot X^1 + B^2 =

\begin{bmatrix}
w^{2}_{1,1} & w^{2}_{1,2} & \cdots & w^{2}_{1,n}  \\
\end{bmatrix}

\cdot 

\begin{bmatrix}
x^{1}_{1,1} & x^{1}_{1,2} & \cdots & x^{1}_{1,m}  \\
x^{1}_{2,1} & x^{1}_{2,2} & \cdots & x^{1}_{2,m} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{1}_{n,1} & x^{1}_{n,2} & \cdots & x^{1}_{n,m} \\
\end{bmatrix}

+ 

\begin{bmatrix}
b^{2}_{1,1} & b^{2}_{1,2} & \cdots & b^{2}_{1,m}  \\
\end{bmatrix}
\\
= 
\begin{bmatrix}
a^{2}_{1,1} & a^{2}_{1,2} & \cdots & a^{2}_{1,m}  \\
\end{bmatrix}
$$


维度检查：$(1 \times n) (n \times m) \rightarrow (1 \times m)$

 



第二层（输出层）激活值（输出值）


$$
f(g(W^2,X^1,B^2)) = X^2 = \begin{bmatrix}
x^{2}_{1,1} & x^{2}_{1,2} & \cdots & x^{2}_{1,m}  \\

\end{bmatrix}
$$




损失值（均方误差）
$$
\mathcal{L} = h(X^2, Y) =  \frac{1}{2m} \sum^{m}_{i=1}(x^2_{1,i} - y^{i})^2
$$




## 反向传播



若将损失值$\mathcal{L}$ 视为某个激活值的函数，则它是若干个映射的复合

> 将这个激活值视为$\mathcal{L}$ 的唯一变量，其它所有量都认为是常量
>
> 这是为了求偏导



例如第一层的某个激活值：


$$
\mathcal{L}(x^1_{i,j}) = h( f ( g( x^1_{i,j} ) ) )
$$


根据链式法则，$\mathcal{L}$ 对$x^k_{i,j}$ （这里的$k = 1$）的雅可比（导数）是这三个映射在相应位置的雅可比之积：


$$
\frac{\partial \mathcal{L}}{\partial x^k_{i,j}} = A_h \cdot A_f \cdot A_g
$$

$$
A_h = \begin{pmatrix}
\frac{\partial \mathcal{L}}{\partial x^{k+1}_{1,1}} & \cdots & \frac{\partial \mathcal{L}}{\partial x^{k+1}_{1,m}}
\end{pmatrix}
$$










