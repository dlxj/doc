

最小熵原理（一）：无监督学习的原理

https://kexue.fm/archives/5448





信息熵越大，信息量到底是越大还是越小？

https://www.zhihu.com/question/274997106



**熵越大信息量越大**

- **一方面熵越大混乱程度越大，另一方面信息熵越大信息量越大**



热力学熵和信息熵就是同一个东西，所以先理清熵的概念：



如果记录没有随机性的东西就没有信息量

不要把信息熵理解成信息本身了

熵本身不是对信息量的一个度量，而是对事件不确定性的一个度量，而信息量又是对熵减少的一个度量





在信息论中，熵（英語：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、平均自信息量。



**某个汉字的信息量**
$$
I(p_c) = -log\ p_c
$$
如果认为中文的基本单位是字，那么中文就是字的组合，$p_c$就意味着对应字的概率，而$-log\ p_c$就是该字的信息量。





**汉字的平均信息量**
$$
H_c = - \sum_{c \in 汉字集} p_c \ log \ p_c
$$

- 平均信息量，又被称为**信息熵**、平均自信息量。



通过大量的语料统计后，每个汉字的平均信息量大约是 9.65比特，每个英文字母的平均信息量约为 4.03比特。这个数字意味着什么呢？一般可以认为，我们接收或记忆信息的速度是固定的，那么这个信息量的大小事实上也就相当于我们接收这个信息所需要的时间（或者所花费的精力，等等），从而也可以说**这个数字意味着我们学习这个东西的难度（记忆负荷）**。比如，假设我们每秒只能接收1比特的信息，那么按字来记忆一篇800字文章，就需要9.65×800秒的时间。



“按字来记忆” 代表了一种很机械的记忆方式，而我们事实上不是这样做的。



**分词相当于降低了信息量**



像念经一样逐字背诵是很困难的，组词理解后就容易些，如果能找到一些语法规律，就更加容易记忆和理解了。但是我们接收（记忆）信息的速度还是固定的，这也就意味着分词、语法这些步骤，降低了语言的信息量，从而降低了我们的学习成本！



你记住了一个词组，或是一个固定搭配（例如成语），在背诵的时侯就可以利用你的现有知识成功预测后面将要出现的字/词或搭配。事实上这些字/词或搭配的出现概率在原有基础上大大增加了，所以它们的信息量减少了。信息量减少意味着记忆负荷减小。









信息熵正比于信息量
$$
I(c) \sim -log\ p_c
$$
log 是以2 为底的对数则信息量的单位是比特





























