

# 【吴恩达深度学习】深度神经网络的反向传播推导



## 前言：

继吴恩达机器学习公开课之后，吴恩达大佬又和网易合作，免费开放了深度学习的课程，感谢大佬，感谢网易，同样感谢提供配套课程讲义的黄海广博士。

本文主要是个人的学习总结，原内容源于视频第一门课：神经网络和深度学习—第三周，文章内容同步发布在[语雀文档](https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/3bb85013-03d4-4dfe-8c42-8dc07eeddca7)上。

**课程资源**： [网易云课堂课程资源](https://link.zhihu.com/?target=https%3A//mooc.study.163.com/smartSpec/detail/1001319001.htm) [课程配套讲义](https://link.zhihu.com/?target=https%3A//github.com/fengdu78/deeplearning_ai_books)

------

## **吴恩达深度学习1—深度神经网络的反向传播推导**

神经网络的正向传播和反向传播梯度下降的推导过程其实在：[【吴恩达机器学习】第五周—神经网络反向传播算法](https://zhuanlan.zhihu.com/p/74167352) 已经推导过了，不过之前的例子不够通用，重点在于解释反向传播的过程。这次我们进行更通用点的推导。

**首先，我们总结一下单个神经网络和简单神经网络的向量化表示，然后对一个2层神经网络进行前向传播 + 反向传播的公式推导；最后总结出一个更一般的神经网络反向传播推导公式。**

## **1. 神经网络的向量化表示**

**1.1 单个神经元：**



![img](https://pic3.zhimg.com/80/v2-57abdda168cff19006e765c6bf3fa442_720w.jpg)



![[公式]](https://www.zhihu.com/equation?tex=x) 为输入层输出变量矩阵； 我们用 ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) 来表示神经网络第l层的所有神经元激活后的结果矩阵。根据定义， ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5BL%5D%7D) 即表示输出层的输出结果。

这里： ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B0%5D%7D+%3D+x+%3D++%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_1+%5C%5C+x_2+%5C%5C+x_3+++%5Cend%7Bmatrix%7D+%5Cright%5D) ***，\*** ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Cleft%5B++%5Cbegin%7Bmatrix%7D++a%5E%7B%5B1%5D%7D_1+%5C%5C+++%5Cend%7Bmatrix%7D+%5Cright%5D) ，***w\*** 是权重矩阵，因为***w\***为列向量，故在计算 ![[公式]](https://www.zhihu.com/equation?tex=z+%3D+w%5ETx+%2B+b) 时，用的是其转置。



**1.2 简单神经网络**

![img](https://pic2.zhimg.com/80/v2-2b7880c996cbd65add34964504bd1235_720w.jpg)

我们用 ***L\*** 表示层数，上图：***L\*** = 2表示神经网络层数为2，输入层 ![[公式]](https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D%3D+n_x+%3D+3)

有三个变量，同时 ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B0%5D%7D+%3D+x+%3D++%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_1+%5C%5C+x_2+%5C%5C+x_3+++%5Cend%7Bmatrix%7D+%5Cright%5D) 中间的隐藏层有4个神经元 ![[公式]](https://www.zhihu.com/equation?tex=n%5E%7B%5B1%5D%7D+%3D+4) ，隐藏层激活后的结果矩阵 ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Cleft%5B++%5Cbegin%7Bmatrix%7D++a%5E%7B%5B1%5D%7D_1+%5C%5C++a%5E%7B%5B1%5D%7D_2+%5C%5C++a%5E%7B%5B1%5D%7D_3+%5C%5C++a%5E%7B%5B1%5D%7D_4+%5C%5C+++++%5Cend%7Bmatrix%7D+++%5Cright%5D) ；输出层有1个神经元 ![[公式]](https://www.zhihu.com/equation?tex=n%5E%7B%5B2%5D%7D+%3D+1)

**1.2.1单样本向量化表示**

用方程式表示隐藏层的z和激活结果a，如上图右边的方程式，看起来很直观，不过实际计算时，不可能一个个地根据上述方程来计算，而是用向量化地方式表示。

![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7Dx+%2B+b%5E%7B%5B1%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B1%5D%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7Dx+%2B+b%5E%7B%5B2%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B2%5D%7D%29)

**1.2.2 多样本向量化表示**

1.2.1中的公式适用于单个样本，当我们有多个样本(mini-batch)时，假设有m批样本，则我们就需要从i = 1~m，重复计算这四个过程。实际情况没那么复杂，我们可以直接用多样本向量化的公式表示如下：

![img](https://pic2.zhimg.com/80/v2-93a5aeb1ab6e9db18c2188619723fb7d_720w.jpg)



![img](https://pic2.zhimg.com/80/v2-1af3fd821d07b6d7c90e6a5fd2b63e45_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-d694074ad2ca032cf6bc2f61e11f1c41_720w.jpg)

**注：此处 ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%28i%29%7D) 中[1]表示神经网络第一层,\*(i)\*表示第\*i\* 批样本**

![img](https://pic3.zhimg.com/80/v2-723c73cb3f0f9f1614fc9fc931f6850a_720w.jpg)

## **2.简单神经网络的推导**

**损失函数Cost function公式：** ![[公式]](https://www.zhihu.com/equation?tex=J%28W%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2CW%5E%7B%5B2%5D%7D%2Cb%5E%7B%5B2%5D%7D%29+%3D+%5Cfrac%7B1%7D%7Bm%7D+%5CSigma%5Em_%7Bi%3D1%7DL%28%5Cwidehat%7By%7D%2Cy%29)

![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+W%5E%7B%5B1%5D%7D%7D%2C+db%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+b%5E%7B%5B1%5D%7D%7D)

![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D+%3A%3D+W%5E%7B%5B1%5D%7D+-%5Calpha+dW%5E%7B%5B1%5D%7D+%2C+b%5E%7B%5B1%5D%7D+%3A%3D+b%5E%7B%5B1%5D%7D+-%5Calpha+db%5E%7B%5B1%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D%2C+db%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+b%5E%7B%5B2%5D%7D%7D)



**正向传播forward propagation：**

![[公式]](https://www.zhihu.com/equation?tex=Z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7Dx+%2B+b%5E%7B%5B1%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B1%5D%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=Z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D+%2B+b%5E%7B%5B2%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5B2%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B2%5D%7D%29)



**反向传播back propagation:**

![[公式]](https://www.zhihu.com/equation?tex=dZ%5E%7B%5B2%5D%7D+%3D+A%5E%7B%5B2%5D%7D-Y%2CY%3D%5By%5E%7B%5B1%5D%7D+y%5E%7B%5B2%5D%7D...y%5E%7B%5Bm%5D%7D%5D)

![[公式]](https://www.zhihu.com/equation?tex=+Y%E8%A1%A8%E7%A4%BA%E4%BA%86m%E4%B8%AA%E6%A0%B7%E6%9C%AC%EF%BC%8Cy%5E%7B%28i%29%7D%E8%A1%A8%E7%A4%BA%E7%AC%ACi%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%A2%84%E6%B5%8B%E5%80%BC%E7%9F%A9%E9%98%B5)

![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+dZ%5E%7B%5B2%5D%7D+A%5E%7B%5B1%5DT%7D)

![[公式]](https://www.zhihu.com/equation?tex=db%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7Dnp.sum%28dZ%5E%7B%5B2%5D%7D%2C+axis%3D1%2C+keepdims+%3D+True%29)

![[公式]](https://www.zhihu.com/equation?tex=dZ%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B2%5DT%7DdZ%5E%7B%5B2%5D%7D+%2A+g%27%5E%7B%5B1%5D%7D%28Z%5E%7B%5B1%5D%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+dZ%5E%7B%5B1%5D%7D+X%5E%7BT%7D)

![[公式]](https://www.zhihu.com/equation?tex=db%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7Dnp.sum%28dZ%5E%7B%5B1%5D%7D%2C+axis%3D1%2C+keepdims+%3D+True%29)



**dW的公式推导：**

![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D+%3D++%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+Z%5E%7B%5B2%5D%7D%7D+%2A+%5Cfrac%7B%5Cvartheta+Z%5E%7B%5B2%5D%7D%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D+%3D+dZ%5E%7B%5B2%5D%7D%2A%5Cfrac%7B%5Cvartheta+%28W%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D%2Bb%29%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D+%3D++dZ%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7DdZ%5E%7B%5B2%5D%7DA%5E%7B%5B1%5DT%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%E5%92%8CA%5E%7B%5B1%5DT%7D)

1.这里需要注意的是，我们会repeaat i次（i从1~m），每次都会计算dW，推导公式中的

![[公式]](https://www.zhihu.com/equation?tex=%5Cvartheta+J) 表示第i次过程汇总的损失。故 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+Z%5E%7B%5B2%5D%7D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7DdZ%5E%7B%5B2%5D%7D)

\2. ![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D) 是pxq矩阵，其中p表示第二层神经元数量、q表示第一层神经元数量； ![[公式]](https://www.zhihu.com/equation?tex=dZ%5E%7B%5B2%5D%7D)

是px1矩阵； ![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D) 是qx1矩阵，为了保持队形，故将其转置。



## **3.神经网络反向传播推导**

**3.1前向传播**

![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D.a%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D) ，b为偏差

**第\*l\* 层的结果矩阵:**

![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D+%3D+g%5E%7B%5Bl%5D%7D%28z%5E%7B%5Bl%5D%7D%29) ，g(z)为激活函数

**向量化地表示：**

![[公式]](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D.A%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D+%3D+g%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29)

我们从***l = 0\*** 开始，即 ![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5B0%5D%7D+%3D+X) 来表示输入层矩阵，开始计算 ![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7D.A%5E%7B%5B0%5D%7D+%2B+b%5E%7B%5B1%5D%7D) 重复此过程直到最后的输出层：![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%5BL%5D%7D+%3D+W%5E%7B%5BL%5D%7D.A%5E%7B%5BL-1%5D%7D+%2B+b%5E%7B%5BL%5D%7D%2CA%5E%7B%5BL%5D%7D+%3D+g%5E%7B%5BL%5D%7D%28Z%5E%7B%5BL%5D%7D%29) ，便完成了整个前向传播的推导



**3.2反向传播**

完成前向传播的推导后，我们知道了神经网络每一层的激活值，于是可以根据激活值从后往前逐层推导每一层的偏导。

**向量化表示的步骤如下：**

![[公式]](https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D+%3D+dA%5E%7B%5Bl%5D%7D%2A%5Bg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29%5D%27)

![[公式]](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7DdZ%5E%7B%5Bl%5D%7D.A%5E%7B%5Bl-1%5DT%7D)

![[公式]](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7Dnp.sum%28dZ%5E%7B%5Bl%5D%7D%2C+axis+%3D+1+%2Ckeepdims+%3D+True%29)

![[公式]](https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl-1%5D%7D+%3D+W%5E%7B%5Bl%5DT%7D.dZ%5E%7B%5Bl%5D%7D)