{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](https://www.haskell.org/onlinereport)\n",
    "[](http://penkovsky.com/neural-networks/day1)\n",
    "[](https://gist.github.com/masterdezign/34ab610715df7dbf504cbe7cacdba68e)\n",
    "[](https://crypto.stanford.edu/~blynn/play/tictactoe.html)\n",
    "[](https://castel.dev/post/lecture-notes-1)\n",
    "[](https://github.com/gibiansky/IHaskell)\n",
    "[](https://hoogle.haskell.org/)\n",
    "[](http://hackage.haskell.org/package/hmatrix)\n",
    "[](http://dis.um.es/~alberto/material/hmatrix.pdf)\n",
    "[](https://blog.csdn.net/linxilinxilinxi/article/details/84026890)\n",
    "[](https://www.math.ubc.ca/~pwalls/math-python/jupyter/latex)\n",
    "[](https://castel.dev/post/lecture-notes-2/)\n",
    "[](https://bonxg.com/p/85.html)\n",
    "[](https://blog.csdn.net/qq_29695701/article/details/86304782)\n",
    "[](https://blog.csdn.net/wcs_152/article/details/81182669)\n",
    "[](https://www.jianshu.com/p/57523e147480)\n",
    "[](http://www.zvon.org/other/haskell/Outputprelude/recip_f.html)\n",
    "[](https://www.haskell.org/onlinereport/standard-prelude.html)\n",
    "\n",
    "**神经网络的出现是为了自动从数据中学习出感知机 A(perceptron)  的合适权重参数**\n",
    "\n",
    "\n",
    "\n",
    "**激活函数(activation function)**  \n",
    "\n",
    "- **将输入信号的总和转换为输出信号**\n",
    "\n",
    "\n",
    "\n",
    "**激活函数是连接感知机和神经网络的桥梁**\n",
    "\n",
    "- 一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。“多层感知机”是指神经网络，即使用 sigmoid  函数(后述)等平滑的激活函数的多层网络。\n",
    "- 实际上，**如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了**\n",
    "\n",
    "> 阶跃函数是指一旦输入超过阈值，就切换输出的函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sigmoid 函数**\n",
    "\n",
    "```\n",
    "$$\\frac{分子}{分母}$$\n",
    "\n",
    "```\n",
    "\n",
    "$$h(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "logistic sigmoid 函数通常用来产生Bernoulli 分布中的参数 φ，因为它的范围是(0, 1)，处在φ 的有效取值范围内。图 3.3 给出了sigmoid 函数的图示。sigmoid 函数在**变量取绝对值非常大**的正值或负值时会出现**饱和(saturate)现象**，意味着**函数会变得很平**，并且对**输入的微小改变会变得不敏感**  \n",
    "《Deep Learning book》\n",
    "\n",
    "> Mathematica\n",
    "```\n",
    "h[x_]:=1/(1+E^(-x))\n",
    "h[1]//N  (*数值表式*)\n",
    "Plot[h[x],{x,-6,6}]\n",
    "```\n",
    "> Haskell\n",
    "```\n",
    "sigmoid = cmap f\n",
    "  where\n",
    "    f x = recip $ 1.0 + exp (-x)\n",
    "stack runghc stack_runghc.hs  {- 不是stack下运行就找不到Numeric.LinearAlgebra -}  \n",
    "```\n",
    "\n",
    "[](http://hackage.haskell.org/package/hmatrix)\n",
    "> Numeric.LinearAlgebra.Data  \n",
    "```-- | like 'fmap' (cannot implement instance Functor because of Element class constraint)\n",
    "cmap :: (Element b, Container c e) => (e -> b) -> c e -> c b\n",
    "cmap = cmap'\n",
    "cmap'        :: (Element b) => (e -> b) -> c e -> c b\n",
    "cmap' f = liftMatrix (mapVector f)\n",
    "```\n",
    "\n",
    "[](https://www.haskell.org/onlinereport/standard-prelude.html)\n",
    "```\n",
    "class  Functor f  where\n",
    "    fmap              :: (a -> b) -> f a -> f b\n",
    "\n",
    "\n",
    "\n",
    "运算符优先级的定义\n",
    "infixr 9  .\n",
    "infixr 8  ^, ^^, **\n",
    "infixl 7  *, /, `quot`, `rem`, `div`, `mod`\n",
    "infixl 6  +, -\n",
    "\n",
    "-- The (:) operator is built-in syntax, and cannot legally be given\n",
    "-- a fixity declaration; but its fixity is given by:\n",
    "--   infixr 5  :\n",
    "\n",
    "infix  4  ==, /=, <, <=, >=, >\n",
    "infixr 3  &&\n",
    "infixr 2  ||\n",
    "infixl 1  >>, >>=\n",
    "infixr 1  =<<\n",
    "infixr 0  $, $!, `seq`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[haskell神经网络实现](https://gist.github.com/masterdezign/34ab610715df7dbf504cbe7cacdba68e)  \n",
    "[hmatrix文档](https://hackage.haskell.org/package/hmatrix-0.20.0.0/docs/doc-index.html)  \n",
    "[标准库文档](https://www.haskell.org/onlinereport/standard-prelude.html)  \n",
    "[kaggle云计算](https://www.kaggle.com/masterdezign/iris-with-onehotencoded-targets)  \n",
    "[调试](https://www.jianshu.com/p/57523e147480)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Initial loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- import Numeric.LinearAlgebra as LA {- stack exec jupyter -- notebook -}\n",
    "-- stack  runghc --package hmatrix Iris.hs  \n",
    "import Numeric.LinearAlgebra.Data(cmap, loadMatrix)\n",
    "{-\n",
    "sigmoid = cmap f\n",
    "  where\n",
    "    f x = recip $ 1.0 + exp (-x)\n",
    "sigmoid 1\n",
    "-}\n",
    "recip 2 -- 倒数\n",
    "exp (-1)   -- e 的-1 次方\n",
    "recip $ 1.0 + exp (-1)\n",
    "\n",
    "putStrLn \"Initial loss \"\n",
    "dta <- loadMatrix \"x.dat\"\n",
    "tgt <- loadMatrix \"y.dat\"\n",
    "  -- dta <- loadMatrix \"x.dat\"\n",
    "  -- tgt <- loadMatrix \"y.dat\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTex 数学公式\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "$$\\int_a^b f(x) = F(b) - F(a)$$\n",
    "```\n",
    "$$\\int_a^b f(x) = F(b) - F(a)$$\n",
    "---\n",
    "\n",
    "\n",
    "```\n",
    "$$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x-a}$$\n",
    "```\n",
    "$$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x-a}$$\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
