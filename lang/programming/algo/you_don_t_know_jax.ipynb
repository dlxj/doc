{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "you-don-t-know-jax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkVW_bEGaV9q",
        "outputId": "ccd22959-1fb1-429b-a4b0-9ae73e5cd53f"
      },
      "source": [
        "\n",
        "# https://github.com/craffel/jax-tutorial/blob/master/you-don-t-know-jax.ipynb\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "# Current convention is to import original numpy as \"onp\"\n",
        "import numpy as onp\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "def ReLU(x):\n",
        "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def stable_softmax(X):\n",
        "    exps = np.exp(X - np.max(X))\n",
        "    return exps / np.sum(exps)\n",
        "\n",
        "# Sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Computes our network's output\n",
        "def net(params, x):\n",
        "    w1, b1, w2, b2 = params\n",
        "    #hidden = np.tanh(np.dot(w1, x) + b1)\n",
        "    #hidden = ReLU(np.dot(w1, x) + b1)\n",
        "    #hidden = sigmoid(np.dot(w1, x) + b1)\n",
        "    #hidden = jax.nn.softplus(np.dot(w1, x) + b1)\n",
        "    #hidden = jax.nn.leaky_relu(np.dot(w1, x) + b1)\n",
        "\n",
        "    hidden = jax.nn.selu(np.dot(w1, x) + b1)\n",
        "    \n",
        "    \n",
        "    #out = ReLU(np.dot(w2, hidden) + b2)\n",
        "    return sigmoid(np.dot(w2, hidden) + b2)\n",
        "\n",
        "    #return sigmoid(out)\n",
        "\n",
        "# Cross-entropy loss\n",
        "def loss(params, x, y):\n",
        "    out = net(params, x)\n",
        "    cross_entropy = -y * np.log(out) - (1 - y)*np.log(1 - out)\n",
        "    return cross_entropy\n",
        "\n",
        "# Utility function for testing whether the net produces the correct\n",
        "# output for all possible inputs\n",
        "def test_all_inputs(inputs, params):\n",
        "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
        "    for inp, out in zip(inputs, predictions):\n",
        "        print(inp, '->', out)\n",
        "    return (predictions == [onp.bitwise_xor(*inp) for inp in inputs])\n",
        "\n",
        "\n",
        "\n",
        "def initial_params():\n",
        "    return [\n",
        "        onp.random.randn(3, 2),  # w1\n",
        "        onp.random.randn(3),  # b1\n",
        "        onp.random.randn(3),  # w2\n",
        "        onp.random.randn(),  #b2\n",
        "    ]\n",
        "\n",
        "loss_grad = jax.grad(loss)\n",
        "\n",
        "# Stochastic gradient descent learning rate\n",
        "learning_rate = 0.1\n",
        "# All possible inputs\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "# Initialize parameters randomly\n",
        "params = initial_params()\n",
        "\n",
        "for n in itertools.count():\n",
        "    # Grab a single random input\n",
        "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
        "    # Compute the target output\n",
        "    y = onp.bitwise_xor(*x)\n",
        "    # Get the gradient of the loss for this input/output pair\n",
        "    grads = loss_grad(params, x, y)\n",
        "    # Update parameters via gradient descent\n",
        "    params = [param - learning_rate * grad\n",
        "              for param, grad in zip(params, grads)]\n",
        "    # Every 100 iterations, check whether we've solved XOR\n",
        "    if not n % 100:\n",
        "        print('Iteration {}'.format(n))\n",
        "        if test_all_inputs(inputs, params):\n",
        "            break\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "[0 0] -> 1\n",
            "[0 1] -> 1\n",
            "[1 0] -> 1\n",
            "[1 1] -> 1\n",
            "Iteration 100\n",
            "[0 0] -> 1\n",
            "[0 1] -> 0\n",
            "[1 0] -> 0\n",
            "[1 1] -> 0\n",
            "Iteration 200\n",
            "[0 0] -> 1\n",
            "[0 1] -> 1\n",
            "[1 0] -> 1\n",
            "[1 1] -> 1\n",
            "Iteration 300\n",
            "[0 0] -> 1\n",
            "[0 1] -> 1\n",
            "[1 0] -> 1\n",
            "[1 1] -> 0\n",
            "Iteration 400\n",
            "[0 0] -> 1\n",
            "[0 1] -> 0\n",
            "[1 0] -> 1\n",
            "[1 1] -> 0\n",
            "Iteration 500\n",
            "[0 0] -> 1\n",
            "[0 1] -> 1\n",
            "[1 0] -> 1\n",
            "[1 1] -> 0\n",
            "Iteration 600\n",
            "[0 0] -> 0\n",
            "[0 1] -> 0\n",
            "[1 0] -> 1\n",
            "[1 1] -> 0\n",
            "Iteration 700\n",
            "[0 0] -> 0\n",
            "[0 1] -> 1\n",
            "[1 0] -> 1\n",
            "[1 1] -> 0\n"
          ]
        }
      ]
    }
  ]
}