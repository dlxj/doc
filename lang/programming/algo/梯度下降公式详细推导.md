[TOC]

李宏毅2020机器学习深度学习(完整版)国语

https://www.bilibili.com/video/BV1JE411g7XF?from=search&seid=15726120516517372277

https://github.com/Sakura-gh/ML-notes



详解 误差反向传播算法推导

https://blog.csdn.net/ViatorSun/article/details/82696475

- step(阶越函数)，只能输出0或1，不连续所以不可导
- 梯度下降法需要求解相邻层的梯度，这就要求网络中需要处处可导



深度学习中常用激活函数：sigmod、tanh、ReLU、ELU、PReLU

https://blog.csdn.net/ViatorSun/article/details/82418578



反向传播

https://blog.csdn.net/weixin_44406200/article/details/104310991



# 梯度下降公式详细推导





## 序言



本文将从最基本的微分法则开始，然后逐步深入梯度下降公式，不漏掉每一处细节。最后给出一个应用实例：“与门(OR Gate)的单层神经网络实现”，并附上完整的Python 代码 





## 预备知识



### 一般求导法则(Elementary rules of differentiation)



$$
\begin{align}
\frac{d \ e^x}{d x} &= \lim \limits_{\Delta x \rightarrow 0} \frac{e^x - e^{x - \Delta x}}{\Delta x} \\
&= e^x \lim \limits_{\Delta x \rightarrow 0} \frac{1 - e^{- \Delta x}}{\Delta x} \\
&= e^x
\end{align}
$$


$$
\begin{align}
g(x) &= \frac{1}{f(x)} \\
\frac{d}{d x} g(x) &= \frac{d}{d x} \frac{1}{f(x)} \\
&= - \frac{ \frac{d}{d x}f(x) }{f(x)^{2}}
\end{align}
$$

$$
\begin{align}
f(x) &= cx^{n} \\
f(x)'&= cn \ast x^{n -1}
\end{align}
$$

$$
\frac{d}{dx} c = 0, \ \frac{d}{dx} x = 1, \\
\frac{d}{dx} [c\cdot f(x)] = c\cdot\frac{df}{dx} \ \ \ \text{(linearity)}, \\
\frac{d}{dx}[f(x)+g(x)] = \frac{df}{dx} + \frac{dg}{dx} \ \ \ \text{(linearity)},  \\
\frac{d}{dx}[f(x)]^2 = 2f(x)\cdot\frac{df}{dx} \ \ \ \text{(chain rule)}.
$$





## 激活函数(Activation Function)



### Sigmoid Function



$$
g(x) = \frac{1}{1 + e^{-x}}
$$



> e 是欧拉数（Euler’s number），近似数值 2.718281，是一个无理数
>
> Sigmoid 意为“S形的”



#### 激活函数求导(Derivation)


$$
\frac{d}{d x} g(x) = \frac{d}{d x} (\frac{1}{1 + e^{-x}}) =  \frac{ - \frac{d}{d x} (1 + e^{-x}) }{(1 + e^{-x})^{2}} \\
= \frac{ - \frac{d}{dx} e^{-x} }{(1 + e^{-x})^{2}} = \frac{ - \frac{d}{dx} \frac{1}{e^{x}} }{(1 + e^{-x})^{2}} = \frac{ - (\frac{- \frac{d}{dx} e^x}{(e^x)^2}) }{(1 + e^{-x})^{2}} = \frac{e^{-x}}{(1 + e^{-x})^2} \\
= (\frac{1}{1+e^{-x}}) (\frac{e^{-x}}{1 + e^{-x}}) \\
= (\frac{1}{1+e^{-x}}) (\frac{1 + e^{-x}}{1 + e^{-x}} -\frac{1}{1 + e^{-x}}) \\
= g(x)(1 - g(x))
$$




## 前向传播(Forward propagation)



上标是行，下标是列。**上标带括号的是行向量**，否则是标量

> 列向量是多维空间的一个点由不同维度的坐标组成的向量。 
>
> 行向量容易让人迷惑，并且不容易描述清楚。
>
> > 在矩阵中它排列成一行的形式，它可以写成列向量的转置，这都只是为了运算需求的表象。
> >
> > 本质上，还是应该把它想像成列向量的形式，把它排成行或进行转置都只是在表明这是一个行向量，这是表象。
> >
> > 列向量张成列空间，它把行空间的向量变换到列空间
> >
> > 行向量张成行空间，它把列空间的向量变换到行空间

矩阵用大写字母表示，矩阵里的向量用小写字母加上下标表示



$$
X = 
\begin{bmatrix}
x^{1}_{0} & x^{1}_{1} & \cdots & x^{1}_{n}  \\
x^{2}_{0} & x^{2}_{1} & \cdots & x^{2}_{n} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{m}_{0} & x^{m}_{1} & \cdots & x^{m}_{n} \\
\end{bmatrix}
$$


$$
W =
\begin{bmatrix}
w_{0}  \\
w_{1}  \\
\vdots \\
w_{n}  \\
\end{bmatrix}
$$


$$
Y =
\begin{bmatrix}
y^{1}  \\
y^{2}  \\
...  \\
y^{m}  \\
\end{bmatrix}
$$


$$
h_{W}(X) =
X \cdot W
=
\begin{bmatrix}
x^{1}_{0} & x^{1}_{1} & \cdots & x^{1}_{n}  \\
x^{2}_{0} & x^{2}_{1} & \cdots & x^{2}_{n} \\
\vdots & \vdots & \ddots & \vdots & \\
x^{m}_{0} & x^{m}_{1} & \cdots & x^{m}_{n} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
w_{0}  \\
w_{1}  \\
\vdots \\
w_{n}  \\
\end{bmatrix}
=
\begin{bmatrix}
w_{0} x^{1}_{0} + w_{1} x^{1}_{1} + \ \cdots \  + w_{n} x^{1}_{n}  \\
w_{0} x^{2}_{0} + w_{1} x^{2}_{1} +\ \cdots \  + w_{n} x^{2}_{n}  \\
\vdots \\
w_{0} x^{m}_{0} + w_{1} x^{m}_{1} + \ \cdots \  + w_{n} x^{m}_{n}  \\
\end{bmatrix}  \\
=
\begin{bmatrix}
h_{W}(x^{^{(1)}}) \\
h_{W}(x^{^{(2)}})  \\
...  \\
h_{W}(x^{^{(m)}}) \\
\end{bmatrix}
$$



$$
g(h_{W}(X)) = 
\begin{bmatrix}
g( h_{W}(x^{^{(1)}}) ) \\
g( h_{W}(x^{^{(2)}}) ) \\
... \\
g( h_{W}(x^{^{(m)}}) ) \\
\end{bmatrix}
$$


$$
E =
h_{w}(X) - Y
=
\begin{bmatrix}
g( h_{W}(x^{^{(1)}}) ) - y^{1} \\
g( h_{W}(x^{^{(2)}}) ) - y^{2} \\
... \\
g( h_{W}(x^{^{(m)}}) ) - y^{m} \\
\end{bmatrix}
=
\begin{bmatrix}
e^{1}  \\
e^{2}  \\
...  \\
e^{m}  \\
\end{bmatrix}
$$




## 均方误差代价函数


$$
J(W) = 
 \frac{1}{2m} \sum^{m}_{i=1}(g(h_{W}(x^{(i)})) - y^{i})^2
$$



We have
$$
h_{W}(x^{i}) = w_{0} x^{(i)}_{0} + w_{1} x^{(i)}_{1} + w_{2} x^{(i)}_{2}
$$
and
$$
J(w_{0},w_{1},w_{2}) = \frac{1}{2m} \sum^{m}_{i=1}(h_{W}(x^{i}) - y^{i})^2
$$
We first compute
$$
\frac{\partial}{\partial w_{0}} h_{W}(x^{i}) = \frac{\partial}{\partial w_{0}}(w_{0} x^{(i)}_{0} + w_{1} x^{(i)}_{1} + w_{2} x^{(i)}_{2}) \\
= \frac{\partial}{\partial w_{0}} w_{0}x^{i}_{0} + 
\frac{\partial}{\partial w_{0}} w_{1}x^{i}_{1} + 
\frac{\partial}{\partial w_{0}} w_{2}x^{i}_{2} \\
= x^{i}_{0} + 0 + 0 = x^{i}_{0}
$$

$$
\frac{\partial}{\partial w_{1}} h_{W}(x^{i}) = \frac{\partial}{\partial w_{1}}(w_{0} x^{(i)}_{0} + w_{1} x^{(i)}_{1} + w_{2} x^{(i)}_{2}) \\
= \frac{\partial}{\partial w_{1}} w_{0}x^{i}_{0} + 
\frac{\partial}{\partial w_{1}} w_{1}x^{i}_{1} + 
\frac{\partial}{\partial w_{1}} w_{2}x^{i}_{2} \\
= 0 + x^{i}_{1} + 0 = x^{i}_{1}
$$

$$
\frac{\partial}{\partial w_{2}} h_{W}(x^{i}) = \frac{\partial}{\partial w_{2}}(w_{0} x^{(i)}_{0} + w_{1} x^{(i)}_{1} + w_{2} x^{(i)}_{2}) \\
= \frac{\partial}{\partial w_{2}} w_{0}x^{i}_{0} + 
\frac{\partial}{\partial w_{2}} w_{1}x^{i}_{1} + 
\frac{\partial}{\partial w_{2}} w_{2}x^{i}_{2} \\
= 0 + 0 + x^{i}_{2} = x^{i}_{2}
$$

so:



for j=0, 1, 2；i = 1,2,3,4:

$$
\frac{\partial}{\partial w_{j}} h_{W}(x^{i}) = x^{i}_{j}
$$







### 梯度

$$
\frac{\partial}{\partial w_{j}} J(W) = 
\frac{\partial}{\partial w_{j}} \bigg [ \frac{1}{2m} \sum^{m}_{i=1}(g(h_{W}(x^{(i)})) - y^{i})^2 \bigg ] \\
= \frac{1}{2m} \sum^{m}_{i=1}\frac{\partial}{\partial w_{j}} (g(h_{W}(x^{(i)})) - y^{i})^2   \quad \text{(by linearity of the derivative)} \\
= \frac{1}{2m} \sum^{m}_{i=1} 

2 \cdot (g(h_{W}(x^{(i)})) - y^{i}) \frac{\partial}{\partial w_{j}} (g(h_{W}(x^{(i)})) - y^{i})   \quad \text{(by chain rule)} \\
= \frac{1}{2m} \cdot 2 \sum^{m}_{i=1} 

(g(h_{W}(x^{(i)})) - y^{i}) \bigg [ \frac{\partial}{\partial w_{j}} g(h_{W}(x^{(i)})) - \frac{\partial}{\partial w_{j}} y^{i} \bigg ]  \quad \text{(by linearity of the derivative)}  \\
= \frac{1}{m} \sum^{m}_{i=1} 

(g(h_{W}(x^{(i)})) - y^{i}) \bigg [ \frac{\partial}{\partial w_{j}} g(h_{W}(x^{(i)})) - 0 \bigg ]   \\

= \frac{1}{m} \sum^{m}_{i=1} 

(g(h_{W}(x^{(i)})) - y^{i}) \frac{\partial}{\partial w_{j}} g(h_{W}(x^{(i)}))     \\
= \frac{1}{m} \sum^{m}_{i=1} 

(g(h_{W}(x^{(i)})) - y^{i}) g(h_{W}(x^{(i)}))(1 - g(h_{W}(x^{(i)}))) x^{i}_{j}
$$

### 参数更新



Lagrange's Mean Value Theorem （拉格朗日中值定理）


$$
f(x + \Delta x) = f(x) + f'(x + \theta \Delta x) \Delta x, 0 < \theta < 1
$$
so:
$$
f(x + \Delta x ) \simeq f(x) + \Delta x \nabla f(x)
$$

> 也可以从泰勒公式的一阶展开推导



从微商的定义推导

> $$
> \Delta x \  是一个微小量 \quad \text{and} \quad \Delta x \rightarrow 0 \\
> 
> \nabla f(x) = \frac{f(x + \Delta x) - f(x)}{ \Delta x } \\
> \Delta x \nabla f(x) = f(x + \Delta x) - f(x) \\
> f(x + \Delta x) = f(x) + \Delta x \nabla f(x)
> $$



我们是想要x 移动一小步后函数值变小，既：
$$
f(x + \Delta x) < f(x)
$$
只需：$\Delta x \nabla f(x) < 0$，令 $ \Delta x = - \alpha \nabla f(x), \quad (\alpha > 0) $


$$
\Delta x \nabla f(x) = - \alpha (\nabla f(x))^{2}
$$


so:
$$
f(x + \Delta x) = f(x - \alpha \Delta f(x)) < f(x)
$$


所以，要让函数变小，x 的更新方法是：
$$
x' \leftarrow x - \alpha \Delta f(x)
$$

$$
w_{j} := w_{j} - \alpha \frac{\partial}{\partial w_{j}} J(W) \\
= w_{j} - \alpha \frac{1}{m} \sum^{m}_{i=1} 

(g(h_{W}(x^{(i)})) - y^{i}) g(h_{W}(x^{(i)}))(1 - g(h_{W}(x^{(i)}))) x^{i}_{j}
$$





## 与门(OR Gate)梯度下降Python 实现



```python

import numpy as np
import itertools

"""
与门的脑补参数实现
    《深度学习入门：基于Python的理论与实现》 p.28
"""
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5]) # 仅权重和偏置与AND不同！
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

"""
与门单层神经网络实现
    神经网络实现的是向量x 到向量 h的仿射变换
    仿射变换相比线性变换多了一个平移，原点变了  
    线性变换保证几何体的形状和比例不变  
    平移是通过加上一个常量既偏置完成的

输入信号矩阵左乘权重矩阵，求出输入信号的加权和，激活函数把若干个加权和压缩到0 和1 之间，这就是隐层的单元
    隐层单元又作为输入信号开始新一轮的计算，最后在输出层得到前向传播的最终结果，这就是预测值
        利用梯度下降或反向传播算法不断的更新权重，从而减小预测值和期望值之间的误差
        当误差小到一定范围时神经网络就训练好了

矩阵中的元素
    上标是行，下标是列

维度
    行向量是多维空间的一个点由不同维度的坐标组成的向量
    列向量是多维空间的多个点的同一维度的坐标组成的向量

网络结构
    输入层2 结点，没有隐层，输出层1 结点

输入矩阵
    X 维度是 4*3 （四组输入，每一组是 1*3）
    输入矩阵要左乘权重矩阵，既X点乘W： X.W
        W 右乘X，X 的列降维到和W 一至，降维的方法是矩阵所有的列加权求和
            左乘是行变换，右乘是列变换
            列向量右乘一个矩阵，左边的矩阵行数没变列数被降维了（被降到和列向量对齐）

权重矩阵
    第一层3 输入对应1 输出，共有三条边，所以需要3 权重
    W 维度是 3*1

输出矩阵
    Y 4*1（四组输入，对应四个输出，四个标量组成的列向量）

梯度下降过程向量化 - Logistic回归总结 洞庭之子
    https://www.cnblogs.com/earendil/p/8268757.html
    doc\lang\programming\梯度下降过程向量化
"""

import numpy as np

# 此激活函数设计为可以接受列向量
# 返回结果也是列向量
# np.exp 的实现可以接受向量作为参数，所以代码很简洁
def sigmoid (x):
    return 1/(1 + np.exp(-x))

m = 4 # 样本数
n = 3 # 列数
alpha = 0.5 # 学习率
maxIter = 50000 # 最大迭代次数

X = np.array([
                [1, 0, 0],
                [1, 0, 1],
                [1, 1, 0],
                [1, 1, 1]
             ], np.float)

# 输入矩阵，维度4*3。注意偏置作为列向量添加进来了

Y = np.array([
                [0],
                [1],
                [1],
                [1]
             ], np.float)
# 4*1 输出

W = np.random.uniform(size=(3, 1))   # 3*1 权重
#W = W * 0.1 # 据说对于sigmoid 激活函数，更小的权重更容易收敛
"""
观察输入输出的维度，可以看出需要对X 的列进行压缩，既把列从维度3 降维到1
    右乘是列变换，应该让权重矩阵W 右乘X，既 X.W
    矩阵点乘的维度变化
        (4*3).(3*1) = 4*1
"""

for k in range(maxIter): 

    A = np.dot(X, W)  # 前向传播
    H = sigmoid(A)    # 预测结果
    E = H - Y         # 误差值

    errs = sum( list(itertools.chain(*abs(E))) )  # 误差总和
    """
    注意这里算的不是均方误差，但是下面的权重更新是用了均方误差函数的导数来算的
    这里的errs 只是用来衡量预测结果有多接近期望结果，以便提前结速迭代
    """
    if errs < 0.05:
        print(f'stop at {k}')
        print("Weight: ")
        print(W)
        break

    """
    权重更新
    """
    for j in range(n):
        s = 0
        for i in range(m):
            s += E[i] * H[i] * (1 - H[i]) * X[i][j]  # W_{j} 的梯度
            W[j] = W[j] - alpha * 1/m * s # 更新权重

    print(f"err sum is: {errs}, curr iter num: {k}")
    print(H)
    
```



## Pytorch 实现



```

"""
OR 的pytorch 实现

reference:
    doc\lang\programming\pytorch\李宏毅2020机器翻译\iAttention.py
    doc\lang\programming\pytorch\数字识别\ihandwritten_digit_recognition_GPU.ipynb
    https://gist.github.com/user01/68514db1127eb007f24d28bfd11dd60e

"""
import torch
import torch.utils.data as torch_data
from torch import nn
from torch import optim

from time import time

def Data(type='training'):
    data = [
        [ torch.Tensor([0, 0]), torch.Tensor([0]) ],
        [ torch.Tensor([0, 1]), torch.Tensor([1]) ],
        [ torch.Tensor([1, 0]), torch.Tensor([1]) ],
        [ torch.Tensor([1, 1]), torch.Tensor([1]) ]
    ]

    return data

class TorchDataset(torch_data.Dataset):
  def __init__(self, data):
    self.data = data
        
  def __len__(self):
    return len(self.data)
  def __getitem__(self, Index):
    item = self.data[Index]

    return item[0], item[1]

def infinite_iter(data_loader):
  it = iter(data_loader)
  while True:
    try:
      item_in , item_out = next(it)
      yield item_in, item_out
    except StopIteration:
      it = iter(data_loader)

# Layer details for the neural network
input_size = 2 # 输入层两个神经元
# hidden_sizes = [128, 64] # 没有隐层
output_size = 1 # 输出一个值

# Build a feed-forward network
model = nn.Sequential(
  nn.Linear(input_size, output_size),
  nn.ReLU())
print(model)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)



train_dataset = TorchDataset(Data(type='training'))
train_loader = torch_data.DataLoader(train_dataset, batch_size = 2, shuffle=True)  # 每个输入是维度是(2) 的一维数组，每一批总共输入2 组，维度既是：(2, 2)
train_iter = infinite_iter(train_loader)

sources, targets = next(train_iter)

print( sources, targets )


optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.05)
criterion = nn.MSELoss() #nn.NLLLoss()  # https://zhuanlan.zhihu.com/p/264366034
  # NLLloss 和交叉熵一样只适用于分类任务， NLLLoss是基于softmax，softmax得到结果向量的概率分布，是离散值。回归任务建议MSE或MAE等损失函数
  # 否则提示多个target报错

time0 = time()
epochs = 50000

# 训练模型
for e in range(epochs):
    running_loss = 0
    sources, targets = next(train_iter)

    #targets = targets.squeeze(1) # 降维
    #targets = targets.to(device=device, dtype=torch.long)
    #targets =torch.tensor(targets, dtype=torch.long) # 类型转换

    # Training pass
    optimizer.zero_grad()
        
    output = model(sources.cpu())
    loss = criterion(output, targets.cpu())
    #This is where the model learns by backpropagating
    loss.backward()
        
    #And optimizes its weights here
    optimizer.step()
        
    running_loss += loss.item()
    
    print("Epoch {} - Training loss: {}".format(e, running_loss/4)) # 4 是总样本数

print("\nTraining Time (in minutes) =",(time()-time0)/60)


# 验证训练出的模型是否正确
sources, targets = next(train_iter)
with torch.no_grad():
    output = model(sources.cpu())
    print( sources, output )

```



```

"""
OR 的pytorch 实现

reference:
    doc\lang\programming\pytorch\李宏毅2020机器翻译\iAttention.py
    doc\lang\programming\pytorch\数字识别\ihandwritten_digit_recognition_GPU.ipynb
    https://gist.github.com/user01/68514db1127eb007f24d28bfd11dd60e

"""
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

EPOCHS_TO_TRAIN = 50000

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(2, 3, True)
        self.fc2 = nn.Linear(3, 1, True)

    def forward(self, x):
        x = F.sigmoid(self.fc1(x))
        x = self.fc2(x)
        return x


net = Net()
inputs = list(map(lambda s: Variable(torch.Tensor([s])), [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
]))
targets = list(map(lambda s: Variable(torch.Tensor([s])), [
    [0],
    [1],
    [1],
    [1]
]))


criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

print("Training loop:")
for idx in range(0, EPOCHS_TO_TRAIN):
    for input, target in zip(inputs, targets):
        optimizer.zero_grad()   # zero the gradient buffers
        output = net(input)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()    # Does the update
    if idx % 5000 == 0:
        print("Epoch {: >8} Loss: {}".format(idx, loss.data.numpy()))



print("")
print("Final results:")
for input, target in zip(inputs, targets):
    output = net(input)
    print("Input:[{},{}] Target:[{}] Predicted:[{}] Error:[{}]".format(
        int(input.data.numpy()[0][0]),
        int(input.data.numpy()[0][1]),
        int(target.data.numpy()[0]),
        round(float(output.data.numpy()[0]), 4),
        round(float(abs(target.data.numpy()[0] - output.data.numpy()[0])), 4)
    ))

```



