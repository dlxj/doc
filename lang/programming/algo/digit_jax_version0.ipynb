{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_jax_version0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwD--Dz0iJfE"
      },
      "source": [
        "\n",
        "# https://roberttlange.github.io/posts/2020/03/blog-post-10/\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as onp\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap, value_and_grad\n",
        "from jax import random\n",
        "\n",
        "# Generate key which is used to generate random numbers\n",
        "key = random.PRNGKey(1)\n",
        "\n",
        "# Import some additional JAX and dataloader helpers\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.experimental import optimizers\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "#from helpers import plot_mnist_examples\n",
        "\n",
        "\n",
        "# Set the PyTorch Data Loader for the training & test set\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "def initialize_mlp(sizes, key):\n",
        "    \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
        "    keys = random.split(key, len(sizes))\n",
        "    # Initialize a single layer with Gaussian weights -  helper function\n",
        "    def initialize_layer(m, n, key, scale=1e-2):\n",
        "        w_key, b_key = random.split(key)\n",
        "        return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
        "    return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "# Return a list of tuples of layer weights\n",
        "params = initialize_mlp(layer_sizes, key)\n",
        "\n",
        "def ReLU(x):\n",
        "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "jit_ReLU = jit(ReLU)\n",
        "\n",
        "def relu_layer(params, x):\n",
        "    \"\"\" Simple ReLu layer for single sample \"\"\"\n",
        "    return ReLU(np.dot(params[0], x) + params[1])\n",
        "\n",
        "def batch_version_relu_layer(params, x):\n",
        "    \"\"\" Error prone batch version \"\"\"\n",
        "    return ReLU(np.dot(X, params[0].T) + params[1])\n",
        "\n",
        "def vmap_relu_layer(params, x):\n",
        "    \"\"\" vmap version of the ReLU layer \"\"\"\n",
        "    return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))\n",
        "\n",
        "def forward_pass(params, in_array):\n",
        "    \"\"\" Compute the forward pass for each example individually \"\"\"\n",
        "    activations = in_array\n",
        "\n",
        "    # Loop over the ReLU hidden layers\n",
        "    for w, b in params[:-1]:\n",
        "        activations = relu_layer([w, b], activations)\n",
        "\n",
        "    # Perform final trafo to logits\n",
        "    final_w, final_b = params[-1]\n",
        "    logits = np.dot(final_w, activations) + final_b\n",
        "    return logits - logsumexp(logits)\n",
        "\n",
        "# Make a batched version of the `predict` function\n",
        "batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)\n",
        "\n",
        "\n",
        "def one_hot(x, k, dtype=np.float32):\n",
        "    \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
        "    return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "def loss(params, in_arrays, targets):\n",
        "    \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
        "    preds = batch_forward(params, in_arrays)\n",
        "    return -np.sum(preds * targets)\n",
        "\n",
        "def accuracy(params, data_loader):\n",
        "    \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
        "    acc_total = 0\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        images = np.array(data).reshape(data.size(0), 28*28)\n",
        "        targets = one_hot(np.array(target), num_classes)\n",
        "\n",
        "        target_class = np.argmax(targets, axis=1)\n",
        "        predicted_class = np.argmax(batch_forward(params, images), axis=1)\n",
        "        acc_total += np.sum(predicted_class == target_class)\n",
        "    return acc_total/len(data_loader.dataset)\n",
        "\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
        "    value, grads = value_and_grad(loss)(params, x, y)\n",
        "    opt_state = opt_update(0, grads, opt_state)\n",
        "    return get_params(opt_state), opt_state, value\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "step_size = 1e-3\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "num_epochs = 10\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
        "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
        "    # Initialize placeholder for loggin\n",
        "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
        "\n",
        "    # Get the initial set of parameters\n",
        "    params = get_params(opt_state)\n",
        "\n",
        "    # Get initial accuracy after random init\n",
        "    train_acc = accuracy(params, train_loader)\n",
        "    test_acc = accuracy(params, test_loader)\n",
        "    log_acc_train.append(train_acc)\n",
        "    log_acc_test.append(test_acc)\n",
        "\n",
        "    # Loop over the training epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            if net_type == \"MLP\":\n",
        "                # Flatten the image into 784 vectors for the MLP\n",
        "                x = np.array(data).reshape(data.size(0), 28*28)\n",
        "            elif net_type == \"CNN\":\n",
        "                # No flattening of the input required for the CNN\n",
        "                x = np.array(data)\n",
        "            y = one_hot(np.array(target), num_classes)\n",
        "            params, opt_state, loss = update(params, x, y, opt_state)\n",
        "            train_loss.append(loss)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        train_acc = accuracy(params, train_loader)\n",
        "        test_acc = accuracy(params, test_loader)\n",
        "        log_acc_train.append(train_acc)\n",
        "        log_acc_test.append(test_acc)\n",
        "        print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
        "                                                                    train_acc, test_acc))\n",
        "\n",
        "    return train_loss, log_acc_train, log_acc_test\n",
        "\n",
        "\n",
        "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
        "                                                          opt_state,\n",
        "                                                          net_type=\"MLP\")\n",
        "\n",
        "# Plot the loss curve over time\n",
        "from helpers import plot_mnist_performance\n",
        "plot_mnist_performance(train_loss, train_log, test_log,\n",
        "                       \"MNIST MLP Performance\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}