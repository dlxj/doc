

# 项目反应理论 [u](https://github.com/nababasky/IRT) 



因为log函数是单调递增的，所以求p(x, z)的最大值，即求log(p(x, z))的最大值。[u](http://blog.sciencenet.cn/blog-2970729-1191928.html)



**马尔科夫链蒙特卡洛积分**(Markov Chain Monte Carlo, MCMC)。MCMC 参数估计

- 项目反应理论中潜在心理特质“填补”的参数.pdf



比如LOGIST 是根据**三参数**对数模式设计的,**要求被试人数至少为1000, 项目数至少为40**;

- 项目反应理论与经典测验理论之比较



（**讲得最清楚**）**EM算法(期望最大化算法)简介** [u](http://blog.sciencenet.cn/blog-2970729-1191928.html)

- 二项系数刚好等于$n$ 选$k$ 的组合数

$$
\begin{pmatrix}
n \\
k
\end{pmatrix}
= C^k_n = \frac{n!}{k!(n-k)!}
$$

> 5取3排列可以看成P(5，3)=C(5,3)*3！，5取3排列是5取3组合的3！倍
>
> 排列 (P, Permutation) 就是將組合 (C) 每個結果做一次排列
>
> 从n个不同元素中每次取出m（1≤m≤n）个不同元素，排成一列，称为从n个元素中取出m个元素的无重复排列或直线排列，简称排列
>
> 从n个不相同元素中取出m个排成一列（有序），第一个位置可以有n个选择，第二个位置可以有n-1个选择（已经有1个放在前一个位置），则同理可知第三个位置可以有n-2个选择，以此类推第m个位置可以有n-m+1个选择




$$
C_{n}^{m} = \frac{n!}{(n-m)!m!} \ \ \text{组合,n选m} \\
\text {组合公式的推导是由排列公式去掉重复的部分而来的}
$$

$$
P^m_n = C_n^m \times P_m^m \\
= \frac{n!}{(n-m)!m!} \times m! \\
= \frac{n!}{(n-m)!}
$$

- $P_m^m 易推$，$P_n^m$ 引入$C_n^m$，是关键，先选出全部组合再对每一个组合做一次全排列
- $P^m_n = C_n^m \times P_m^m$ 联系组合，全部通了


$$
1 \ \ n - 0  \\ 
2 \ \ n - 1 \\
\vdots \\
m \ \ n - (m-1) \\
n \ \ n - (n-1)
$$

> n选n 的全排列：第一个位置有n 种选择，第二 n-1 种，第 m 个 n-m+1 种，第n 个 1 种，
>
> 所以 $P_n^n = n!$，$P_n^m = $





【机器学习】【白板推导系列】【合集 1～23】.pdf [u](D:\workcode\algo\项目反应理论)

从最大似然到EM算法：一致的理解方式 [u](https://kexue.fm/archives/5239)

AI算法工程师手册 - EM算法原理 [u](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/13_EM.html)

人人都懂EM算法 [u](https://zhuanlan.zhihu.com/p/36331115)

> E步：计算联合分布的条件概率期望：
>
> ![image-20210105081435221](项目反应理论.assets/image-20210105081435221.png)
>
> M步：极大化 $L(\theta)$ ,得到:
>
> ![image-20210105081512597](项目反应理论.assets/image-20210105081512597.png)

EM算法原理及推导 [u](https://zhuanlan.zhihu.com/p/85236423)

**Gaussian Processes for Machine Learning** (Adaptive Computation and Machine Learning) by Carl Edward Rasmussen, Christopher K. I. Williams (z-lib.org).pdf

**Mathematical Methods and Algorithms for Signal Processing** by Todd K. Moon, Wynn C. Stirling (z-lib.org).pdf

- p.444 条件期望

- 条件分布：事件$A$ 发生的条件下，随机变量$X$ 的**条件分布列**

  ![image-20210105091203503](项目反应理论.assets/image-20210105091203503.png)





双参数二级计分模型的参数估计 [u](https://zhuanlan.zhihu.com/p/29887184)
$$
P = \frac{e^{a \theta + b}}{1+e^{a \theta + b}}
$$

## 似然函数是联合概率，既出现这一组观察数据的概率

- 观察到一条条的数据同时出现，这是联合概率，似然函数就是给出某组数据的联合概率



## EAP

```
EAP（expected a posteriori）期望后验参数估计算法中的高斯-埃尔米特(Gauss-Hermite) 积分数值计算方法研究
    用于实时估计考生能力，　
    应用场景在于考生在线实时答题过程中快速实时调整对该考生的能力估计，
    要求计算要快，消耗资源要少
    EAP（expected a posteriori）算法是唯一不需要迭代的算法，所以它的计算速度是最快的，常用于在线测验的参数估计，其理论依据是贝叶斯法则。


あき時間 vacant hours 空闲时间 あき〔明き〕

双参数二级计分模型进行参数估计：https://zhuanlan.zhihu.com/p/29887184
注意这公式和书上的有点不同
# https://github.com/17zuoye/pyirt/blob/master/pyirt/util/tools.py 
# 另一个参考项目，基于论文的实现 IRT Parameter Estimation using the EM Algorithm By Brad Hanson 2000


高斯厄米特积分点的数量，影响积分计算的精度
    肯定是越大越好，但也没太多必要，6个积分点就够了

参见:Gauss–Hermite积分 in 深入理解神经网络：从逻辑回归到CNN.md
```



### 似然函数



似然函数是说，假设某个被试的能力$\theta$ 已知，$n$ 道题的区分度$a$，难度$b$ 已知，则这个被试出现这一组答题结果$U_{(1 \times n)},\ u_i \in \{0 \  if\ 答错, 1\ if\ 答对\}$ 的概率



根据局部独立性假设，联合概率(就是似然概率)是所有$u_i$ 概率的乘积。答对则$u_i$ 的概率是$P_i$，答错则$u_i$ 的概率是$1-P_i$
$$
L(U|\theta,a,b) = \prod^n_i P_i^{u_i} (1-P_i)^{1-u_i}
$$

- 如果$u_i$ 是1，则$(1-P_i)^{0} = 1$

- 如果$u_i$ 是0，则$P_i^{0} = 1$

> 效果就是保证只留下其中一项



### EAP 公式

$$
E(\theta_i) = \theta_i = \frac{\int \theta_i g(\theta)L(\theta_i)d\theta}{\int g(\theta)L(\theta_i)d\theta}
$$

- $g(\theta)$ 是概率密度函数，假设为服从正态分布，$\theta_i$ 是某个被试的能力，从此分布抽出



上式可以**用Gauss–Hermite积分处理** （高斯-埃尔米特）





### Gauss–Hermite积分


$$
\int e^{-x^2} f(x)dx \approx \sum w_if(x_i)
$$
假设$\theta \sim N(0,1)$ 于是Gauss–Hermite积分变为：
$$
\sum \frac{w_i}{\sqrt{\pi}} f(\sqrt{2} x_i)
$$



```python
import numpy as np

def Z(slop, threshold, theta):
    # z函数
    _z = slop * theta + threshold
    _z[_z > 35] = 35
    _z[_z < -35] = -35
    return _z

def P(z):
    # 回答正确的概率函数
    e = np.exp(z)
    p = e / (1.0 + e)
    return p

def get_gh_point(gp_size):
    # 积分点的数量，影响积分计算的精度，肯定是越大越好，但也没太多必要，6个积分点就够了
    x_nodes, x_weights = np.polynomial.hermite.hermgauss(gp_size)  # Gauss–Hermite积分点数
    x_nodes = x_nodes * 2 ** 0.5
    x_nodes.shape = x_nodes.shape[0], 1
    x_weights = x_weights / np.pi ** 0.5
    x_weights.shape = x_weights.shape[0], 1
    return x_nodes, x_weights


class EAPIrt2PLModel(object):

    def __init__(self, score, slop, threshold): # 得分，区分度，阀值
        self.x_nodes, self.x_weights = get_gh_point(21)
        z = Z(slop, threshold, self.x_nodes)
        p = P(z)
        self.lik_values = np.prod(p**score*(1.0 - p)**(1-score), axis=1) 
        """
        计算所有元素的乘积，按行连乘(维度变成n*1)。
        如果不指定轴，则不管是几维都展平成一维然后连乘
        """

    @property
    def g(self):
        x = self.x_nodes[:, 0]
        weight = self.x_weights[:, 0]
        return np.sum(x * weight * self.lik_values)

    @property
    def h(self):
        weight = self.x_weights[:, 0]
        return np.sum(weight * self.lik_values)

    @property
    def res(self):
        return round(self.g / self.h, 3)


if __name__ == "__main__":

    """
    一个人的真实能力是1，他答了1000 道题，使用EAP 算法估计他的能力
    """
    num = 3 # 题数
    a = np.random.uniform(1, 3, num)       # 1000 道题的区分度  # 均匀分布
    b = np.random.normal(0, 1, size=num)   # 1000 个阀值        # 正态分存
    z = Z(a, b, 1) # 区分度，阀值，能力
    p = P(z)
    score = np.random.binomial(1, p, num)
    # 计算并打印潜在特质估计值
    eap = EAPIrt2PLModel(score, a, b)  # 得分，区分度，阀值
    print(eap.res)


    """
    阀值 = -1 * ( 难度 * 区分度 )
    难度 = -1 * ( 阀值 / 区分度 )
    """

    """
    SELECT t.Alpha, t.Beta FROM testirt t ORDER BY RAND() LIMIT 1000;
    """
```



## EM 

(Expectation-Maximization) 期望最大化

EM算法是机器学习十大算法之一

EM算法，是一种迭代算法，用于含有隐变量（hidden variable）的概率参数模型的**最大似然估计或极大后验概率估计**。



#### E-Step和M-Step

- E-Step：通过observed data和现有模型估计参数值 missing data

- M-Step：假设missing data已知的情况下，最大化似然函数

算法保证每次迭代后似然函数都会增大，所以函数最终会收敛



```
    """
    先求11 个积分点
    """
    x_nodes, x_weights = np.polynomial.hermite.hermgauss(gp_size)  # Gauss–Hermite积分点数
    x_nodes = x_nodes * 2 ** 0.5
    x_nodes.shape = x_nodes.shape[0], 1
    x_weights = x_weights / np.pi ** 0.5
    x_weights.shape = x_weights.shape[0], 1
```







## 成品



flexMIRT  蔡力  



"3blue1brown" "概率"

```python

# 成品


from __future__ import print_function, division
import numpy as np
import warnings

"""
双参数二级计分模型进行参数估计：https://zhuanlan.zhihu.com/p/29887184
注意这公式和书上的有点不同
# https://github.com/17zuoye/pyirt/blob/master/pyirt/util/tools.py 
# 另一个参考项目，基于论文的实现 IRT Parameter Estimation using the EM Algorithm By Brad Hanson 2000
"""

class BaseIrt(object):

    def __init__(self, scores=None):
        self.scores = scores

    @staticmethod
    def p(z):
        # 回答正确的概率函数
        e = np.exp(z)
        p = e / (1.0 + e)
        return p

class Irt2PL(BaseIrt):

    @staticmethod
    def z(slop, threshold, theta):
        # z函数
        _z = slop * theta + threshold
        _z[_z > 35] = 35
        _z[_z < -35] = -35
        return _z

   # EM算法求解
    def __init__(self, init_slop=None, init_threshold=None, max_iter=10000, tol=1e-5, gp_size=11,
                 m_step_method='newton', *args, **kwargs):
        """
        :param init_slop: 斜率初值
        :param init_threshold: 阈值初值
        :param max_iter: EM算法最大迭代次数
        :param tol: 精度
        :param gp_size: Gauss–Hermite积分点数
        """
        super(Irt2PL, self).__init__(*args, **kwargs)
        # 斜率初值
        if init_slop is not None:
            self._init_slop = init_slop
        else:
            self._init_slop = np.ones(self.scores.shape[1])
        # 阈值初值
        if init_threshold is not None:
            self._init_threshold = init_threshold
        else:
            self._init_threshold = np.zeros(self.scores.shape[1])
        self._max_iter = max_iter
        self._tol = tol
        self._m_step_method = '_{0}'.format(m_step_method)
        self.x_nodes, self.x_weights = self.get_gh_point(gp_size)

    @staticmethod
    def get_gh_point(gp_size):
        x_nodes, x_weights = np.polynomial.hermite.hermgauss(gp_size)
        x_nodes = x_nodes * 2 ** 0.5
        x_nodes.shape = x_nodes.shape[0], 1
        x_weights = x_weights / np.pi ** 0.5
        x_weights.shape = x_weights.shape[0], 1
        return x_nodes, x_weights

    def _lik(self, p_val):
        # 似然函数
        scores = self.scores
        loglik_val = np.dot(np.log(p_val + 1e-200), scores.transpose()) + \
                     np.dot(np.log(1 - p_val + 1e-200), (1 - scores).transpose())
        return np.exp(loglik_val)

    def _e_step(self, p_val, weights):
        # EM算法E步
        # 计算theta的分布人数
        scores = self.scores
        lik_wt = self._lik(p_val) * weights
        # 归一化
        lik_wt_sum = np.sum(lik_wt, axis=0)
        _temp = lik_wt / lik_wt_sum
        # theta的人数分布
        full_dis = np.sum(_temp, axis=1)
        # theta下回答正确的人数分布
        right_dis = np.dot(_temp, scores)
        full_dis.shape = full_dis.shape[0], 1
        # 对数似然值
        print(np.sum(np.log(lik_wt_sum)))
        return full_dis, right_dis

    def _irls(self, p_val, full_dis, right_dis, slop, threshold, theta):
        # 所有题目误差列表
        e_list = (right_dis - full_dis * p_val) / full_dis * (p_val * (1 - p_val))
        # 所有题目权重列表
        _w_list = full_dis * p_val * (1 - p_val)
        # z函数列表
        z_list = self.z(slop, threshold, theta)
        # 加上了阈值哑变量的数据
        x_list = np.vstack((threshold, slop))
        # 精度
        delta_list = np.zeros((len(slop), 2))
        for i in range(len(slop)):
            e = e_list[:, i]
            _w = _w_list[:, i]
            w = np.diag(_w ** 0.5)
            wa = np.dot(w, np.hstack((np.ones((self.x_nodes.shape[0], 1)), theta)))
            temp1 = np.dot(wa.transpose(), w)
            temp2 = np.linalg.inv(np.dot(wa.transpose(), wa))
            x0_temp = np.dot(np.dot(temp2, temp1), (z_list[:, i] + e))
            delta_list[i] = x_list[:, i] - x0_temp
            slop[i], threshold[i] = x0_temp[1], x0_temp[0]
        return slop, threshold, delta_list


    def _newton(self, p_val, full_dis, right_dis, slop, threshold, theta):
        # 一阶导数
        dp = right_dis - full_dis * p_val
        # 二阶导数
        ddp = full_dis * p_val * (1 - p_val)
        # jac矩阵和hess矩阵
        jac1 = np.sum(dp, axis=0)
        jac2 = np.sum(dp * theta, axis=0)
        hess11 = -1 * np.sum(ddp, axis=0)
        hess12 = hess21 = -1 * np.sum(ddp * theta, axis=0)
        hess22 = -1 * np.sum(ddp * theta ** 2, axis=0)
        delta_list = np.zeros((len(slop), 2))
        # 把求稀疏矩阵的逆转化成求每个题目的小矩阵的逆
        for i in range(len(slop)):
            jac = np.array([jac1[i], jac2[i]])
            hess = np.array(
                [[hess11[i], hess12[i]],
                 [hess21[i], hess22[i]]]
            )
            delta = np.linalg.solve(hess, jac)
            slop[i], threshold[i] = slop[i] - delta[1], threshold[i] - delta[0]
            delta_list[i] = delta
        return slop, threshold, delta_list

    def _est_item_parameter(self, slop, threshold, theta, p_val):
        full_dis, right_dis = self._e_step(p_val, self.x_weights)
        return self._m_step(p_val, full_dis, right_dis, slop, threshold, theta)

    def _m_step(self, p_val, full_dis, right_dis, slop, threshold, theta):
        # EM算法M步
        m_step_method = getattr(self, self._m_step_method)
        return m_step_method(p_val, full_dis, right_dis, slop, threshold, theta)

    def em(self):
        max_iter = self._max_iter
        tol = self._tol
        slop = self._init_slop
        threshold = self._init_threshold
        for i in range(max_iter):
            z = self.z(slop, threshold, self.x_nodes)
            p_val = self.p(z)
            slop, threshold, delta_list = self._est_item_parameter(slop, threshold, self.x_nodes, p_val)
            if np.max(np.abs(delta_list)) < tol:
                print(i)
                return slop, threshold
        warnings.warn("no convergence")
        return slop, threshold

class EAPIrt2PLModel(object):

    def __init__(self, score, slop, threshold, model=Irt2PL):
        self.x_nodes, self.x_weights = model.get_gh_point(21)
        z = model.z(slop, threshold, self.x_nodes)
        p = model.p(z)
        self.lik_values = np.prod(p**score*(1.0 - p)**(1-score), axis=1)

    @property
    def g(self):
        x = self.x_nodes[:, 0]
        weight = self.x_weights[:, 0]
        return np.sum(x * weight * self.lik_values)

    @property
    def h(self):
        weight = self.x_weights[:, 0]
        return np.sum(weight * self.lik_values)

    @property
    def res(self):
        return round(self.g / self.h, 3)


f = 'lsat.csv'
score = np.loadtxt(f, delimiter=",")
res = Irt2PL(scores=score).em()
alpha = res[0]                  # 区分度(斜率) slop
beta = -1 * (res[1] / alpha)    # 难度
"""
结果和R语言ltm 包算出来的一致
ltm 用法参考这本书：《Using R for Item Response Theory Model Applications by Insu Paek, Ki Cole (z-lib.org)》

#detach("package:eRm")
#install.packages("ltm")
library("ltm")
setwd("D:\\workcode\\algo")
getwd()
#answers <- read.csv("answers_2PL.csv")

#ddd<- read.csv("answers_2PL.csv")
ddd<- read.csv('lsat.csv')

#ddd<-read.fwf("c2_2pl.dat", widths=c(10,rep(1,20)))  # read fixed width data format
# 前10 列是person identification
dim(ddd) #2.2_2
#数据维度是：(1000*21)，默认列名是：V1 到V21
#第一列是person index

#ddd <- ddd[,-1] # 2.2_3 
#去掉第一列，只取 V2到V21
#用head(ddd) 或 names(ddd) 来验证这一点

#names(ddd) <- paste("It",1:20, sep="") #2.2_4
#列名重命名为：It1 到It20

mod.2pl <- ltm(ddd~z1, IRT.param=T) #2. 2_5
mod.2pl$conv #2. 2_6

summary(mod.2pl) #2.2_7
co <- coef(mod.2pl) #2.2_8
co
"""


"""
估计一个人的能力，用1000 道题的得分和参数
"""
# 模拟参数
a = np.random.uniform(1, 3, 1000)
b = np.random.normal(0, 1, size=1000)
z = Irt2PL.z(a, b, 1) # slop, threshold, theta (_z = slop * theta + threshold )
p = Irt2PL.p(z)
score = np.random.binomial(1, p, 1000)
# 计算并打印潜在特质估计值
eap = EAPIrt2PLModel(score, a, b)
print(eap.res)


"""
阀值 = -1 * ( 难度 * 区分度 )
难度 = -1 * ( 阀值 / 区分度 )
"""

n_persons = 1000
n_questions = 10
np.random.seed(543678)
true_theta = np.random.normal(loc=0, scale=1, size=(n_persons,1))   # 能力 (1000*1)
true_theta = np.tile(true_theta, n_questions)                       # 每道题的能力都一样，列复制10次 (1000*1) -> (1000*10)
true_beta = np.random.normal(loc=0, scale=1, size=(1,n_questions))  # 10个问题的难度 (1*10)

true_alpha = np.random.uniform(1, 3, (1,n_questions) ) # 区分度 (1*10)
true_threshold = -1 * ( true_beta * true_alpha )
z = Irt2PL.z(true_alpha, true_threshold, true_theta) # slop, threshold, theta (_z = slop * theta + threshold )
likelihood = Irt2PL.p(z)

score = np.random.binomial(size=(n_persons, n_questions), p=likelihood, n=1)
res = Irt2PL(scores=score).em()
alpha = res[0]                   # 区分度(斜率) slop
threshold = res[1]               # 阀值
beta = -1 * (threshold / alpha)  # 难度


# 难度的均方误差
print('difficulty mse: {}'.format(np.mean((beta - true_beta) ** 2)))

# 区分度的均方误差
print('slop mse: {}'.format(np.mean((alpha - true_alpha) ** 2)))


"""
下面估计第一个人的能力
EAP（expected a posteriori）算法是唯一不需要迭代的算法，所以它的计算速度是最快的，常用于在线测验的参数估计，其理论依据是贝叶斯法则。
"""
# 第一个人10 道题的得分(用0 或1表示)
theta = []
for i in range(n_persons):
    sc = score[i]
    eap = EAPIrt2PLModel(sc, alpha, threshold)  # 得分，区分度，阀值
    theta.append( eap.res )
    #print(eap.res)

true_theta_first_col = true_theta[:,0] # 取第一列 
true_theta_first_col = list( map( lambda x:round(x, 2), true_theta_first_col ) )
theta = list( map( lambda x:round(x, 2), theta ) )
print( true_theta_first_col[:25] )
print( theta[:25] )

# 能力均方误差
# print('theta mse: {}'.format(np.mean((theta - true_theta_first_col) ** 2)))

print('hi,,,')
```



## 数值积分 

统计计算 [u](https://www.math.pku.edu.cn/teachers/lidf/docs/statcomp/html/_statcompbook/approx-quad.html)

统计计算：方法与实践 [u](https://yanfei.site/docs/statscompbook/index.html)



在统计计算中经常需要计算积分。比如，从密度$p(x)$**计算分布函数**$F(x)$，如果**没有解析表达式**和精确的计算公式， **需要用积分来计算**



从联合密度**计算边缘密度**， 要用积分计算



贝叶斯分析中从先验密度$\pi(\theta)$ 和似然函数$p(x|\theta)$，**计算后验密度**
$$
p(\theta|x) = \frac{p(\theta,x)}{p(x)} = \frac{\pi(\theta)p(x|\theta)}{\int \pi(u)p(x|u)du}
$$

 不能得到后验密度 $p(\theta|x)$ 的解析表达式时，需要计算积分，**用后验密度求期望、平均损失函数**也需要计算积分



从参数的初始估计开始，交替进行E-step和M-step

- 在E-step考虑「观测数据」和「当前参数估计值」，计算目标函数的条件期望

- 在M-step使条件期望对于目标参数最大化

- 更新估计值并迭代至收敛







```python
# 双参模型
# 生成模似数据：D:\workcode\algo\项目反应理论\mytest.py # https://github.com/17zuoye/pyirt
# 好像不太对生成数据：D:\workcode\python\std\iIRT.py
# 验证：D:\workcode\algo\项目反应理论\itm_2pl.R
```

```python
# 双参模型
#detach("package:eRm")
#install.packages("ltm")
library("ltm")
setwd("D:\\workcode\\algo")
getwd()
#answers <- read.csv("answers_2PL.csv")

ddd<-read.fwf("c2_2pl.dat", widths=c(10,rep(1,20)))  # read fixed width data format
"""
前10 列是person identification

""" 
#2.2_1
dim(ddd) #2.2_2
"""
数据维度是：(1000*21)，默认列名是：V1 到V21
第一列是person index
"""
ddd <- ddd[,-1] # 2.2_3 
"""
去掉第一列，只取 V2到V21
用head(ddd) 或 names(ddd) 来验证这一点
"""
names(ddd) <- paste("It",1:20, sep="") #2.2_4
"""
列名重命名为：It1 到It20
"""

mod.2pl <- ltm(ddd~z1, IRT.param=T) #2. 2_5
mod.2pl$conv #2. 2_6

summary(mod.2pl) #2.2_7
co <- coef(mod.2pl) #2.2_8
co
```





**讲得最清晰的书** [u](The Basics of Item Response Theory Using R by Frank B. Baker, Seock-Ho Kim (auth.) (z-lib.org))



**项目反应理论参数的EM估计** [u](https://blog.csdn.net/zoe_su/article/details/84761812?utm_medium=distribute.pc_relevant.none-task-blog-baidulandingword-2&spm=1001.2101.3001.4242)

**Python与项目反应理论：基于EM和MCMC的参数估计算法** [u](https://zhuanlan.zhihu.com/p/29887184)

- **"期望后验" EAP**（expected a posteriori）
  
  - 高斯-埃尔米特（Gauss-Hermitt）求积公式 [u](https://www.yuque.com/lingr7/nhruk5/nlnygg)
    - 数值计算方法与算法（第三版） 张韵华
  - [u](http://honglin.fun/2020/06/05/%E8%AF%8D%E6%9D%A1/Gauss-Hermite-Quadrature/)
  - **高斯求积简介** [u](https://discourse.juliacn.com/t/topic/1024)
  - 高斯型求积公式 [u](https://blog.csdn.net/Reborn_Lee/article/details/80940918)
    - 待定系数法与高斯型求积公式 [u](D:\workcode\algo\项目反应理论\待定系数法与高斯型求积公式.pdf)
- 代码 [u](https://github.com/nababasky/IRT)

  > 他的难度定义有点奇葩，包含在阈值（或截距）b里面 
  >
  > > -1 * 阈值（或截距）/ 区分度（或斜率）= 难度
  > >
  > > -1 * b / a 



<img src="项目反应理论.assets/image-20201223113438030.png" alt="image-20201223113438030" style="zoom:50%;" />



<img src="项目反应理论.assets/image-20201223113720890.png" alt="image-20201223113720890" style="zoom:50%;" />

D:\workcode\algo\项目反应理论\A note on improving EAP trait estimation in oblique.pdf

https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9





**基于论文的实现**  [u](https://github.com/17zuoye/pyirt)

- IRT Parameter Estimation using the EM Algorithm By Brad Hanson 2000

  - Github 实现 [u](https://github.com/17zuoye/pyirt/blob/master/pyirt/solver/model.py)

    > **创建环境**
    >
    > conda create -n irt  python=3.8
    >
    > conda activate irt
    >
    > cd ~/pyirt-master
    >
    > pipenv --three
    > make
    >
    > pipenv shell
    >
    > **使用环境**
    >
    > conda activate irt
    >
    > cd ~/pyirt-master
    >
    > pipenv shell





**introduction irt modeling in R** [u](https://quantdev.ssri.psu.edu/tutorials/introduction-irt-modeling)

> ```R
> library(ltm)
> answers <- read.csv("answers_rasch.csv")
> head(answers)
> describe(answers)
> PL1.rasch<-rasch(irtdat)
> summary(PL1.rasch)
> item.fit(PL1.rasch,simulate.p.value=T)  # 数据太大就是拟合不好
> theta.rasch<-ltm::factor.scores(PL1.rasch) # 估计能力
> summary(theta.rasch$score.dat$z1)
> ```





**py-irt in pyro 清晰高端** [u](https://github.com/jplalor/py-irt)

> scipy.special.expit
>
> expit(x) = 1/(1+exp(-x))
>
> ```python
> 
> # py-irt 
> 
> import argparse
> import csv 
> 
> import numpy as np
> import pyro
> import torch
> import torch.nn as nn
> 
> from py_irt.models.one_param_logistic import OneParamLog
> from scipy.special import expit
> 
> import pandas as pd
> import os
> import matplotlib.pyplot as plt
> 
> n_persons = 100
> n_questions = 5
> 
> # 生成模似数据：难度, 能力
> def gen_data(n_persons, n_questions):
>     np.random.seed(543678)
>     true_theta = np.random.normal(loc=0, scale=1, size=(n_persons,1))   # 2个人的能力 (2*1)
>     true_theta = np.tile(true_theta, n_questions)                       # 2个人每道题的能力都一样，列复制2次 (2*1) -> (2*2)
>     true_beta = np.random.normal(loc=0, scale=1, size=(1,n_questions))  # 2个问题的难度 (1*2)
>     return (true_theta, true_beta)
> 
> def answers_from_rasch(theta, beta, n_persons, n_questions):
>     """
>     维度不同的减法：
>     (2*2) - (1*2) = ( (1*2) - (1*2),
>                       (1*2) - (1*2)
>                     )
>     = (2*2) 
>     """
>     likelihood = np.exp(theta - beta) / (1 + np.exp(theta - beta))
>     answers = np.random.binomial(size=(n_persons, n_questions), p=likelihood, n=1) # calculate answers based on theta and likelihood function
>     answers = torch.tensor(answers).float() # convert to tensor
>     """
>     size 和 p 的维度相同
>     二项分布，值为1 的概率由p 给出
>     """
>     return answers
> 
> # 显示柱状图，有多少人答对多少题
> def showhist(answers):
>     sums = torch.sum(answers, 1)  # 按行求和  # 每个人答对多少题
>     plt.hist(sums) 
>     plt.show()
> 
> def save_answers(answers, n_questions):
>     answers_numpy = answers.numpy()
>     seg = pd.DataFrame(answers_numpy,columns= [ 'item'+str(i+1) for i in range( n_questions ) ]  ) # ['item1','item2','item3','item4','item5']
>     seg.to_csv(os.path.join( os.path.dirname( os.getcwd() ), 'answers_rasch.csv'), index=False ,encoding="utf-8")    
> 
> 
> (true_theta, true_beta) = gen_data(n_persons, n_questions)
> answers = answers_from_rasch(true_theta, true_beta, n_persons, n_questions)
> showhist(answers)
> save_answers(answers,n_questions)
> 
> """
> R 语言 itm 包 单参Rash 模型算出来的结果：
> 
> D:\workcode\algo\项目反应理论>Rscript itm_rash.R
> [1] "D:/workcode/algo"
> [1] 1 2
> [1] 5 3
> [1] "matrix" "array"
>           Dffclt Dscrmn P(x=1|z=0)
> item1 -1.0139307      1  0.7337887
> item2 -0.7962753      1  0.6891772
> item3  0.5855046      1  0.3576670
> item4  0.8462623      1  0.3002175
> item5  0.9553656      1  0.2778070
> """
> 
> 
> 
> parser = argparse.ArgumentParser()
> parser.add_argument('-e', '--num-epochs', default=10000, type=int)
> parser.add_argument('--gpu', action='store_true')
> parser.add_argument('--priors', help='[vague, hierarchical]', default='hierarchical')
> parser.add_argument('--model', help='[1PL,2PL(coming soon)]', default='1PL')  # 2pl not implemented yet
> args = parser.parse_args()
> 
> device = torch.device('cpu')
> if args.gpu:
>     device = torch.device('cuda')
> 
> # 1. load data from file 
> # 2. combine into obs 3-tuples
> 
> models = []
> items = []
> responses = []
> 
> real_theta = true_theta[:,0].T  # 取第一列
> real_diff  = true_beta[0]       # 取第一行
> 
> # real_theta = np.random.normal(size=[100])
> # real_diff = np.random.normal(size=[5])
> 
> obs = []
> for i in range(len(real_theta)):
>     for j in range(len(real_diff)):
>         y = np.random.binomial(1, expit(real_theta[i] - real_diff[j]))
>         models.append(i) 
>         items.append(j) 
>         responses.append(y) 
> 
> print(real_theta)
> print(real_diff)
> print(responses) 
> 
> num_models = len(set(models))
> num_items = len(set(items))
> print(num_items, num_models)
> 
> models = torch.tensor(models, dtype=torch.long, device=device)          # 能力
> items = torch.tensor(items, dtype=torch.long, device=device)            # 难度
> responses = torch.tensor(responses, dtype=torch.float, device=device)   # 对错
> 
> 
> # 3. define model and guide accordingly
> if args.model == '1PL':
>     m = OneParamLog(args.priors, device, num_items, num_models)
> 
> 
> # 4. fit irt model with svi, trace-elbo loss
> m.fit(models, items, responses, args.num_epochs) 
> 
> # 5. once model is fit, write outputs (diffs and thetas) to disk, 
> #       retaining original modelIDs and itemIDs so we can use them 
> 
> 
> for name in pyro.get_param_store().get_all_param_names():
>     print(name)
>     val = pyro.param(name).data.numpy()
>     print(val)
>     if name == 'loc_diff':
>         print('mse: {}'.format(np.mean((val - real_diff) ** 2)))
>     elif name == 'loc_ability':
>         print('mse: {}'.format(np.mean((val - real_theta) ** 2)))
> ```
>
> 





```R
#detach("package:eRm")
#install.packages("ltm")
library("ltm")
#setwd("D:\\workcode\\algo\\项目反应理论\\IRT-master")
getwd()
answers <- read.csv("1000x50000.csv")
#head(answers)

ct = cbind( ncol(answers)+1, 1) # 区分度(斜率) 固定为1
dim(ct)

o <- rasch(answers, constraint=ct)
co <- coef(o, TRUE, TRUE) # 提取参数，type 是list 或matrix
dim(co)
class(co)
co
write.table(co,file="test.txt") # 存参数
```



```python
from __future__ import print_function, division, unicode_literals
import sys
#sys.path.append('..')
import numpy as np
from psy import Irt2PL


import numpy as np
import torch
from torch.distributions import constraints
import pyro
import pyro.infer
import pyro.optim
import pyro.distributions as dist

import pandas as pd
import os
import matplotlib.pyplot as plt

# f = 'data/lsat.csv'
# score = np.loadtxt(f, delimiter=",")
# res = Irt2PL(scores=score).em()
# print(res)

"""
https://github.com/nababasky/IRT

概率编程语言入门指南

有偏差的掷硬币(a biased coin toss)

随机函数被叫做模型，表达模型的方法，和正常的Python方法没有区别
模型（model）和变分分布（guide）的参数。【注：所谓变分就是将原始函数换作另一（易处理的）函数的数学技巧】
最大化证据（evidence）  证据下限”ELBO（evidence lower bound）


https://colab.research.google.com/drive/1SZDm5ppWBFIpowO8KIATfoYZXEVbuVGr#scrollTo=tdhST1QE8MuX

Fitting a Distribution with Pyro: Part 2 - Beta
https://www.richard-stanton.com/2020/05/03/fit-dist-with-pyro_2.html


A Gentle Introduction to Probabilistic Programming Languages
https://medium.com/swlh/a-gentle-introduction-to-probabilistic-programming-languages-bf1e19042ab6
"""

"""
#detach("package:eRm")
#install.packages("ltm")
library("ltm")
#setwd("D:\\workcode\\algo\\项目反应理论\\IRT-master")
getwd()
answers <- read.csv("1000x50000.csv")
#head(answers)

ct = cbind( ncol(answers)+1, 1) # 区分度(斜率) 固定为1
dim(ct)

o <- rasch(answers, constraint=ct)
co <- coef(o, TRUE, TRUE) # 提取参数，type 是list 或matrix
dim(co)
class(co)
co
write.table(co,file="test.txt") # 存参数
"""

n_persons = 1000
n_questions = 50000
pyro.enable_validation(True)
np.random.seed(543678)
true_theta = np.random.normal(loc=0, scale=1, size=(n_persons,1))   # 2个人的能力 (2*1)
true_theta = np.tile(true_theta, n_questions)                       # 2个人每道题的能力都一样，列复制2次 (2*1) -> (2*2)
true_beta = np.random.normal(loc=0, scale=1, size=(1,n_questions))  # 2个问题的难度 (1*2)


def answers_from_rasch(theta, beta, n_persons, n_questions):
    """
    维度不同的减法：
    (2*2) - (1*2) = ( (1*2) - (1*2),
                      (1*2) - (1*2)
                    )
    = (2*2) 
    """
    likelihood = np.exp(theta - beta) / (1 + np.exp(theta - beta))
    answers = np.random.binomial(size=(n_persons, n_questions), p=likelihood, n=1) # calculate answers based on theta and likelihood function
    answers = torch.tensor(answers).float() # convert to tensor
    """
    size 和 p 的维度相同
    二项分布，值为1 的概率由p 给出
    """
    return answers

# 显示柱状图，有多少人答对多少题
def showhist(answers):
    sums = torch.sum(answers, 1)  # 按行求和  # 每个人答对多少题
    plt.hist(sums) 
    plt.show()

def save_answers(answers, n_questions):
    answers_numpy = answers.numpy()
    seg = pd.DataFrame(answers_numpy,columns= [ 'item'+str(i+1) for i in range( n_questions ) ]  ) # ['item1','item2','item3','item4','item5']
    seg.to_csv(os.path.join( os.getcwd(), 'answers_numpy.csv'), index=False ,encoding="utf-8")    


answers = answers_from_rasch(true_theta, true_beta, n_persons, n_questions)
showhist(answers)
save_answers(answers,n_questions)


# import os
# os.environ['R_HOME'] = "C:\\Program Files\\R\\R-4.0.3"
# os.environ['R_USER'] = "D:\\usr\\python38\\lib\\site-packages\\rpy2"

# import rpy2.robjects as robjects
# robjects.r.source("D:\\workcode\\algo\\项目反应理论\\IRT-master\\op.R")
# a = robjects.r('op()')
# print(type(a))

R = r"C:\Program Files\R\R-4.0.3\bin\x64\Rscript.exe op.R"
#re = os.system(R)
print(R)

import subprocess
out_bytes = subprocess.check_output([r"C:\Program Files\R\R-4.0.3\bin\x64\Rscript.exe", r"C:\Program Files\R\R-4.0.3\bin\x64\op.R"])
out_text = out_bytes.decode('utf-8')


# R包itm 算出来的难度 [-1.014, -0.796, 0.586, 0.846, 0.955]
estimated_beta = np.array( [ [-1.014, -0.796, 0.586, 0.846, 0.955] ] )
answers2 = answers_from_rasch(true_theta, estimated_beta, n_persons, n_questions)
showhist(answers2)

"""
# Compare prior sample of theta to true theta
plt.hist(true_theta[:, 0], alpha=0.5, label='True theta')
plt.hist(theta_prior.flatten(), alpha=0.5, label='Theta prior')
plt.legend()
"""

if __name__ == "__main__":
    pass



"""
`probs` <-
function (x) {
    pr <- plogis(x)
    if (any(ind <- pr == 1))
        pr[ind] <- 1 - sqrt(.Machine$double.eps)
    if (any(ind <- pr == 0))
        pr[ind] <- sqrt(.Machine$double.eps)
    pr
}


`betas.rasch` <-
function (betas, constraint, p) {
    if (!is.null(constraint)) {
        betas. <- numeric(p + 1)
        betas.[constraint[, 1]] <- constraint[, 2]
        betas.[-constraint[, 1]] <- betas
        cbind(betas.[1:p], abs(betas.[p + 1]))
    } else {
        cbind(betas[1:p], abs(betas[p + 1]))
    }
}



`gauher` <-
function (n) {
    EPS <- 3e-14
    PIM4 <- 0.751125544464943
    MAXIT <- 10
    m <- trunc((n + 1)/2)
    x <- w <- rep(-1, n)
    for (i in 1:m) {
        if (i == 1) {
            z <- sqrt(2 * n + 1) - 1.85575 * (2 * n + 1)^(-0.16667)
        }
        else if (i == 2) {
            z <- z - 1.14 * n^0.426/z
        }
        else if (i == 3) {
            z <- 1.86 * z - 0.86 * x[1]
        }
        else if (i == 4) {
            z <- 1.91 * z - 0.91 * x[2]
        }
        else {
            z <- 2 * z - x[i - 2]
        }
        for (its in 1:MAXIT) {
            p1 <- PIM4
            p2 <- 0
            for (j in 1:n) {
                p3 <- p2
                p2 <- p1
                p1 <- z * sqrt(2/j) * p2 - sqrt((j - 1)/j) * 
                  p3
            }
            pp <- sqrt(2 * n) * p2
            z1 <- z
            z <- z1 - p1/pp
            if (abs(z - z1) <= EPS) 
                break
        }
        x[i] <- z
        x[n + 1 - i] <- -z
        w[i] <- 2/(pp * pp)
        w[n + 1 - i] <- w[i]
    }
    list(x = x, w = w)
}


`GHpoints` <-
function (form, k) {
    av <- all.vars(form)
    factors <- sum(av %in% c("z1", "z2"))
    cf <- paste(form[3])
    form. <- paste(" ~ ", paste("z", 1:factors, collapse = " + ", sep = ""), " + ", cf)
    form. <- as.formula(form.)
    GH <- gauher(k)
    grid.t <- expand.grid(lapply(1:factors, function (k, u) u$x, u = GH))
    names(grid.t) <- paste("z", 1:factors, sep = "")
    out <- model.matrix(form., sqrt(2) * grid.t)
    colnams <- colnames(out)
    dimnames(out) <- attr(out, "assign") <- NULL
    grid.w <- as.matrix(expand.grid(lapply(1:factors, function (k, u) u$w, u = GH)))
    grid.w <- (2^(factors/2)) * apply(grid.w, 1, prod) * exp(rowSums(grid.t * grid.t))
    grid.w <- grid.w * exp(rowSums(dnorm(out[, seq(2, factors + 1), drop = FALSE], log = TRUE)))
    names(grid.w) <- NULL
    list(x = out, w = grid.w, colnams = colnams)
}


`start.val.rasch` <-
function (start.val, data) {
    data <- na.exclude(data)
    attr(data, "na.action") <- NULL
    n <- nrow(data)
    p <- ncol(data)
    cmptStrVal <- is.null(start.val) || (start.val == "random" || (all(is.numeric(start.val)) && length(start.val) != p+1))
    randStrVal <- length(start.val) == 1 && start.val == "random"
    if (cmptStrVal) {
        rs <- as.vector(rowSums(data))
        len.uni <- length(unique(rs))
        rs <- factor(rs, labels = 1:len.uni)
        rs <- as.numeric(levels(rs))[as.integer(rs)]
        z <- cbind(1, seq(-3, 3, len = len.uni)[rs])
        if (randStrVal)
            z[, 2] <- rnorm(n)
        coefs <- matrix(0, p, 2)
        for (i in 1:p) {
            fm <- try(glm.fit(z, data[, i], family = binomial()), silent = TRUE)
            coefs[i, ] <- if (!inherits(fm, "try-error")) {
                fm$coef
            } else {
                glm.fit(cbind(1, rnorm(n)), data[, i], family = binomial())$coef
            }
        }
        c(coefs[, 1], mean(coefs[, 2]))
    } else 
        start.val
}



`irasch` <-
function (data, constraint = NULL, IRT.param = TRUE, start.val = NULL, na.action = NULL, control = list()) {
    cl <- match.call()
    if ((!is.data.frame(data) & !is.matrix(data)) || ncol(data) == 1)
        stop("'data' must be either a numeric matrix or a data.frame, with at least two columns.\n")
    X <- data.matrix(data)
    if (!all(its <- apply(X, 2, function (x) { x <- x[!is.na(x)]; length(unique(x)) } ) == 2))
        stop("'data' contain more that 2 distinct values for item(s): ", paste(which(!its), collapse = ", "))
    X <- apply(X, 2, function (x) if (all(unique(x) %in% c(1, 0, NA))) x else x - 1)
    if (!is.null(na.action))
        X <- na.action(X)    
    oX <- X
    colnamsX <- colnames(X)
    dimnames(X) <- NULL
    n <- nrow(X)
    p <- ncol(X)
    con <- list(iter.qN = 150, GHk = 21, method = "BFGS", verbose = getOption("verbose"))
    con[names(control)] <- control
    betas <- start.val.rasch(start.val, oX)
    if (!is.null(constraint)) {
        if (!is.matrix(constraint) || (nrow(constraint) > p + 1 | ncol(constraint) != 2))
            stop("'constraint' should be a 2-column matrix with at most ", p + 1, " rows (read help file).\n")
        if (any(constraint[, 1] < 1 | constraint[, 1] > p + 1))
            stop("the 1st column of 'constraint' should between 1 and ", p + 1, " (read help file).\n")
        constraint <- constraint[order(constraint[, 1]), , drop = FALSE]
        constraint[, 1] <- round(constraint[, 1])
        betas[constraint[, 1]] <- NA
    }
    pats <- apply(X, 1, paste, collapse = "")
    freqs <- table(pats)
    obs <- as.vector(freqs)
    X <- apply(cbind(names(freqs)), 1, function (x) {
                nx <- nchar(x)
                out <- substring(x, 1:nx, 1:nx)
                out <- out[out != "A"]
                out[out == "N"] <- NA
                out
        })
    X <- as.numeric(t(X))
    dim(X) <- c(length(freqs), p)
    mX <- 1 - X
    if (any(na.ind <- is.na(X)))
        X[na.ind] <- mX[na.ind] <- 0
    GH <- GHpoints(data ~ z1, con$GHk)
    Z <- GH$x
    GHw <- GH$w
    logLik.rasch <- function (betas, constraint) {
        betas <- betas.rasch(betas, constraint, p)
        pr <- probs(Z %*% t(betas))
        p.xz <- exp(X %*% t(log(pr)) + mX %*% t(log(1 - pr)))
        p.x <- rep(c(p.xz %*% GHw), obs)
        -sum(log(p.x))
    }
    score.rasch <- function (betas, constraint) {
        betas <- betas.rasch(betas, constraint, p)
        pr <- probs(Z %*% t(betas))
        p.xz <- exp(X %*% t(log(pr)) + mX %*% t(log(1 - pr)))
        p.x <- c(p.xz %*% GHw)
        p.zx <- p.xz / p.x
        Nt <- GHw * colSums(p.zx * obs)
        scores <- matrix(0, p, 2)
        for (i in 1:p) {
            ind <- !na.ind[, i]
            rit <- if (all(ind)) GHw * colSums(p.zx * X[, i] * obs) else GHw * colSums(p.zx[ind, ] * X[ind, i] * obs[ind])
            scores[i, ] <- -c(crossprod(rit - pr[, i] * Nt, Z))
        }    
        if (!is.null(constraint))
            c(scores[, 1], sum(scores[, 2]))[-constraint[, 1]]
        else
            c(scores[, 1], sum(scores[, 2]))
    }
    environment(logLik.rasch) <- environment(score.rasch) <- environment()
    res.qN <- optim(betas[!is.na(betas)], fn = logLik.rasch, gr = score.rasch, method = con$method, hessian = TRUE, 
                control = list(maxit = con$iter.qN, trace = as.numeric(con$verbose)), constraint = constraint)
    if (all(!is.na(res.qN$hessian) & is.finite(res.qN$hessian))) {
        ev <- eigen(res.qN$hessian, TRUE, TRUE)$values
        if (!all(ev >= -1e-06 * abs(ev[1]))) 
            warning("Hessian matrix at convergence is not positive definite; unstable solution.\n")
    } else 
        warning("Hessian matrix at convergence contains infinite or missing values; unstable solution.\n")
    betas <- betas.rasch(res.qN$par, constraint, p)
    rownames(betas) <- if (!is.null(colnamsX)) colnamsX else paste("Item", 1:p)
    colnames(betas) <- c("beta.i", "beta")
    max.sc <- max(abs(score.rasch(res.qN$par, constraint)))
    fit <- list(coefficients = betas, log.Lik = -res.qN$value, convergence = res.qN$conv, hessian = res.qN$hessian, 
                counts = res.qN$counts, patterns = list(X = X, obs = obs), GH = list(Z = Z, GHw = GHw), max.sc = max.sc, 
                constraint = constraint, IRT.param = IRT.param, X = oX, control = con, na.action = na.action, call = cl)
    class(fit) <- "rasch"
    fit
}


setwd("D:\\workcode\\algo\\项目反应理论\\IRT-master")
getwd()
answers <- read.csv("answers_numpy.csv")
dim(answers)

ct = cbind( ncol(answers)+1, 1) # 区分度(斜率) 固定为1
dim(ct)

o <- irasch(answers, constraint=ct)
o

"""
```





```R
R script (saved in C:/myRscript.R)

#!/usr/bin/env Rscript

#Assign the arguments passed from the batch file to R variables
args = commandArgs(trailingOnly = TRUE)
outputFolder = args[[1]]
value = as.integer(args[[2]])

#Run the R code
print("Start the R code")
myData = data.frame(random = runif(value))
write.csv(myData, paste0(outputFolder, "randomNrs.csv"), row.names = F)

Batch file (shell)

outputFolder=C:/Documents/project/

echo "This is the start of the batch script"

#Run the R script and pass it arguments
Rscript C:/myRscript.R $outputFolder 10

ls $outputFolder
```





**R矩阵和数组** [u](https://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/prog-type-matrix.html)

```R
`op` <-
function(x){
  y <- list(name=x)
  y
}

o <- op(1)
class(o)
```

```python
import subprocess
out_bytes = subprocess.check_output([r"C:\Program Files\R\R-4.0.3\bin\x64\Rscript.exe", r"C:\Program Files\R\R-4.0.3\bin\x64\op.R"])
out_text = out_bytes.decode('utf-8')
```





```R
mat <- matrix(1:10,ncol=2)
rownames(mat) <- letters[1:5]
colnames(mat) <- LETTERS[1:2]

mat
write.table(mat,file="test.txt") # keeps the rownames
read.table("test.txt",header=TRUE,row.names=1) # says first column are rownames
unlink("test.txt")
write.table(mat,file="test2.txt",row.names=FALSE) # drops the rownames
read.table("test.txt",header=TRUE) 
unlink("test2.txt")
```



**Tutorial: Rasch and 2PL Model in R** [u](https://hansjoerg.me/2018/04/23/rasch-in-r-tutorial/)

> class(dat_1)
>
> R语言+data.frame
>
> **Reading the CSV file into Data frames in R** [u](https://www.journaldev.com/38079/r-read-csv-file-into-data-frame)
>
> > ```R
> > detach("package:eRm")
> > #install.packages("ltm")
> > library("ltm")
> > setwd("D:\\workcode\\algo\\项目反应理论\\IRT-master")
> > getwd()
> > answers <- read.csv("answers_numpy.csv")
> > out <- rasch(answers)
> > out
> > ```

> ```R
> ## First we fit the original form of the Rasch model assuming
> ## fixed discrimination parameter equal to 1; results are reported
> ## under the usual IRT parameterization; in order to fix the
> ## discrimination parameter the 'constraint' argument is used
> 
> m1 <- rasch(LSAT, constr = cbind(length(LSAT) + 1, 1))  # 列和并(行必须相同)
> # 两个(1*1) -> 一个(1*2)
> ```
>
> ```python
> # 根据真实参数生成Rasch 数据，方便后面验证估计得准不准
> from __future__ import print_function, division, unicode_literals
> import sys
> #sys.path.append('..')
> import numpy as np
> from psy import Irt2PL
> 
> 
> import numpy as np
> import torch
> from torch.distributions import constraints
> import pyro
> import pyro.infer
> import pyro.optim
> import pyro.distributions as dist
> 
> import matplotlib.pyplot as plt
> 
> # f = 'data/lsat.csv'
> # score = np.loadtxt(f, delimiter=",")
> # res = Irt2PL(scores=score).em()
> # print(res)
> 
> """
> https://github.com/nababasky/IRT
> 
> 概率编程语言入门指南
> 
> 有偏差的掷硬币(a biased coin toss)
> 
> 随机函数被叫做模型，表达模型的方法，和正常的Python方法没有区别
> 模型（model）和变分分布（guide）的参数。【注：所谓变分就是将原始函数换作另一（易处理的）函数的数学技巧】
> 最大化证据（evidence）  证据下限”ELBO（evidence lower bound）
> 
> 
> https://colab.research.google.com/drive/1SZDm5ppWBFIpowO8KIATfoYZXEVbuVGr#scrollTo=tdhST1QE8MuX
> 
> Fitting a Distribution with Pyro: Part 2 - Beta
> https://www.richard-stanton.com/2020/05/03/fit-dist-with-pyro_2.html
> 
> 
> A Gentle Introduction to Probabilistic Programming Languages
> https://medium.com/swlh/a-gentle-introduction-to-probabilistic-programming-languages-bf1e19042ab6
> """
> 
> n_persons = 100
> n_questions = 5
> pyro.enable_validation(True)
> np.random.seed(543678)
> true_theta = np.random.normal(loc=0, scale=1, size=(n_persons,1))   # 2个人的能力 (2*1)
> true_theta = np.tile(true_theta, n_questions)                       # 2个人每道题的能力都一样，列复制2次 (2*1) -> (2*2)
> true_beta = np.random.normal(loc=0, scale=1, size=(1,n_questions))  # 2个问题的难度 (1*2)
> 
> """
> 维度不同的减法：
> (2*2) - (1*2) = ( (1*2) - (1*2),
>                   (1*2) - (1*2)
>                 )
> = (2*2) 
> """
> likelihood = np.exp(true_theta - true_beta) / (1 + np.exp(true_theta - true_beta))
> answers = np.random.binomial(size=(n_persons, n_questions), p=likelihood, n=1) # calculate answers based on theta and likelihood function
> answers = torch.tensor(answers).float() # convert to tensor
> """
> size 和 p 的维度相同
> 二项分布，值为1 的概率由p 给出
> """
> 
> sums = torch.sum(answers, 1)  # 按行求和  # 每个人答对多少题
> plt.hist(sums) 
> plt.show()
> 
> answers_numpy = answers.numpy()
> # res = Irt2PL(scores=answers_numpy).em()
> # print(res)
> 
> import pandas as pd
> import os
> seg = pd.DataFrame(answers_numpy,columns=['item1','item2','item3','item4','item5'])
> seg.to_csv(os.path.join( os.getcwd(), 'answers_numpy.csv'), index=False ,encoding="utf-8")
> 
> if __name__ == "__main__":
>     pass
> ```
>
> 
>
> ```R
> # ltm package source code
> 
> `probs` <-
> function (x) {
>     pr <- plogis(x)
>     if (any(ind <- pr == 1))
>         pr[ind] <- 1 - sqrt(.Machine$double.eps)
>     if (any(ind <- pr == 0))
>         pr[ind] <- sqrt(.Machine$double.eps)
>     pr
> }
> 
> 
> `betas.rasch` <-
> function (betas, constraint, p) {
>     if (!is.null(constraint)) {
>         betas. <- numeric(p + 1)
>         betas.[constraint[, 1]] <- constraint[, 2]
>         betas.[-constraint[, 1]] <- betas
>         cbind(betas.[1:p], abs(betas.[p + 1]))
>     } else {
>         cbind(betas[1:p], abs(betas[p + 1]))
>     }
> }
> 
> 
> 
> `gauher` <-
> function (n) {
>     EPS <- 3e-14
>     PIM4 <- 0.751125544464943
>     MAXIT <- 10
>     m <- trunc((n + 1)/2)
>     x <- w <- rep(-1, n)
>     for (i in 1:m) {
>         if (i == 1) {
>             z <- sqrt(2 * n + 1) - 1.85575 * (2 * n + 1)^(-0.16667)
>         }
>         else if (i == 2) {
>             z <- z - 1.14 * n^0.426/z
>         }
>         else if (i == 3) {
>             z <- 1.86 * z - 0.86 * x[1]
>         }
>         else if (i == 4) {
>             z <- 1.91 * z - 0.91 * x[2]
>         }
>         else {
>             z <- 2 * z - x[i - 2]
>         }
>         for (its in 1:MAXIT) {
>             p1 <- PIM4
>             p2 <- 0
>             for (j in 1:n) {
>                 p3 <- p2
>                 p2 <- p1
>                 p1 <- z * sqrt(2/j) * p2 - sqrt((j - 1)/j) * 
>                   p3
>             }
>             pp <- sqrt(2 * n) * p2
>             z1 <- z
>             z <- z1 - p1/pp
>             if (abs(z - z1) <= EPS) 
>                 break
>         }
>         x[i] <- z
>         x[n + 1 - i] <- -z
>         w[i] <- 2/(pp * pp)
>         w[n + 1 - i] <- w[i]
>     }
>     list(x = x, w = w)
> }
> 
> 
> `GHpoints` <-
> function (form, k) {
>     av <- all.vars(form)
>     factors <- sum(av %in% c("z1", "z2"))
>     cf <- paste(form[3])
>     form. <- paste(" ~ ", paste("z", 1:factors, collapse = " + ", sep = ""), " + ", cf)
>     form. <- as.formula(form.)
>     GH <- gauher(k)
>     grid.t <- expand.grid(lapply(1:factors, function (k, u) u$x, u = GH))
>     names(grid.t) <- paste("z", 1:factors, sep = "")
>     out <- model.matrix(form., sqrt(2) * grid.t)
>     colnams <- colnames(out)
>     dimnames(out) <- attr(out, "assign") <- NULL
>     grid.w <- as.matrix(expand.grid(lapply(1:factors, function (k, u) u$w, u = GH)))
>     grid.w <- (2^(factors/2)) * apply(grid.w, 1, prod) * exp(rowSums(grid.t * grid.t))
>     grid.w <- grid.w * exp(rowSums(dnorm(out[, seq(2, factors + 1), drop = FALSE], log = TRUE)))
>     names(grid.w) <- NULL
>     list(x = out, w = grid.w, colnams = colnams)
> }
> 
> 
> `start.val.rasch` <-
> function (start.val, data) {
>     data <- na.exclude(data)
>     attr(data, "na.action") <- NULL
>     n <- nrow(data)
>     p <- ncol(data)
>     cmptStrVal <- is.null(start.val) || (start.val == "random" || (all(is.numeric(start.val)) && length(start.val) != p+1))
>     randStrVal <- length(start.val) == 1 && start.val == "random"
>     if (cmptStrVal) {
>         rs <- as.vector(rowSums(data))
>         len.uni <- length(unique(rs))
>         rs <- factor(rs, labels = 1:len.uni)
>         rs <- as.numeric(levels(rs))[as.integer(rs)]
>         z <- cbind(1, seq(-3, 3, len = len.uni)[rs])
>         if (randStrVal)
>             z[, 2] <- rnorm(n)
>         coefs <- matrix(0, p, 2)
>         for (i in 1:p) {
>             fm <- try(glm.fit(z, data[, i], family = binomial()), silent = TRUE)
>             coefs[i, ] <- if (!inherits(fm, "try-error")) {
>                 fm$coef
>             } else {
>                 glm.fit(cbind(1, rnorm(n)), data[, i], family = binomial())$coef
>             }
>         }
>         c(coefs[, 1], mean(coefs[, 2]))
>     } else 
>         start.val
> }
> 
> 
> 
> `irasch` <-
> function (data, constraint = NULL, IRT.param = TRUE, start.val = NULL, na.action = NULL, control = list()) {
>     cl <- match.call()
>     if ((!is.data.frame(data) & !is.matrix(data)) || ncol(data) == 1)
>         stop("'data' must be either a numeric matrix or a data.frame, with at least two columns.\n")
>     X <- data.matrix(data)
>     if (!all(its <- apply(X, 2, function (x) { x <- x[!is.na(x)]; length(unique(x)) } ) == 2))
>         stop("'data' contain more that 2 distinct values for item(s): ", paste(which(!its), collapse = ", "))
>     X <- apply(X, 2, function (x) if (all(unique(x) %in% c(1, 0, NA))) x else x - 1)
>     if (!is.null(na.action))
>         X <- na.action(X)    
>     oX <- X
>     colnamsX <- colnames(X)
>     dimnames(X) <- NULL
>     n <- nrow(X)
>     p <- ncol(X)
>     con <- list(iter.qN = 150, GHk = 21, method = "BFGS", verbose = getOption("verbose"))
>     con[names(control)] <- control
>     betas <- start.val.rasch(start.val, oX)
>     if (!is.null(constraint)) {
>         if (!is.matrix(constraint) || (nrow(constraint) > p + 1 | ncol(constraint) != 2))
>             stop("'constraint' should be a 2-column matrix with at most ", p + 1, " rows (read help file).\n")
>         if (any(constraint[, 1] < 1 | constraint[, 1] > p + 1))
>             stop("the 1st column of 'constraint' should between 1 and ", p + 1, " (read help file).\n")
>         constraint <- constraint[order(constraint[, 1]), , drop = FALSE]
>         constraint[, 1] <- round(constraint[, 1])
>         betas[constraint[, 1]] <- NA
>     }
>     pats <- apply(X, 1, paste, collapse = "")
>     freqs <- table(pats)
>     obs <- as.vector(freqs)
>     X <- apply(cbind(names(freqs)), 1, function (x) {
>                 nx <- nchar(x)
>                 out <- substring(x, 1:nx, 1:nx)
>                 out <- out[out != "A"]
>                 out[out == "N"] <- NA
>                 out
>         })
>     X <- as.numeric(t(X))
>     dim(X) <- c(length(freqs), p)
>     mX <- 1 - X
>     if (any(na.ind <- is.na(X)))
>         X[na.ind] <- mX[na.ind] <- 0
>     GH <- GHpoints(data ~ z1, con$GHk)
>     Z <- GH$x
>     GHw <- GH$w
>     logLik.rasch <- function (betas, constraint) {
>         betas <- betas.rasch(betas, constraint, p)
>         pr <- probs(Z %*% t(betas))
>         p.xz <- exp(X %*% t(log(pr)) + mX %*% t(log(1 - pr)))
>         p.x <- rep(c(p.xz %*% GHw), obs)
>         -sum(log(p.x))
>     }
>     score.rasch <- function (betas, constraint) {
>         betas <- betas.rasch(betas, constraint, p)
>         pr <- probs(Z %*% t(betas))
>         p.xz <- exp(X %*% t(log(pr)) + mX %*% t(log(1 - pr)))
>         p.x <- c(p.xz %*% GHw)
>         p.zx <- p.xz / p.x
>         Nt <- GHw * colSums(p.zx * obs)
>         scores <- matrix(0, p, 2)
>         for (i in 1:p) {
>             ind <- !na.ind[, i]
>             rit <- if (all(ind)) GHw * colSums(p.zx * X[, i] * obs) else GHw * colSums(p.zx[ind, ] * X[ind, i] * obs[ind])
>             scores[i, ] <- -c(crossprod(rit - pr[, i] * Nt, Z))
>         }    
>         if (!is.null(constraint))
>             c(scores[, 1], sum(scores[, 2]))[-constraint[, 1]]
>         else
>             c(scores[, 1], sum(scores[, 2]))
>     }
>     environment(logLik.rasch) <- environment(score.rasch) <- environment()
>     res.qN <- optim(betas[!is.na(betas)], fn = logLik.rasch, gr = score.rasch, method = con$method, hessian = TRUE, 
>                 control = list(maxit = con$iter.qN, trace = as.numeric(con$verbose)), constraint = constraint)
>     if (all(!is.na(res.qN$hessian) & is.finite(res.qN$hessian))) {
>         ev <- eigen(res.qN$hessian, TRUE, TRUE)$values
>         if (!all(ev >= -1e-06 * abs(ev[1]))) 
>             warning("Hessian matrix at convergence is not positive definite; unstable solution.\n")
>     } else 
>         warning("Hessian matrix at convergence contains infinite or missing values; unstable solution.\n")
>     betas <- betas.rasch(res.qN$par, constraint, p)
>     rownames(betas) <- if (!is.null(colnamsX)) colnamsX else paste("Item", 1:p)
>     colnames(betas) <- c("beta.i", "beta")
>     max.sc <- max(abs(score.rasch(res.qN$par, constraint)))
>     fit <- list(coefficients = betas, log.Lik = -res.qN$value, convergence = res.qN$conv, hessian = res.qN$hessian, 
>                 counts = res.qN$counts, patterns = list(X = X, obs = obs), GH = list(Z = Z, GHw = GHw), max.sc = max.sc, 
>                 constraint = constraint, IRT.param = IRT.param, X = oX, control = con, na.action = na.action, call = cl)
>     class(fit) <- "rasch"
>     fit
> }
> 
> 
> setwd("D:\\workcode\\algo\\项目反应理论\\IRT-master")
> getwd()
> answers <- read.csv("answers_numpy.csv")
> dim(answers)
> 
> ct = cbind( ncol(answers)+1, 1) # 区分度(斜率) 固定为1
> dim(ct)
> 
> o <- irasch(answers, constraint=ct)
> co <- coef(o, TRUE, TRUE) # 提取参数，type 是list 或matrix
> dim(co)
> class(co)
> co
> write.table(co,file="test.txt") # 存参数
> ```
>
> 



**IRT 教科书代码实现 in github** [u](https://github.com/pluralsight/irt_parameter_estimation)

- book [u](https://www.routledge.com/Item-Response-Theory-Parameter-Estimation-Techniques-Second-Edition/Baker-Kim/p/book/9780824758257)



**Measuring what Matters: Introduction to Rasch Analysis in R** [u](https://bookdown.org/dkatz/Rasch_Biome/)



理论较完备 [u](https://m-clark.github.io/sem/item-response-theory.html)

- **The Logit Link Function** [u](https://www.sciencedirect.com/topics/mathematics/logit-link-function)



**Variational Item Response Theory: Fast, Accurate, and Expressive** [u](https://arxiv.org/abs/2002.00276)

- IRT in **pytorch** [u](https://github.com/mhw32/variational-item-response-theory-public)

**introduction irt modeling** in R [u](https://quantdev.ssri.psu.edu/tutorials/introduction-irt-modeling)

**IRT in Pyro** [u](https://colab.research.google.com/drive/1SZDm5ppWBFIpowO8KIATfoYZXEVbuVGr#scrollTo=tdhST1QE8MuX)

**A MATLAB Package for Markov Chain Monte Carlo with a Multi-Unidimensional IRT Model** [u](https://www.jstatsoft.org/article/view/v028i10)

- **此网站论文+代码 全开放下载，概率统计专题**

  > Static toolbox
  >
  > Garch toolbox

**mirt: A Multidimensional Item Response Theory Package for the R Environment** [u](https://www.jstatsoft.org/article/view/v048i06)



book **Item Response Theory  Parameter Estimation Techniques** [u](Item Response Theory  Parameter Estimation Techniques, Second Edition by Baker, Frank B. Kim, Seock-Ho (z-lib.org).pdf)

github [xxIRT in R] [u](https://github.com/xluo11/xxIRT#parameter-estimation)

![image-20201202114201780](项目反应理论.assets/image-20201202114201780.png)



**概率编程语言入门指南** [u](https://www.yanxishe.com/TextTranslation/2691)



全部知识点的综合掌握程度 = 能力 = 潜在特质（latent traits）



**计算机自适应测验中RASCH模型稳健性的模拟研究** [u](https://wenku.baidu.com/view/8fe865eb856a561252d36fd7.html)

> **基于pytorch的单参模型代码** [u](https://github.com/jplalor/py-irt)
>
> > **pyro** [u](https://github.com/pyro-ppl/pyro)
> >
> > **SVI 随机变分推断** [u](https://causalai.github.io/pyro_zh_tutorial/svi_part_i.html)
> >
> > **Pyro简介：产生式模型实现库（一），模型** [u](https://www.jianshu.com/p/b7267ce2d763)
> >
> > **Pyro 从入门到出门** [u]()
> >
> > > 搞机器学习的同学大部分在线性代数、矩阵论、微积分上没什么问题，**但是概率论、信息论的知识还是有很大空白的**。因此，这里给大家推荐一个现成的库 Pyro
> > >
> > > ```python
> > > m = Bernoulli(torch.tensor([0.3])) # 二项（伯努利）分布
> > > m.sample()
> > > # tensor([ 0.])
> > > # 有 30% 可能性出现 1，70% 可能性出现 0
> > > ```
> > >
> > > ```python
> > > m = Normal(torch.tensor([0.0]), torch.tensor([1.0])) # 正态（高斯）分布
> > > m.sample()
> > > # tensor([ 0.1046])
> > > ```
> > >
> > > ```python
> > > m = Poisson(torch.tensor([4])) # 泊松分布
> > > m.sample()
> > > # tensor([ 3.])
> > > ```
> > >
> > > 只需要记住，在**增强学习中要调用 sample，在 VAE 中要调用 rsample** 就可以了

> y = np.random.binomial(1, expit(real_theta[i] - real_diff[j])) 
>
> \# The expit function, also known as the logistic sigmoid function, is defined as expit(x) = 1/(1+exp(-x)). 



- **许多学术论文将术语“变量”、“分布”、“密度”，甚至“模型”互换使用**。这种做法本身不一定导致错误，因为X、P(X)和p(X)都可以通过一对一的对应关系相互指代。



**变分贝叶斯初探** [u](https://www.jianshu.com/p/86c5d1e1ef93)

**Rasch model By Pyro** [u](https://forum.pyro.ai/t/rasch-model/1224)

> https://colab.research.google.com/drive/1SZDm5ppWBFIpowO8KIATfoYZXEVbuVGr

**pip install girth** --upgrade [u](https://github.com/eribean/girth)

- **非常非常好**



**python机器学习笔记：EM算法** [u](https://www.cnblogs.com/wj-1314/p/12856388.html)

> $p(x|\theta)$ 对分布采样，分布是生产者，它会按一定概率生产不同的产品

**手把手教你实现一个高斯混合模型** [u](https://developer.ibm.com/zh/technologies/analytics/articles/ba-lo-machine-learning-hands-on7-gmm/)

- **高斯混合模型（GMM）** [u](https://blog.csdn.net/benzhujie1245com/article/details/104737005)

> **求得**男生身高和女生身高这两个**高斯分布的参数**

> **当我们只有身高数据的时候，如何将身高数据聚成男女生两个簇？这就是高斯混合分布可以解决的问题**。高斯混合分布首先将该问题转换为包含隐变量（即每条样本属于不同类别的概率）和模型参数（即男女生两个高斯分布的参数）的极大似然估计问题。由于该极大似然估计问题中包含隐变量和模型参数，所以无法用传统的求偏导的方法求得。这时，需要利用EM算法，即期望最大化算法求解参数。

> 在高斯混合模型期望**(E-Step)**这一步中，需要**求解每个样本点属于男女生两类的概率**





**项目反应理论参数的EM估计** [u](https://blog.csdn.net/Zoe_Su/article/details/84761812)

> 　　将EM算法应用到IRT模型中来，则E步和M步可以描述为：
>
> 　　E步：即在给定缺失数据的分布，观察数据和参数初值时，求完全数据的对数似然函数的条件期望。
>
> 　　M步：即使用E步计算出的完全数据充分统计量的条件期望值，极大化完全数据的对数似然函数的条件期望求解参数的值。
>
> 　　不断的循环迭代E步和M步，直到参数估计收敛。
>
> 　　在IRT模型当中，我们通常认为能力参数 θθ 是连续随机变量，故可以取任意值。在EM算法估计参数的过程中，我们是能力参数 θθ 为离散分布。能力值只能取 q1,q2,......,qKq1,q2,......,qK, K个值中的一个，且 P(θ=qk)=πkP(θ=qk)=πk 。



**Jensen不等式** [u](http://zuzhiang.cn/2019/09/15/EM/)



**最大后验推断**(Maximum A Posteriori) ，**MAP 推断**



> """
> 概率编程语言入门指南
>
> 有偏差的掷硬币(a biased coin toss)
>
> 随机函数被叫做模型，表达模型的方法，和正常的Python方法没有区别
> 模型（model）和变分分布（guide）的参数。【注：所谓变分就是将原始函数换作另一（易处理的）函数的数学技巧】
> 最大化证据（evidence）  证据下限”ELBO（evidence lower bound）
>
>
> Fitting a Distribution with Pyro: Part 2 - Beta
> https://www.richard-stanton.com/2020/05/03/fit-dist-with-pyro_2.html
>
> A Gentle Introduction to Probabilistic Programming Languages
> https://medium.com/swlh/a-gentle-introduction-to-probabilistic-programming-languages-bf1e19042ab6
> """



自适应考试系统的分级组卷模板实现.doc



我们最终目的是要根据学生的做题对错情况来评估这个学生的当前能力值，也就是公式中的“θ”参数。其实问题就转化为**当我们看到观测数据（学生做题对错），确定该学生的能力值是多少才能得到这样的做题对错序列**。这样我们可以使用极大似然估计方法来估计“θ”的值，因为极大似然估计的目标就是找出一组参数，使得模型产生出观测数据的概率最大。







![image-20201124145430607](项目反应理论.assets/image-20201124145430607.png)



<img src="项目反应理论.assets/image-20201124150142395.png" alt="image-20201124150142395" style="zoom: 50%;" />







**剑桥专家为你一键解锁计算机自适应考试** [u](http://www.jzlt100.com/question/2385)

> 人工智能赋能英语学习”在线系列讲座是2020剑桥英语节的重要主题之一，由剑桥大学英语考评部首席研究经理徐兢博士作为主讲嘉宾，为大家深度解读英语测评的基本概念和人工智能在英语测评领域中的应用。



自适应测试：让定制化测试成为现实”(Building personalised assessment via Computer Adaptive Testing)

- **高效精准**：水平高的考生无需回答过多简单试题，水平有限的考生也不会遇到太多难题，从而可以确保在短时间内获得较为精确的测评结果。
- **安全便捷**：由人工智能加持，通过远程监考，并配合计算机自动评分，可以实现随时随地进行考试。
- **降低考生焦虑情绪**：每位考生遇到的题目难度不会超出其承受水平，这可以有效降低考生的焦虑情绪，让考生在考试中充分发挥其语言水平。



项目反应理论(Item Response Theory, 简称 IRT)，又称为隐性特征理论。该理论构建了一整套数学模型来描述**考生能力(test taker ability) 、题目特性 (task difficulty) 与考生答对率 (probability of correct answer)**之间的关系。



Rasch模型规定，当某个题目的难度和考生的能力相当，那么考生能够答对该题的概率为50%。这个数值也可以通过以下公式推导得出：

<img src="项目反应理论.assets/image-20201124141908328.png" alt="image-20201124141908328" style="zoom: 80%;" />

Rasch模型基本公式

在Rasch模型中，当一道题目难度中等，我们将该题赋值为0；当某位考生水平中等，则该考生水平也赋值为0，那通过以上公式可推导出该考生答对该题概率为50%。计算过程见下图。



<img src="项目反应理论.assets/image-20201124142553296.png" alt="image-20201124142553296" style="zoom: 67%;" />



<img src="项目反应理论.assets/image-20201124142702391.png" alt="image-20201124142702391" style="zoom: 80%;" />



<img src="项目反应理论.assets/image-20201124143148678.png" alt="image-20201124143148678" style="zoom: 67%;" />



由此可见，以上公式可以通过题目难度和考生能力，计算出考生答对题目的概率。但在计算机自适应考试中，计算机能够实时获取考生的答题结果。**因此通过将该公式反向推导，计算机可以根据每位考生对一系列考题的答题对错与否和相应考题的难易程度，反向估算出考生最有可能的语言能力水平。考生答题越多，能力估算就越精确。考试在达到预设的精确度后就会自动停止，给出最终结果。**





在经典测试理论(Classic Testing Theory)中，对于一整份考卷，**每个考生的答题表现可以总结到一个表格中**(如下图)。其中，顶部横项为题目，左侧纵向为考生名字（化名），数字1代表考生答对该题，数字0代表考生答错该题。经典测试理论通过累计考生答对题目的总数量来计算考生的水平。

![image-20201124144124096](项目反应理论.assets/image-20201124144124096.png)









**计算机自适应测试(Computer Adaptive Test，简称CAT)**

- 以项目反应理论（Item Response Theory）为基础

- 应用：托福留学外语考试（TOEFIE）、研究生资格考试（GRE）、护士资格证书等

- 模型计分的方式有两种，一种是多级计分，另一种1、0计分（正确记为1，错误记为0），本研究仅介绍1、0计分的逻辑谛斯克模型

- 项目选择 - **选题策略**是CAT一个非常重要的环节，它**直接影响测验的效率**问题
  
- 一种是**信息函数最大化策略**，另一种提加权偏离模型（Weighted Deviation Model ,WDM）。本研究中使用信息函数最大化策略
  
- CAT施测过程

  - 能力探查阶段

    > 测验刚开始时，一般并无被试真实水平信息，故设置一批探查性项目，初步估计其水平。本研究中，设置五个项目，难度分别为－2，－1,0，1,2；二是精确估计真值阶段，适应被试的水平从题库中挑选出能提供最大信息量的项目
  - 能力精估阶段
  - 终止规则
    
    > 一是确定题数，二是确定测量标准误，三是前两者的结合：若题数已达到，但标准误不及，则仍要继续施测，直至达到指定的测量标准误为止
    >
    > **测验终止规则采用定长：分别为10、15、20题三种**

- 能力估计的准确性的评价指标

  - 均方根误差 （Root Mean Square Error，简称RMSE）
  - 平均差异（Average Difference, 简称 AD）

  

- 选择项目反应模型

- **利用已有答题数据估计出题目参数与被试参数**
  
  - **运用概率与统计的方法来估计参数**







**Rasch模型**

Rasch模型 丹麦学者拉希（Rasch）是最早独立研究项目反应并获得巨大成功的学者，Rasch模型通常也叫单参逻辑什谛克模型，指的是被试的能力与项目难度两者关系的数学模型，它只有一个项目难度参数而没有区分度参数。拉希认为，用一批项目去测试被试，就是要在一个线性系统上去确定被试的特质水平，除了项目难度之外，应该维持所有项目的相同性质。拉希公式如下：
$$
a
$$










模型计分的方式有两种，一种是多级计分，另一种1、0计分（正确记为1，错误记为0），本研究仅介绍1、0计分的逻辑谛斯克模型。



项目反应理论 Rasch 实现 观测数据

**什么是Rasch模型？**[u](https://www.zhihu.com/question/394197045)

> IRT三参数模型的方法是使用更多参数去使“模型适应数据”, 而 **Rasch 模型却要求“数据符合模型”**
>
> 但 Rasch 模型却能**提供更稳定、更精确的题目难度参数**, 以及更好的题目和测验信度。
>
> 基于项目反应理论上的评估模型提供了数据驱动技术，用于为学习者选择适合当前知识水平的学习资源，并可用于不断地更新学习者的知识水平



基于多面Rasch模型的评分效应分析 [u](https://wenku.baidu.com/view/eea5393dad02de80d5d84058.html)

> 可以实现在同一个**洛基量尺（Logit scale）**上分析主观试题中**考生的能力、试卷的难度**、阅卷老师的宽严度以及评分量表的准确度等方面的表现以及他们之间的交互作用

基于Rasch 模型的英语阅读能力测试与评估



三参模型

b：代表项目难度系数，理论上可以取（-∞,+∞）,典型值在[-3,3]之间
a：代表项目区分度系数，理论上可以取（-∞,+∞），典型值在[-2.8,2.8]之间
c：代表猜测参数，猜测参数取值范围是[0,1]，典型值通常超过0.35
θ：代表能力值



计算机自适应考试（Computer-Adaptive Test, CAT）

> GRE考试的自适应模式算法
>
> 虽然dao官方没有公布du自适应算法，但zhi根据模考和大量dao考生数据可以推理出如专下模型：
> 第一套属算分的section一定是medium难度的，然后如果你对0-6个，下一个同类型section就会进入easy模式；7-13个，进入median模式；14-20个，进入hard模式。
> easy模式并不代表好拿分，因为得分不仅和正确题数有关，还和这个部分的难度系数有关。
> ETS没有公布官方的算分方式，但是根据多方信息汇总，大致可以得出如下数据：
> easy模式的题以难度系数1，2，3为主；
> medium模式的题以难度系数3，4为主；
> hard模式的题以难度系数4，5为主。
> 比如第一个section对了14题，那么第二个section=就进入到了hard模式，但这时只对了7题，最后的分数大概是155；
> 如果第一个section对了8题，那么第二个section进入到了medium模式，这时需要对16题才能拿到155分。
> 如果第一个section对了6题，那么第二个section进入到了easy模式，这时哪怕第二个section全对，也只有151分。



**基于认知诊断的个性化试题推荐方法 - 中国科学技术大学**

- 知识点的掌握程度  **知识点能力**



### 



## 自适应考试目的

> 试题精准匹配考生能力，减少刷题量。让高能力考生少刷低难度题，低能力考生少刷高难度题
>
> 根据考点考频确定出题优先级



我们自已的考点考频：高、中、低



**基于IRT的计算机自适应测评系统** [u](http://www.fengjunchen.com/cat/)

> 不可观察的学生能力和可观察的解题结果之间的不确定性是学习数据分析的根本问题



**Python与项目反应理论：基于EM和MCMC的参数估计算法** [u](https://zhuanlan.zhihu.com/p/29887184)

> **基于pytorch的单参模型代码** [u](https://github.com/jplalor/py-irt)
>
> **基于Tensorflow 的Rasch代码** [u](https://www.kaggle.com/mlarionov/the-rasch-model)
>
> **Rasch 英文教材(有matlab代码)** [u](Bayesian Reasoning and Machine Learning by David Barber (z-lib.org).pdf p.425)
>
> **人人都懂EM算法** [u](https://zhuanlan.zhihu.com/p/36331115)
>
> > 概率统计的思想，根据样本估算总体
> >
> > 正态分布(normal distribution) = 高斯分布(Gaussian distribution)
>
> **项目反应理论 EM估计** [u](https://blog.csdn.net/zoe_su/article/details/84761812?utm_medium=distribute.pc_relevant.none-task-blog-baidulandingword-2&spm=1001.2101.3001.4242)
>
> **IRT模型的参数估计方法**（EM算法和MCMC算法） [u](https://www.cnblogs.com/jiangxinyang/p/9621997.html)
>
> **什么是Rasch模型？**[u](https://www.zhihu.com/question/394197045)

**变分自编码器，神经网络应用于计量心理学模型的初探**  [u](https://zhuanlan.zhihu.com/p/141742408)

> **SVD分解(一)：自编码器与人工智能** [u](https://kexue.fm/archives/4208)



**人人都懂EM算法** [u](https://zhuanlan.zhihu.com/p/36331115)

> 概率统计的思想，根据样本估算总体
>
> 正态分布(normal distribution) = 高斯分布(Gaussian distribution)



**来个例子，再解释一次 EM 算法** [u](https://blog.csdn.net/xo3ylAF9kGs/article/details/105320790)



样本集 $X = x_1, x_2, \cdots, x_N$ 表示200 个人的身高

- 假设学校**所有学生的身高服从正态分布** $N(μ, \sigma^2)$
- 期望$μ$，方差$\sigma^2$ 未知
- 目标是：**从样本估算出未知的期望和方差**



**概率密度$p(x|\theta)$ 服从高斯分布$N(μ, \sigma^2)$** 

- 未知参数是 $\theta = [\mu, \sigma]^T$
- **目标是：估算 $\theta$**

每个样本都是独立地从$p(x|\theta)$中抽取的



**正好抽到这特定的 200 个身高的概率是一个联合概率**
$$
L(\theta) = L(x_1, x_2, \cdots , x_n; \theta) = \prod^n_{i=1} p(x_i|\theta), \theta \in \Theta
$$
**当概率密度函数的参数是$\theta$ 时，得到$X$ 这组样本的概率是$L(\theta)$**



**联合概率$L(\theta)$ 表示**在不同参数$\theta$ 取值下，**正好抽到$X $ 这组样本的概率**



**联合概率$L(\theta)$** **又称为**参数$\theta$ 相对于样本集$X$ 的**似然函数**(likehood function)



为了求导方便，**对似然函数取对数**，使连乘变成连加，**就得到对数似然函数**：
$$
H(\theta) = \ln L(\theta) = \ln \prod^n_{i=1} p(x_i|\theta) = \sum^n_{i=1} p(x_i|\theta)
$$





## IRT Parameter Estimation using the EM Algorithm



**观测数据(Observed Data)**
$$
Y_{(N \times J)} = \begin{pmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,J} \\
\vdots & \vdots   & \ddots & \vdots \\
y_{N,1} & y_{N,2} & \cdots & y_{N,J}
\end{pmatrix}
$$

- $N$ 个被试
- $J$ 道试题



$y_{i,j} = 1 \ \ \text{if 第i 被试答对第 j 道题}$ 

$y_{i,j} = 0 \ \ \text{if 第i 被试答错第 j 道题}$ 



**缺失数据(Missing Data)**



缺失数据是$N$ 个被试的**潜在能力**(unobserved latent)
$$
\theta = ( \theta_1, \theta_2, \cdots, \theta_N )
$$

- $\theta_i$ 是第$i$ 个被试的能力
- real numbers (**latent trait models**) or categories (**latent class models**).



**完全数据(Complete Data)**



完全数据是每一个被试的**观测数据加上缺失数据**
$$
[(y_1,\theta_1), (y_2,\theta_2), \cdots,(y_N,\theta_N),]
$$

- 每个被试的**答题数据加上自已的能力**








































































CTT的核心概念——信度，无法计算，这是最致命的



心理测量只是测量了一个**人对测验项目所进行的行为反应**，从而间接了解人的心理属性



**测评的本质是什么？是把不同人在同一个度量空间里区分开来**。运用IRT自适应的算法，**每个人虽然做的题目不一样**（由前一道题的答题情况决定下一道题测什么），但是测出来的知识点掌握程度是可以比较的。



IRT可以说是心理测量界的一次革命，也正是因为IRT理论的存在，SAT、ACT、雅思、托业等考试才能做到一年多次考试（其中的玄机在于**IRT等值和基于IRT的自适应测验**），同时，运用IRT的非认知测验（例如人格等），也在**处理自比数据和抵抗作假**等方面成果卓越。



假设考生在考试时**对试题的反应受某种心理潜在特质支配**，通过某种方法**估计这种心理潜在特质**

- 通过估计得到的是心理潜在特质的**观测值或观察分数**

- **真分数** = 观察分数 + 误差分数



**潜在特质空间**(能力空间)
$$
H = (\theta_1 \ \theta_2, \cdots , \ \theta_k)
$$


**双参数二级计分模型的参数估计**



试题作答正确概率为$P$：
$$
P = \frac{e^{a \theta  + b}}{ 1 + e^{a \theta  + b} }
$$

> $\theta$ 是潜在特质
>
> $z = a \theta  + b$，$z$ 是潜在特质的线性变换
>
> $a$ 是区分度（或斜率）
>
> $b$ 是阈值（或截距）
>
> $- \frac{b}{a}$ 是难度（或通俗度）



$a = 1$ 时，P函数就是**Rasch函数**

> - **如果能力和问题的难度一样，正确率是 .50**



自适应组卷

> GMAT和GRE都是自适应考试，所谓自适应考试的意思就是，当你做完了第一道题之后，系统会跟根据你这一道题回答的情况，来决定你下一道题的难度。如果你这一题答对了，那么，系统就会认为你的水平比较高，下一道题的难度就会相应高一些；如果你一直答对，你越做到后面，题目的难度就越高。相对应的，如果你这一题的答案答错了，系统就会认为你的水平稍微菜一点，那么下一题的难度，就会相对应的低一些，如果你一直答错，难度就会一直降低。当然了，因为每一题的难度系数不同，所以，分值也不一样。如果你答的题，难度系数都比较高，虽然你错的题目的数量会相对多一些，但是，你的总分可能并不低，因为，你做的全是很有难度的题。同理，如果你答的题，难度系数都很低，虽然你答对的题目数量很多，但总分可能并不高，因为，你答对的，都是一些很简单的题。

![image-20201118142656736](项目反应理论.assets/image-20201118142656736.png)





选择**与人的能力相匹配的题目**

> 题目能力与考生能力匹配



**计算机自适应测验的优势**

> 高能力考生无须回答过多的简单题，低能力考生也无须回答太多难题，通过较少题目就能对考生的能力水平做出有效的测量



**基于知识图谱的自适应推荐系统**

> 1. **减少刷题数量**
> 2. **明确刷题优先级**：尽量去掉无效刷题时间，并进一步提高学习效率
> 3. 知识点的掌握程度的定量测评





题目的覆盖面，能覆盖多少能力范围的学生

衡量哪些试题是不是没起到太大的作用，完全被其他试题覆盖了。我们就可以尝试删去这些没啥评价能力的试题



抽题具体方法
a分层将测验分为n个阶段,从每个阶段抽出若干道试题,其中第一阶段是依据难度随机抽题,不计算被试特质,其他阶段依据被试特质估计值进行抽题
第一阶段的抽题策略是:假如第一阶段需要抽出5道题,则依据难度大小把第一层次的试题分成5份,然后从这五份子试题池中抽出5道题, 从而保证随机抽题的难度均匀分布,避免发生所抽试题太简单或太难
其他阶段的抽题策略是:首先计算被试的特质估计值,依据估计值,寻找估计值与试题难度值差值绝对值最小的30道题,这30道题形成一个子试题池, 计算这30道题的总共使用次数,然后计算每道题的使用次数与总共使用次数之比,以及信息函数值, 计算前者与后者的比值, 最后抽出比值最小的题. 这样抽题的好处一个是计算上节省资源,不用计算每道题的信息函数,只需计算试题池中的题目, 既考虑了测验效率又降低了试题曝光率
参数估计方法
极大后验均值估计(MAP)

极大后验均值估计与其他流行方法比较
相比极大似然估计(MLE),极大后验均值估计对初值不敏感, 估计也比较稳健
相比期望后验均值(EAP), 极大后验均值的计算速度要慢了一倍(python的for循环非常耗时)...
经10000测试数据(每个数据包含10道题)检验, CORE M 0.8g的CPU下, 期望后验均值的时间是大约是2.4秒, 极大后验均值的时间大约是6秒





## 经典测量理论CTT

经典测量理论（Classical Test Theory，简称CTT）发端于100年前



只关心总得分，单维度

#### 仅仅以试卷总得分代表考生的能力，无法区分相同成绩考生的能力

1. 一个考生成绩越高，说明他的能力越高
2. 全体考生成绩越高，说明测验越简单



## 项目反应理论（IRT）

项目反应理论也称潜在特质理论

在测验中，**潜在特质一般是指潜在的能力**



IRT有三条基本假设，一是单维性，二是局部独立性，三是**项目反应函数假设**，但其实前两条假设可有可无（例如mirt理论打破了单维性假设，题组反应理论打破了局部独立性假设），核心的是第三个假设





答对10道难度为1的题目，获得的能力值依然是1，另一考生答对1道难度为8的题目，能力值则为8

> 低能力者很难再通过重复做对简单题拿到高分
>
> 感觉做对了题还不给分，是不是有猫腻啊？你说题目难就难，做对得分就高？我还觉得不难呢！



SAT, TOFEL, GRE等考试都是**以项目反应理论为基础构建的测验**，通过考生答对的题目的难度来确定考生的能力



**Rasch模型**就描述了考生能力、题目难度与考生是否答对之间的关系

> θ 代表考生能力，b 代表题目难度，P（θ）代表能力为θ 的考生在该题目上作答正确的概率



**根据每个学生的做答反应模式，来估算每个学生的能力值。但项目反应理论能估算每道试题的参数，比如试题的难度，区分度，猜测度等**



[如何通俗的理解项目反应理论？](https://www.zhihu.com/question/24671541)





以往的测验中（经典测量理论CTT），考试成绩就代表了某个考生的能力，考生成绩越高，说明其能力越高；群体得分也代表的某次测验的难易程度，整体得分越高，说明测验越简单。

容易发现，这种测量方式并不准确，成绩相同的考生能力真的相同吗？

不同的群体参加测验得到的分数不同，那怎样根据群体得分确定测验难度呢？

为了解决以往测验的问题，项目反应理论（IRT）不再简单的以试卷总得分代表考生的能力，只有考生答对了高能力才能答对的较难的题目时，才认为考生具有较高的能力。

也就是说，考生答对的题目难度是判断考生能力的标准。某考生答对10道难度为1的题目，获得的能力值依然是1，另一考生答对1道难度为8的题目，能力值则为8。

项目反应理论（IRT）构建了一整套数学模型来描述考生能力、题目特性与考生作答之间的关系，例如最基本的IRT模型—Rasch模型就描述了考生能力、题目难度与考生是否答对之间的关系





其中θ代表考生能力，b代表题目难度，P（θ）代表能力为θ的考生在该题目上作答正确的概率。根据该模型作图如下：横轴代表考生能力；纵轴代表正确作答概率。蓝色曲线的含义是，能力越高的考生作答正确的概率越高。（那题目难度体现在哪里呢？我们看原始公式中，当考生能力θ等于题目难度b时，P值为0.5。IRT中难度的定义就是考生答对概率为0.5时，对应的能力值。在这里可以看出这道题的难度基本是0。）

当然还有其他很多更复杂的模型适用于更精确的测量不同的题型，SAT, TOFEL, GRE等考试都是以项目反应理论为基础构建的测验，通过考生答对的题目的难度来确定考生的能力。

如果以后我国推行基于IRT的测验，能力高低会测量的更精确，低能力者很难再通过重复做对简单题拿到高分了。实际上这是更加公平的，不过对于这种测验方式，很多人还是很难接受，因为不了解背后的测量理论的科学性，就会感觉做对了题还不给分，是不是有猫腻啊？你说题目难就难，做对得分就高？我还觉得不难呢！







