



```js
SELECT order_id, order_date, json_extract(customer, '$.name') AS cusName,json_extract(customer, '$.city') AS cusCity FROM read_json_auto('orders.json')
```



```
SELECT sum(od.quantity*od.price) amount
FROM read_json_auto('orders.json') AS o,
LATERAL UNNEST(o.order_details) AS t(od),
LATERAL UNNEST([od.product]) AS t(p)
WHERE p.category = 'Electronics'
```



```

SELECT
    o.order_id, 
    LIST_FILTER(o.order_details, x -> x.product.category = 'Electronics') AS order_details
FROM read_json_auto(orders.json') AS o
WHERE 
    ARRAY_LENGTH(LIST_FILTER(o.order_details, x -> x.product.category = 'Electronics')) > 0
    AND SUM(
        LIST_FILTER(o.order_details, x -> x.product.category = 'Electronics') -> 
            (x -> x.price * x.quantity)
    ) > 200;

```





# parquet ä¼˜ç¼ºç‚¹



Parquet æ˜¯ä¸€ç§åˆ—å¼å­˜å‚¨æ ¼å¼ï¼Œè€Œ DuckDB æ˜¯ä¸€ä¸ªè¿›ç¨‹å†… OLAPï¼ˆè”æœºåˆ†æå¤„ç†ï¼‰æ•°æ®åº“ã€‚ä¸¤è€…ç»“åˆé€šå¸¸è¢«è®¤ä¸ºæ˜¯æ•°æ®åˆ†æé¢†åŸŸçš„â€œé»„é‡‘æ­æ¡£â€ã€‚

ä»¥ä¸‹æ˜¯åœ¨ DuckDB ä¸­ä½¿ç”¨ Parquet æ ¼å¼çš„ä¸»è¦ä¼˜ç¼ºç‚¹ï¼š

**ä¼˜ç‚¹ (Pros)**

1. æé«˜çš„æŸ¥è¯¢æ€§èƒ½ (Columnar Synergy)
   
   - åˆ—å¼å¯¹é½ ï¼šDuckDB çš„å†…éƒ¨æ‰§è¡Œå¼•æ“æ˜¯å‘é‡åŒ–ï¼ˆVectorizedï¼‰å’Œåˆ—å¼çš„ï¼Œè¿™ä¸ Parquet çš„ç‰©ç†å­˜å‚¨å¸ƒå±€å®Œç¾å¥‘åˆã€‚
   - é›¶æ‹·è´/ç›´æ¥è¯»å– ï¼šDuckDB å¯ä»¥ç›´æ¥åœ¨ Parquet æ–‡ä»¶ä¸Šè¿è¡ŒæŸ¥è¯¢ï¼Œè€Œæ— éœ€å…ˆå°†æ•°æ®â€œå¯¼å…¥â€æˆ–å¤åˆ¶åˆ°æ•°æ®åº“å†…éƒ¨å­˜å‚¨ä¸­ã€‚
   - æŠ•å½±ä¸‹æ¨ (Projection Pushdown) ï¼šå¦‚æœä½ çš„æŸ¥è¯¢åªæ¶‰åŠè¡¨ä¸­çš„å‡ åˆ—ï¼ˆä¾‹å¦‚ SELECT col_a FROM table ï¼‰ï¼ŒDuckDB åªä¼šä» Parquet æ–‡ä»¶ä¸­è¯»å– col_a çš„æ•°æ®å—ï¼Œæå¤§å‡å°‘ I/Oã€‚
2. æ™ºèƒ½è¿‡æ»¤ (Filter Pushdown)
   
   - DuckDB åˆ©ç”¨ Parquet æ–‡ä»¶å¤´ä¸­çš„å…ƒæ•°æ®ï¼ˆå¦‚ Row Groups çš„ min/max ç»Ÿè®¡ä¿¡æ¯ï¼‰æ¥è·³è¿‡æ— å…³çš„æ•°æ®å—ã€‚
   - ä¾‹å¦‚æŸ¥è¯¢ WHERE age > 30 ï¼Œå¦‚æœæŸä¸ª Row Group çš„ max_age æ˜¯ 25ï¼ŒDuckDB ä¼šå®Œå…¨è·³è¿‡è¯»å–è¯¥å—ã€‚
3. å­˜å‚¨æ•ˆç‡ä¸å‹ç¼©
   
   - Parquet é’ˆå¯¹æ¯ä¸€åˆ—çš„æ•°æ®ç±»å‹ä½¿ç”¨ç‰¹å®šçš„ç¼–ç å’Œå‹ç¼©ç®—æ³•ï¼ˆå¦‚ Snappy, Zstd, RLE, Dictionary Encodingï¼‰ã€‚
   - ç›¸æ¯” CSV æˆ– JSONï¼ŒParquet æ–‡ä»¶é€šå¸¸æ›´å°ï¼Œä¸ä»…èŠ‚çœç£ç›˜ç©ºé—´ï¼Œè¿˜èƒ½å‡å°‘è¯»å–æ—¶çš„ç£ç›˜ I/O å¸¦å®½å‹åŠ›ã€‚
4. ç”Ÿæ€äº’æ“ä½œæ€§
   
   - Parquet æ˜¯é€šç”¨çš„æ ‡å‡†æ ¼å¼ã€‚ä½ å¯ä»¥åœ¨ Python (Pandas/Polars)ã€Sparkã€AWS S3 ç­‰ç¯å¢ƒä¸­ç”Ÿæˆ Parquet æ–‡ä»¶ï¼Œç„¶åç›´æ¥ç”¨ DuckDB æŸ¥è¯¢ï¼Œæ— éœ€è½¬æ¢æ ¼å¼ã€‚
5. æ”¯æŒå¤æ‚æ•°æ®ç±»å‹
   
   - ç›¸æ¯” CSVï¼ŒParquet åŸç”Ÿæ”¯æŒåµŒå¥—ç»“æ„ï¼ˆLists, Structs, Mapsï¼‰ï¼ŒDuckDB å¯¹è¿™äº›å¤æ‚ç±»å‹çš„æ”¯æŒä¹Ÿè¶Šæ¥è¶Šå®Œå–„ã€‚



**ç¼ºç‚¹ (Cons)**

1. ä¸æ”¯æŒé«˜æ•ˆçš„æ›´æ–°/åˆ é™¤ (Immutability)
   
   - æœ€å¤§ç—›ç‚¹ ï¼šParquet æ–‡ä»¶è®¾è®¡ä¸ºâ€œä¸€æ¬¡å†™å…¥ï¼Œå¤šæ¬¡è¯»å–â€ï¼ˆWrite-Once-Read-Manyï¼‰ã€‚
   - å¦‚æœä½ éœ€è¦ä¿®æ”¹æˆ–åˆ é™¤ä¸€è¡Œæ•°æ®ï¼Œé€šå¸¸éœ€è¦é‡å†™æ•´ä¸ªæ–‡ä»¶ï¼ˆæˆ–è‡³å°‘é‡å†™ç›¸å…³çš„ Row Groupï¼‰ã€‚
   - ä¸é€‚åˆ OLTP ï¼šå¦‚æœä½ çš„åœºæ™¯éœ€è¦é¢‘ç¹çš„å•è¡Œæ’å…¥ã€æ›´æ–°æˆ–äº‹åŠ¡å¤„ç†ï¼ŒåŸç”Ÿ Parquet å¹¶ä¸é€‚åˆï¼ˆDuckDB è‡ªå·±çš„ .db æ ¼å¼æ›´é€‚åˆè¿™ç§æ··åˆè´Ÿè½½ï¼‰ã€‚
2. å°æ–‡ä»¶é—®é¢˜ (Small File Problem)
   
   - å¦‚æœæ•°æ®è¢«åˆ†å‰²æˆæˆåƒä¸Šä¸‡ä¸ªéå¸¸å°çš„ Parquet æ–‡ä»¶ï¼ˆä¾‹å¦‚æ¯ä¸ªæ–‡ä»¶åªæœ‰å‡  KBï¼‰ï¼Œè¯»å–æ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚
   - è¿™æ˜¯å› ä¸ºæ‰“å¼€æ–‡ä»¶ã€è§£æå…ƒæ•°æ®çš„å¼€é”€è¶…è¿‡äº†è¯»å–å®é™…æ•°æ®çš„å¼€é”€ã€‚DuckDB å¤„ç†å•ä¸ªå¤§æ–‡ä»¶ï¼ˆæˆ–é€‚é‡çš„å¤§æ–‡ä»¶ï¼‰çš„æ•ˆç‡è¿œé«˜äºå¤„ç†æµ·é‡å°æ–‡ä»¶ã€‚
3. å†™å…¥å¼€é”€
   
   - ç”Ÿæˆ Parquet æ–‡ä»¶éœ€è¦ CPU è¿›è¡Œç¼–ç å’Œå‹ç¼©ã€‚ä¸ç®€å•çš„è¿½åŠ å†™å…¥ CSV ç›¸æ¯”ï¼Œå†™å…¥ Parquet ä¼šæ¶ˆè€—æ›´å¤šçš„ CPU èµ„æºå’Œæ—¶é—´ï¼ˆå°½ç®¡è¿™é€šå¸¸æ˜¯å€¼å¾—çš„ï¼Œå› ä¸ºè¯»å–é€Ÿåº¦ä¼šå¿«å¾—å¤šï¼‰ã€‚
4. ä¸é€‚åˆæµå¼è¿½åŠ 
   
   - ç”±äº Parquet æ–‡ä»¶åŒ…å«å°¾éƒ¨å…ƒæ•°æ®ï¼ˆFooterï¼‰ï¼Œä½ ä¸èƒ½åƒå†™æ—¥å¿—ä¸€æ ·ç®€å•åœ°é€šè¿‡ cat å‘½ä»¤æŠŠæ–°æ•°æ®è¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾ã€‚å¿…é¡»ç”±ä¸“é—¨çš„åº“æ¥å¤„ç†æ–‡ä»¶çš„åˆå¹¶æˆ–ç”Ÿæˆæ–°æ–‡ä»¶ã€‚



**æ€»ç»“å»ºè®®**

- ä½¿ç”¨åœºæ™¯ ï¼šå†å²æ•°æ®åˆ†æã€æ•°æ®æ¹–æŸ¥è¯¢ã€ä¸€æ¬¡å†™å…¥å¤šæ¬¡è¯»å–çš„æŠ¥è¡¨ç”Ÿæˆã€è·¨å·¥å…·æ•°æ®äº¤æ¢ã€‚
- é¿å…åœºæ™¯ ï¼šéœ€è¦å®æ—¶æ›´æ–°ç”¨æˆ·çŠ¶æ€ã€é«˜é¢‘äº‹åŠ¡äº¤æ˜“ç³»ç»Ÿã€æµå¼æ—¥å¿—çš„å®æ—¶è¿½åŠ å†™å…¥ï¼ˆé™¤éåˆ†æ‰¹æ¬¡å†™å…¥ï¼‰ã€‚
åœ¨ DuckDB ä¸­ï¼Œæœ€å¸¸è§çš„æ¨¡å¼æ˜¯ï¼šä½¿ç”¨ DuckDB çš„å†…éƒ¨æ ¼å¼ï¼ˆ .db ï¼‰å¤„ç†çƒ­æ•°æ®æˆ–éœ€è¦æ›´æ–°çš„æ•°æ®ï¼Œå®šæœŸå°†å†·æ•°æ®æˆ–å½’æ¡£æ•°æ®å¯¼å‡ºä¸º Parquet ä»¥èŠ‚çœç©ºé—´å¹¶ä¾›å…¶ä»–å·¥å…·ä½¿ç”¨ã€‚





# æ›¿æ¢ Json åˆé€‚å—

å¦‚æœä½ çš„ç›®æ ‡æ˜¯ æ›¿æ¢ JSON æ•°æ®æ ¼å¼ ï¼ˆç‰¹åˆ«æ˜¯ç”¨äºå­˜å‚¨ã€æ—¥å¿—è®°å½•æˆ–æ•°æ®äº¤æ¢çš„ JSONï¼‰ï¼Œä½¿ç”¨ DuckDB æ˜¯ éå¸¸åˆé€‚ çš„ï¼Œç”šè‡³å¯ä»¥è¯´æ˜¯ç›®å‰å¸‚é¢ä¸Š æ€§ä»·æ¯”æœ€é«˜ çš„æ–¹æ¡ˆä¹‹ä¸€ã€‚

ä»¥ä¸‹æ˜¯ä¸ºä»€ä¹ˆç”¨ DuckDBï¼ˆåŠå…¶åŸç”Ÿæ ¼å¼æˆ–é…åˆ Parquetï¼‰æ›¿æ¢ JSON æ˜¯ä¸€ä¸ªå·¨å¤§å‡çº§çš„åŸå› ï¼š



1. å­˜å‚¨ç©ºé—´ï¼šä»â€œè†¨èƒ€â€åˆ°â€œæè‡´å‹ç¼©â€
- JSON ï¼šæ˜¯åŸºäºæ–‡æœ¬çš„ï¼Œéå¸¸å•°å—¦ã€‚æ¯ä¸ªå­—æ®µååœ¨æ¯ä¸€è¡Œéƒ½è¦é‡å¤ä¸€éï¼ˆä¾‹å¦‚ {"id": 1, "name": "a"} , {"id": 2, "name": "b"} ... "id" å’Œ "name" è¢«å­˜å‚¨äº†æ— æ•°æ¬¡ï¼‰ã€‚
- DuckDB ï¼š
  - å®ƒæ˜¯åˆ—å¼å­˜å‚¨ï¼Œç›¸åŒçš„å­—æ®µååªå­˜ä¸€æ¬¡å…ƒæ•°æ®ã€‚
  - å®ƒä¼šè‡ªåŠ¨æ£€æµ‹æ¯ä¸€åˆ—çš„æœ€ä½³å‹ç¼©ç®—æ³•ã€‚
  - æ•ˆæœ ï¼šé€šå¸¸èƒ½å°† JSON æ–‡ä»¶çš„å¤§å°å‹ç¼© 5å€åˆ° 20å€ ã€‚10GB çš„ JSON æ—¥å¿—è½¬å…¥ DuckDB å¯èƒ½å˜æˆå‡ ç™¾ MBã€‚




2. è¯»å–æ€§èƒ½ï¼šå‘Šåˆ«â€œè§£æåœ°ç‹±â€
- JSON ï¼šè®¡ç®—æœºå¤„ç† JSON æœ€æ…¢çš„æ­¥éª¤ä¸æ˜¯â€œè¯»å–ç£ç›˜â€ï¼Œè€Œæ˜¯ è§£æï¼ˆParsingï¼‰ ã€‚CPU å¿…é¡»é€ä¸ªå­—ç¬¦æ‰«æ { , } , : , " æ‰èƒ½ç†è§£æ•°æ®ç»“æ„ã€‚è¿™æ˜¯æåº¦æ¶ˆè€— CPU çš„ã€‚
- DuckDB ï¼šæ•°æ®æ˜¯äºŒè¿›åˆ¶æ ¼å¼ï¼Œä¸”æŒ‰åˆ—æ’åˆ—ã€‚
  - æ— éœ€è§£ææ–‡æœ¬ã€‚
  - å¯ä»¥ç›´æ¥å°†æ•°æ®å— memcpy åˆ°å†…å­˜ä¸­ã€‚
  - æ•ˆæœ ï¼šæŸ¥è¯¢é€Ÿåº¦é€šå¸¸æ¯”ç›´æ¥å¤„ç† JSON å¿« 10å€åˆ° 100å€ ã€‚



3. æŸ¥è¯¢èƒ½åŠ›ï¼šSQL vs. è„šæœ¬å¾ªç¯
- JSON ï¼šå¦‚æœä½ æƒ³ä» 100 ä¸‡è¡Œ JSON ä¸­æ‰¾å‡º status: "error" çš„è®°å½•å¹¶æŒ‰ date èšåˆï¼Œä½ é€šå¸¸éœ€è¦å†™ Python/Node.js è„šæœ¬æˆ–ç”¨ jq å·¥å…·ï¼Œè¿™å¾ˆæ…¢ä¸”å®¹æ˜“å‡ºé”™ã€‚
- DuckDB ï¼š
  - ä½ å¯ä»¥ç›´æ¥ç”¨ SQLï¼š SELECT date, count(*) FROM logs WHERE status = 'error' GROUP BY date ã€‚
  - å³ä½¿æ•°æ®ç»“æ„å¾ˆå¤æ‚ï¼ˆåµŒå¥—ï¼‰ï¼ŒDuckDB ä¹Ÿæ”¯æŒ struct å’Œ list ç±»å‹ï¼Œæˆ–è€…ç›´æ¥ä¿ç•™ä¸º JSON ç±»å‹åˆ—è¿›è¡ŒæŸ¥è¯¢ã€‚



4. çµæ´»æ€§ï¼šDuckDB å¯¹ JSON çš„åŸç”Ÿæ”¯æŒ
DuckDB æœ‰ä¸€ä¸ªéå¸¸å¼ºå¤§çš„ç‰¹æ€§ï¼š å®ƒä¸å¼ºè¿«ä½ ç«‹åˆ»å®šä¹‰å®Œç¾çš„ Schema ã€‚

å¦‚æœä½ çš„ JSON å­—æ®µç»å¸¸å˜ï¼ˆè¿™ä¹Ÿæ˜¯å¤§å®¶çˆ±ç”¨ JSON çš„åŸå› ï¼‰ï¼ŒDuckDB æœ‰ä¸¤ç§å¤„ç†æ–¹å¼ï¼š

1. è‡ªåŠ¨æ¨æ–­ï¼ˆAuto Detectionï¼‰ ï¼šå®ƒä¼šæ‰«æ JSON æ–‡ä»¶ï¼Œè‡ªåŠ¨çŒœå‡ºæœ€åˆç†çš„è¡¨ç»“æ„ã€‚
   ```
   SELECT * FROM read_json_auto('data.json');
   ```
2. JSON æ•°æ®ç±»å‹ ï¼šä½ å¯ä»¥æŠŠåŸæœ¬å¤æ‚çš„åµŒå¥—éƒ¨åˆ†ç›´æ¥å­˜ä¸ºä¸€ä¸ª JSON ç±»å‹çš„åˆ—ï¼Œæ—¢äº«å—äº†å‹ç¼©ï¼Œåˆèƒ½ç”¨ JSON å‡½æ•°æŸ¥è¯¢ã€‚
   ```
   -- å³ä½¿ data åˆ—æ˜¯å¤æ‚çš„åµŒå¥— JSONï¼Œä¹Ÿèƒ½æé€ŸæŸ¥è¯¢
   SELECT data->>'user_id' FROM my_table;
   ```
   ä»€ä¹ˆæ—¶å€™ ä¸å»ºè®® æ›¿æ¢ JSONï¼Ÿ
   è™½ç„¶ DuckDB å¾ˆå¼ºï¼Œä½†å¦‚æœä½ çš„åœºæ™¯æ˜¯ä»¥ä¸‹å‡ ç§ï¼Œä¿ç•™ JSON å¯èƒ½æ›´å¥½ï¼š

1. å‰ç«¯ç›´æ¥æ¶ˆè´¹ ï¼šå¦‚æœæ•°æ®æ˜¯ç›´æ¥å‘ç»™ Web æµè§ˆå™¨æˆ–ç§»åŠ¨ç«¯ App çš„ API å“åº”ï¼ŒJSON æ˜¯æ ‡å‡†ï¼ŒDuckDB æ ¼å¼æµè§ˆå™¨è¯»ä¸æ‡‚ã€‚
2. é…ç½®æ–‡ä»¶ ï¼šå¦‚æœæ–‡ä»¶éœ€è¦äººå·¥ç”¨è®°äº‹æœ¬æ‰“å¼€ã€é˜…è¯»å’Œä¿®æ”¹ï¼ˆå¦‚ config.json ï¼‰ï¼Œä¸è¦æ¢ã€‚
3. å•æ¡è®°å½•çš„é«˜é¢‘è¯»å†™ ï¼šå¦‚æœä½ æ˜¯ç”¨ä½œ Key-Value å­˜å‚¨ï¼ˆåƒ Redis æˆ– MongoDB é‚£æ ·ï¼‰ï¼ŒæŒ‰ ID é¢‘ç¹å­˜å–å•æ¡è®°å½•ï¼ŒDuckDB ä¸å¤ªé€‚åˆï¼ˆå®ƒæ˜¯ä¸ºæ‰¹é‡åˆ†æè®¾è®¡çš„ï¼‰ã€‚
æœ€ä½³å®è·µå»ºè®®
å¦‚æœä½ å†³å®šæ›¿æ¢ï¼Œæ¨èçš„è·¯å¾„æ˜¯ï¼š

åŸå§‹ JSON -> [DuckDB æ¸…æ´—/è½¬æ¢] -> Parquet æ–‡ä»¶

è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š

- Parquet æ˜¯å‹ç¼©åçš„äºŒè¿›åˆ¶ï¼Œä¿ç•™äº†åˆ—å¼ä¼˜åŠ¿ã€‚
- DuckDB å¯ä»¥ç§’çº§æŸ¥è¯¢ Parquetã€‚
- å¦‚æœä»¥åä½ éœ€è¦æŠŠæ•°æ®ç»™ Sparkã€Pandas æˆ– ClickHouse ç”¨ï¼ŒParquet æ˜¯é€šç”¨è´§å¸ï¼Œå¤§å®¶éƒ½èƒ½è¯»ï¼Œè€Œ DuckDB çš„ .db æ–‡ä»¶åªæœ‰ DuckDB èƒ½è¯»ã€‚





# å­˜åµŒå…¥å‘é‡åˆé€‚å—



éå¸¸åˆé€‚ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­å°è§„æ¨¡ï¼ˆç™¾ä¸‡çº§ä»¥ä¸‹ï¼‰çš„æ•°æ®é›†åœºæ™¯ä¸‹ã€‚

DuckDB æœ€è¿‘åœ¨å‘é‡å­˜å‚¨å’Œæ£€ç´¢é¢†åŸŸéå¸¸ç«ï¼Œå› ä¸ºå®ƒé€šè¿‡**æš´åŠ›è®¡ç®—ï¼ˆBrute-forceï¼‰**å®ç°äº†æƒŠäººçš„æ€§èƒ½ï¼Œä¸”ä¸éœ€è¦å¤æ‚çš„ç´¢å¼•ç»´æŠ¤ã€‚

ä»¥ä¸‹æ˜¯ DuckDB ä½œä¸ºå‘é‡æ•°æ®åº“çš„è¯¦ç»†åˆ†æï¼š




1. æ ¸å¿ƒä¼˜åŠ¿ï¼šç®€å•ä¸”å¼ºå¤§çš„â€œç²¾ç¡®æœç´¢â€
ä¸»æµå‘é‡æ•°æ®åº“ï¼ˆå¦‚ Milvus, Pinecone, Faissï¼‰é€šå¸¸ä½¿ç”¨ ANNï¼ˆè¿‘ä¼¼æœ€è¿‘é‚»ï¼‰ ç®—æ³•ã€‚ä¸ºäº†é€Ÿåº¦ï¼Œå®ƒä»¬ç‰ºç‰²äº†ä¸€ç‚¹ç²¾åº¦ï¼Œå¹¶ä¸”éœ€è¦æ„å»ºåºå¤§çš„ HNSW ç´¢å¼•ï¼Œè¿™å¾ˆå å†…å­˜ä¸”ç»´æŠ¤éº»çƒ¦ã€‚

DuckDB çš„ç­–ç•¥ä¸åŒï¼š

- åŸºäº SIMD çš„æš´åŠ›æ‰«æ ï¼šDuckDB åˆ©ç”¨ç°ä»£ CPU çš„æŒ‡ä»¤é›†ï¼ˆAVX-512 ç­‰ï¼‰å¹¶è¡Œè®¡ç®—å‘é‡è·ç¦»ã€‚
- æ— éœ€ç´¢å¼• ï¼šå®ƒä¸éœ€è¦é¢„å…ˆå»ºç«‹å¤æ‚çš„å‘é‡ç´¢å¼•ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥éšæ—¶æ’å…¥æ•°æ®ï¼Œéšæ—¶æŸ¥è¯¢ï¼Œæ²¡æœ‰â€œé‡å»ºç´¢å¼•â€çš„å¼€é”€ã€‚
- ç²¾ç¡®ç»“æœ ï¼šå› ä¸ºæ˜¯å…¨è¡¨æ‰«æï¼Œå®ƒè¿”å›çš„æ˜¯æ•°å­¦ä¸Š ç»å¯¹ç²¾ç¡® çš„æœ€è¿‘é‚»ï¼ˆKNNï¼‰ï¼Œè€Œä¸æ˜¯è¿‘ä¼¼å€¼ã€‚
æ€§èƒ½è¡¨ç° ï¼š
åœ¨ 100 ä¸‡æ¡ 768 ç»´ï¼ˆOpenAI Embedding æ ‡å‡†ï¼‰å‘é‡çš„æ•°æ®é›†ä¸Šï¼ŒDuckDB çš„å…¨è¡¨æ‰«æé€šå¸¸èƒ½åœ¨ 0.1ç§’ ~ 0.5ç§’ å†…å®Œæˆã€‚å¯¹äºå¤§å¤šæ•° RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨æ¥è¯´ï¼Œè¿™ä¸ªå»¶è¿Ÿå®Œå…¨å¯ä»¥æ¥å—ã€‚




2. åŸç”Ÿæ”¯æŒçš„æ•°æ®ç±»å‹
DuckDB æœ‰ä¸“é—¨çš„ ARRAY ç±»å‹ï¼Œéå¸¸é€‚åˆå­˜å‘é‡ï¼š

```
-- åˆ›å»ºä¸€ä¸ªå­˜ 3 ç»´å‘é‡çš„è¡¨
CREATE TABLE embeddings (
    id INTEGER,
    text VARCHAR,
    vec FLOAT[3]  -- å›ºå®šé•¿åº¦çš„æ•°ç»„ï¼Œå­˜å‚¨æ•ˆç‡æé«˜
);

-- æ’å…¥æ•°æ®
INSERT INTO embeddings VALUES (1, 'apple', [0.1, 0.5, 0.9]);
INSERT INTO embeddings VALUES (2, 'banana', [0.2, 0.4, 0.8]);

-- å‘é‡æœç´¢ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
-- æŸ¥æ‰¾ä¸ [0.1, 0.5, 0.9] æœ€ç›¸ä¼¼çš„å‰ 3 ä¸ªç»“æœ
SELECT text, array_cosine_similarity(vec, [0.1, 0.5, 0.9]::FLOAT[3]) as score
FROM embeddings
ORDER BY score DESC
LIMIT 3;
```



3. â€œæ··åˆæŸ¥è¯¢â€ï¼ˆHybrid Searchï¼‰çš„ç‹è€…
è¿™æ˜¯ DuckDB ç›¸æ¯”ä¸“ç”¨å‘é‡æ•°æ®åº“æœ€å¤§çš„ä¼˜åŠ¿ã€‚

åœ¨å®é™…ä¸šåŠ¡ä¸­ï¼Œä½ å¾ˆå°‘åªåšçº¯å‘é‡æœç´¢ï¼Œé€šå¸¸éœ€è¦ç»“åˆå…ƒæ•°æ®è¿‡æ»¤ã€‚ä¾‹å¦‚ï¼šâ€œ æŸ¥æ‰¾æœ€è¿‘ 7 å¤©å†…ã€åˆ†ç±»ä¸ºâ€˜è´¢ç»â€™ä¸”ä¸è¿™å¥è¯è¯­ä¹‰ç›¸ä¼¼çš„æ–°é—» â€ã€‚

- ä¸“ç”¨å‘é‡åº“ ï¼šå…ƒæ•°æ®è¿‡æ»¤é€šå¸¸è¾ƒå¼±ï¼ˆPre-filter æˆ– Post-filterï¼‰ï¼Œæ€§èƒ½ä¸ç¨³å®šã€‚
- DuckDB ï¼šå®ƒæœ¬èº«å°±æ˜¯æœ€å¼ºçš„ SQL åˆ†æå¼•æ“ã€‚
  ```
  SELECT text, array_cosine_similarity(vec, query_vec) as score
  FROM documents
  WHERE category = 'finance'       -- æé€Ÿçš„åˆ—å¼è¿‡æ»¤
    AND date > '2023-01-01'        -- æ ‡å‡† SQL è¿‡æ»¤
  ORDER BY score DESC
  LIMIT 5;
  â€‹``` DuckDB ä¼šå…ˆåˆ©ç”¨åˆ—å¼å­˜å‚¨çš„ä¼˜åŠ¿å¿«é€Ÿè¿‡æ»¤æ‰ä¸æ»¡è¶³ WHERE æ¡ä»¶çš„è¡Œï¼Œåªå¯¹å‰©ä¸‹çš„è¡Œè®¡ç®—å‘é‡è·ç¦»ï¼Œé€Ÿåº¦é£å¿«ã€‚
  ```



4. ä»€ä¹ˆæ—¶å€™ ä¸åˆé€‚ï¼Ÿ
å°½ç®¡ DuckDB å¾ˆå¼ºï¼Œä½†åœ¨ä»¥ä¸‹åœºæ™¯è¯·æ…ç”¨ï¼š

1. è¶…å¤§è§„æ¨¡æ•°æ®ï¼ˆ> 500ä¸‡å‘é‡ï¼‰ ï¼š
   - å½“æ•°æ®é‡è¾¾åˆ°åƒä¸‡çº§æ—¶ï¼Œæš´åŠ›æ‰«æä¼šå˜å¾—å¤ªæ…¢ï¼ˆå¯èƒ½éœ€è¦å‡ ç§’é’Ÿï¼‰ã€‚è¿™æ—¶å€™å¿…é¡»ç”¨ HNSW ç´¢å¼•ï¼Œè€Œ DuckDB ç›®å‰çš„åŸç”Ÿç´¢å¼•æ”¯æŒä¸å¦‚ä¸“ç”¨åº“æˆç†Ÿï¼ˆè™½ç„¶æœ‰ vss æ‰©å±•ï¼Œä½†è¿˜åœ¨æ—©æœŸé˜¶æ®µï¼‰ã€‚
2. é«˜å¹¶å‘å®æ—¶æŸ¥è¯¢ ï¼š
   - DuckDB æ˜¯å•æœºè¿›ç¨‹å†…çš„ï¼Œä¸é€‚åˆæ¯ç§’å‡ åƒæ¬¡ï¼ˆQPS > 1000ï¼‰çš„å¹¶å‘æŸ¥è¯¢è¯·æ±‚ã€‚å®ƒæ›´é€‚åˆåˆ†ææˆ–ä½é¢‘é«˜ååçš„ RAGã€‚
3. é¢‘ç¹çš„å•ç‚¹æ›´æ–° ï¼š
   - å’Œä¹‹å‰æåˆ°çš„ Parquet ä¸€æ ·ï¼ŒDuckDB ä¸å–œæ¬¢é¢‘ç¹ä¿®æ”¹å•è¡Œå‘é‡ã€‚å®ƒå–œæ¬¢æ‰¹é‡å†™å…¥ã€‚


æ€»ç»“
- æ¨èåœºæ™¯ ï¼šä¸ªäººçŸ¥è¯†åº“ï¼ˆObsidian/Notion æ’ä»¶ï¼‰ã€æœ¬åœ° RAG åº”ç”¨ã€æ•°æ®åˆ†æå¸ˆçš„è¯­ä¹‰æœç´¢å·¥å…·ã€ç¦»çº¿æ•°æ®æ¸…æ´—ç®¡é“ã€‚
- æ›¿ä»£æ–¹æ¡ˆ ï¼šå¦‚æœä½ çš„æ•°æ®é‡çœŸçš„å¾ˆå¤§ï¼ˆåƒä¸‡çº§ï¼‰ï¼Œæˆ–è€…éœ€è¦æé«˜çš„åœ¨çº¿å¹¶å‘ï¼Œè¯·ä½¿ç”¨ Qdrant ã€ Milvus æˆ– pgvector ï¼ˆPostgreSQL æ’ä»¶ï¼‰ã€‚



# å…¨æ–‡æ£€ç´¢

```

ftså’Œvssæ’ä»¶ä»¥åŠvsssentence-transformers

```





# LanceDB ä¸“ä¸ºå‘é‡è®¾è®¡



è¿™æ˜¯ä¸€ä¸ªéå¸¸ç²¾å½©çš„å¯¹æ¯”ã€‚ç®€å•æ¥è¯´ï¼š DuckDB æ˜¯æ•°æ®åˆ†æé¢†åŸŸçš„ç‘å£«å†›åˆ€ï¼Œè€Œ LanceDB æ˜¯ä¸“ä¸º AI å’Œå‘é‡æ£€ç´¢è®¾è®¡çš„ç‰¹ç§å…µã€‚

å®ƒä»¬çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºåº•å±‚å­˜å‚¨æ ¼å¼ï¼šDuckDB åå¥½ Parquet ï¼ˆæˆ–è‡ªå®¶æ ¼å¼ï¼‰ï¼Œè€Œ LanceDB åŸºäº Lance æ ¼å¼ã€‚

ä»¥ä¸‹æ˜¯è¯¦ç»†çš„ä¼˜ç¼ºç‚¹å¯¹æ¯”ï¼š




1. æ ¸å¿ƒå®šä½ (Core Philosophy)
- DuckDB : é€šç”¨ OLAP æ•°æ®åº“ ã€‚
  - ç›®æ ‡æ˜¯æ›¿ä»£ SQLite åšåˆ†æï¼Œæˆ–è€…ä½œä¸ºæœ¬åœ°çš„æ•°æ®æ¹–æŸ¥è¯¢å¼•æ“ã€‚
  - æ“…é•¿ï¼šèšåˆï¼ˆGroup Byï¼‰ã€å¤æ‚ SQL Joinã€å¤„ç† CSV/Parquet/JSON æ–‡ä»¶ã€‚
- LanceDB : AI å‘é‡æ•°æ®åº“ ã€‚
  - ç›®æ ‡æ˜¯ç®¡ç†å¤šæ¨¡æ€æ•°æ®ï¼ˆEmbedding + åŸå§‹æ•°æ® + å›¾ç‰‡/è§†é¢‘è·¯å¾„ï¼‰ã€‚
  - æ“…é•¿ï¼šå‘é‡æœç´¢ï¼ˆANNï¼‰ã€éšæœºæ•°æ®è¯»å–ã€æ•°æ®ç‰ˆæœ¬æ§åˆ¶ã€‚



2. å‘é‡æ£€ç´¢èƒ½åŠ› (Vector Search)
è¿™æ˜¯ä¸¤è€…å·®è·æœ€å¤§çš„åœ°æ–¹ã€‚

- DuckDB (ç¼ºç‚¹) :
  - æš´åŠ›è®¡ç®— ï¼šå¦‚å‰æ‰€è¿°ï¼ŒDuckDB ä¸»è¦é  SIMD æš´åŠ›æ‰«æã€‚
  - å†…å­˜é™åˆ¶ ï¼šéšç€æ•°æ®é‡å¢å¤§ï¼ˆæ¯”å¦‚è¶…è¿‡ 100 ä¸‡å‘é‡ï¼‰ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾ã€‚
  - ç´¢å¼•ç¼ºå¤± ï¼šè™½ç„¶æœ‰å®éªŒæ€§æ‰©å±•ï¼Œä½†åŸç”Ÿå¹¶ä¸æŒä¹…åŒ–ä¿å­˜ HNSW ç´¢å¼•ï¼Œæ¯æ¬¡é‡å¯å¯èƒ½éœ€è¦é‡ç®—æˆ–é‡æ–°åŠ è½½ã€‚
- LanceDB (ä¼˜ç‚¹) :
  - DiskANN ç´¢å¼• ï¼šè¿™æ˜¯ LanceDB çš„æ€æ‰‹é”ã€‚å®ƒå®ç°äº†åŸºäºç£ç›˜çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€‚
  - æä½å†…å­˜å ç”¨ ï¼šä½ å¯ä»¥ç”¨ 16GB å†…å­˜çš„ç¬”è®°æœ¬ï¼Œæ£€ç´¢ 10äº¿+ è§„æ¨¡çš„å‘é‡æ•°æ®ã€‚å®ƒä¸éœ€è¦åƒ Milvus æˆ– Pinecone é‚£æ ·æŠŠæ‰€æœ‰å‘é‡åŠ è½½åˆ°å†…å­˜é‡Œã€‚
  - é«˜æ€§èƒ½ ï¼šåœ¨æµ·é‡æ•°æ®ä¸‹ï¼Œæ£€ç´¢é€Ÿåº¦è¿œè¶… DuckDB çš„æš´åŠ›æ‰«æã€‚



3. æ•°æ®å­˜å‚¨ä¸è¯»å– (Storage & I/O)
è¿™é‡Œæ¶‰åŠåˆ° Parquet (DuckDB å¸¸ç”¨) å’Œ Lance (LanceDB æ ¸å¿ƒ) æ ¼å¼çš„åŒºåˆ«ã€‚

- DuckDB (Parquet æ¨¡å¼) :
  - ä¼˜ç‚¹ ï¼šParquet æ˜¯é€šç”¨çš„ï¼Œè°éƒ½èƒ½è¯»ã€‚
  - ç¼ºç‚¹ ï¼šParquet ä¸æ”¯æŒé«˜æ•ˆçš„ éšæœºè¯»å– ï¼ˆRandom Accessï¼‰ã€‚å¦‚æœä½ æœåˆ°äº† Top-10 çš„å‘é‡ï¼Œæƒ³æŠŠè¿™ 10 æ¡å¯¹åº”çš„åŸå§‹æ–‡æœ¬è¯»å‡ºæ¥ï¼ŒParquet éœ€è¦è§£å‹æ•´ä¸ª Data Pageï¼Œéå¸¸æ…¢ã€‚
- LanceDB (Lance æ¨¡å¼) :
  - ä¼˜ç‚¹ ï¼šLance æ ¼å¼ä¸“ä¸ºç°ä»£ AI è®¾è®¡ã€‚å®ƒæ—¢æ”¯æŒåˆ—å¼æ‰«æï¼ˆåƒ Parquetï¼‰ï¼Œä¹Ÿæ”¯æŒ äºšæ¯«ç§’çº§çš„éšæœºè¯»å– ã€‚
  - å®æˆ˜æ„ä¹‰ ï¼šåœ¨ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨ä¸­ï¼Œæ£€ç´¢åˆ°å‘é‡åï¼Œå¿…é¡»ç«‹åˆ»å–å‡ºå¯¹åº”çš„ Text Chunk å–‚ç»™ LLMã€‚LanceDB åšè¿™ä¸€æ­¥æ¯” "DuckDB + Parquet" å¿« 100å€ ä»¥ä¸Šã€‚



4. å…¨æ–‡æ£€ç´¢ä¸æ··åˆæœç´¢ (Hybrid Search)
- DuckDB :
  - FTS è¾ƒåŸºç¡€ï¼ˆåŸºäº SQL å®ï¼‰ï¼Œé€‚åˆç®€å•å…³é”®è¯åŒ¹é…ã€‚
- LanceDB :
  - å†…ç½®äº†åŸºäº Tantivy ï¼ˆRust å†™çš„æœç´¢å¼•æ“åº“ï¼Œç±»ä¼¼ Luceneï¼‰çš„å€’æ’ç´¢å¼•ã€‚
  - ä¼˜ç‚¹ ï¼šå®ƒåŸç”Ÿæ”¯æŒ æ··åˆæœç´¢ï¼ˆHybrid Searchï¼‰ ï¼Œå³â€œå…³é”®è¯åŒ¹é… + å‘é‡è¯­ä¹‰åŒ¹é…â€åŒæ—¶è¿›è¡Œï¼Œå¹¶è‡ªåŠ¨è¿›è¡Œ RRFï¼ˆReciprocal Rank Fusionï¼‰æ’åèåˆã€‚è¿™åœ¨ RAG ä¸­æ˜¯æå‡æ•ˆæœçš„å…³é”®æŠ€æœ¯ã€‚



5. ç”Ÿæ€ä¸æˆç†Ÿåº¦
- DuckDB (èµ¢) :
  - SQL æ”¯æŒæå…¶å®Œå–„ï¼ˆPostgreSQL å…¼å®¹ï¼‰ã€‚
  - ç¤¾åŒºå·¨å¤§ï¼Œæ–‡æ¡£ä¸°å¯Œï¼Œå‡ ä¹æ‰€æœ‰æ•°æ®å·¥å…·éƒ½æ”¯æŒã€‚
- LanceDB (è¾“) :
  - è™½ç„¶æ”¯æŒ SQLï¼ˆé€šè¿‡ DataFusion æˆ–é›†æˆ DuckDBï¼‰ï¼Œä½†ä¸»è¦è¿˜æ˜¯é€šè¿‡ Python/JS API æ“ä½œã€‚
  - ç›¸å¯¹è¾ƒæ–°ï¼Œç”Ÿæ€è¿˜åœ¨å¿«é€Ÿå‘å±•ä¸­ã€‚
  æ€»ç»“ï¼šè¯¥æ€ä¹ˆé€‰ï¼Ÿ
  åœºæ™¯ æ¨è åŸå›  é€šç”¨æ•°æ®åˆ†æ DuckDB éœ€è¦å¤æ‚çš„ Group By, Join, Window Functionsã€‚ å°è§„æ¨¡å‘é‡ (<100k) DuckDB æ²¡å¿…è¦å¼•å…¥æ–°ç»„ä»¶ï¼ŒDuckDB çš„æš´åŠ›è®¡ç®—è¶³å¤Ÿå¿«ä¸”ç²¾ç¡®ã€‚ å¤§è§„æ¨¡å‘é‡ (>1M) LanceDB å¿…é¡»ç”¨ç´¢å¼•ï¼Œå¦åˆ™å¤ªæ…¢ã€‚ä¸” LanceDB çœå†…å­˜ã€‚ RAG / AI åº”ç”¨ LanceDB éœ€è¦æ··åˆæœç´¢ï¼ˆHybrid Searchï¼‰å’Œå¿«é€Ÿéšæœºè¯»å–æ–‡æ¡£å†…å®¹ã€‚ å¤šæ¨¡æ€æ•°æ® LanceDB éœ€è¦ç®¡ç†å›¾ç‰‡ã€è§†é¢‘ã€ç‚¹äº‘ç­‰éç»“æ„åŒ–æ•°æ®ã€‚

ç»ˆæå»ºè®®ï¼š å…¶å®å®ƒä»¬å¯ä»¥ å…±å­˜ ã€‚
LanceDB æœ‰ä¸€ä¸ªéå¸¸é…·çš„ç‰¹æ€§ï¼š å®ƒå…è®¸ DuckDB ç›´æ¥æŸ¥è¯¢ Lance æ•°æ®é›† ã€‚
ä½ å¯ä»¥ç”¨ LanceDB æ¥å¤„ç†å‘é‡ç´¢å¼•å’Œå¿«é€Ÿæ£€ç´¢ï¼Œç„¶åç”¨ DuckDB é€šè¿‡ Apache Arrow é›¶æ‹·è´æŠ€æœ¯æ¥å¯¹è¿™äº›ç»“æœè¿›è¡Œå¤æ‚çš„ SQL åˆ†æã€‚

```
import lancedb
import duckdb

1. ç”¨ LanceDB åšå‘é‡æœç´¢
db = lancedb.connect("data.lance")
table = db.open_table("vectors")
results = table.search(query_vec).limit(100).to_arrow()

2. ç”¨ DuckDB å¯¹æœç´¢ç»“æœåšåˆ†æ
duckdb.sql("SELECT category, count(*) FROM results GROUP BY category").show()
```



## Install LanceDB



```

pip install lancedb  # or uv add lancedb

import lancedb

uri = "ex_lancedb"
db = lancedb.connect(uri)

data = [
    {"id": "1", "text": "knight", "vector": [0.9, 0.4, 0.8]},
    {"id": "2", "text": "ranger", "vector": [0.8, 0.4, 0.7]},
    {"id": "9", "text": "priest", "vector": [0.6, 0.2, 0.6]},
    {"id": "4", "text": "rogue", "vector": [0.7, 0.4, 0.7]},
]
table = db.create_table("adventurers", data=data, mode="overwrite")


query_vector = [0.8, 0.3, 0.8]

# Ensure you run `pip install polars` beforehand
result = table.search(query_vector).limit(2).to_polars()
print(result)



```





##  json å¢åˆ æ”¹æŸ¥å¥½ç”¨å—



````


        
            
pip install lancedb pandas 

          
LanceDB å¯¹ JSONï¼ˆåµŒå¥—ç»“æ„ï¼‰çš„æ”¯æŒæ€»ä½“æ¥è¯´æ˜¯**â€œæŸ¥æ”¹æ–¹ä¾¿ï¼Œå¢åˆ çµæ´»ï¼Œä½†å±€éƒ¨æ›´æ–°æœ‰å‘â€**ã€‚å®ƒå†…éƒ¨å°† JSON å­˜å‚¨ä¸º Arrow çš„ `Struct` ç±»å‹ï¼Œå› æ­¤èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œåˆ—å¼å­˜å‚¨å’ŒæŸ¥è¯¢ï¼Œä½†åœ¨å¤„ç†â€œéƒ¨åˆ†å­—æ®µæ›´æ–°â€æ—¶ä¸å¦‚ MongoDB é‚£æ ·éšæ„ã€‚

ä»¥ä¸‹æ˜¯é’ˆå¯¹ JSON æ•°æ®çš„å¢åˆ æ”¹æŸ¥ï¼ˆCRUDï¼‰è¯¦ç»†æµ‹è¯„ï¼š


ç»“è®ºé€Ÿè§ˆ
| æ“ä½œ | è¯„ä»· | è¯´æ˜ |
| :--- | :--- | :--- |
| **å¢ (Create)** | â­â­â­â­â­ | ç›´æ¥æ”¯æŒåˆ—è¡¨åµŒå¥—å­—å…¸ã€Pydantic æ¨¡å‹ï¼Œè‡ªåŠ¨æ¨æ–­ Schemaã€‚ |
| **æŸ¥ (Read)** | â­â­â­â­â­ | æ”¯æŒç‚¹å·è¯­æ³•ï¼ˆå¦‚ `meta.score > 10`ï¼‰è¿›è¡Œè¿‡æ»¤ï¼Œé€Ÿåº¦æå¿«ã€‚ |
| **åˆ  (Delete)** | â­â­â­â­â­ | æ”¯æŒæ ¹æ®åµŒå¥—å­—æ®µæ¡ä»¶åˆ é™¤ï¼ˆå¦‚ `delete("meta.score < 5")`ï¼‰ã€‚ |
| **æ”¹ (Update)** | â­â­â­ | **æœ‰å±€é™**ã€‚ä¸èƒ½åªæ›´æ–° JSON ä¸­çš„æŸä¸ªå­—æ®µï¼ˆå¦‚åªæ”¹ `meta.score`ï¼‰ï¼Œå¿…é¡»**æ›¿æ¢æ•´ä¸ª JSON å¯¹è±¡**ã€‚ |

---


è¯¦ç»†ä»£ç ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹åµŒå¥— JSON æ•°æ®ç»“æ„ï¼š
```json
{
  "id": 1,
  "vector": [0.1, 0.2],
  "meta": {
    "category": "A",
    "score": 10,
    "tags": ["x", "y"]
  }
}
```


1. å¢ (Create) - éå¸¸ä¸æ»‘
ä½ å¯ä»¥ç›´æ¥ä¼ å…¥ Python çš„å­—å…¸åˆ—è¡¨ï¼ŒLanceDB ä¼šè‡ªåŠ¨æ¨æ–­åµŒå¥—ç»“æ„ã€‚

```python
import lancedb

db = lancedb.connect("./data")
data = [
    {"id": 1, "vector": [0.1, 0.2], "meta": {"category": "A", "score": 10}},
    {"id": 2, "vector": [0.3, 0.4], "meta": {"category": "B", "score": 20}},
]
table = db.create_table("items", data)
```


2. æŸ¥ (Read) - æ”¯æŒç‚¹å·è¯­æ³•
ä½ å¯ä»¥åƒæ“ä½œå¯¹è±¡å±æ€§ä¸€æ ·è¿‡æ»¤åµŒå¥—å­—æ®µï¼Œéå¸¸ç›´è§‚ã€‚

```python
# æŸ¥è¯¢ meta.category ä¸º 'A' çš„è®°å½•
result = table.search().where("meta.category = 'A'").to_pandas()
print(result)
```


3. åˆ  (Delete) - åŒæ ·æ”¯æŒåµŒå¥—æ¡ä»¶
```python

åˆ é™¤ meta.score å°äº 15 çš„è®°å½•
table.delete("meta.score < 15")
```


4. æ”¹ (Update) - éœ€è¦æ³¨æ„ï¼
è¿™æ˜¯æœ€å®¹æ˜“è¸©å‘çš„åœ°æ–¹ã€‚LanceDB **ä¸æ”¯æŒ**åƒ MongoDB é‚£æ · `$set: {"meta.score": 99}` åªæ›´æ–°å±€éƒ¨å­—æ®µã€‚ä½ å¿…é¡»æä¾›**å®Œæ•´çš„** `meta` å¯¹è±¡ã€‚

**æ¨èä½¿ç”¨ `merge_insert` (Upsert) è¿›è¡Œæ›´æ–°ï¼š**

```python

âŒ é”™è¯¯åšæ³•ï¼šè¯•å›¾åªä¼ éƒ¨åˆ†å­—æ®µï¼ˆä¼šå¯¼è‡´æŠ¥é”™æˆ–æ•°æ®ä¸¢å¤±ï¼‰

new_data = [{"id": 2, "meta": {"score": 99}}] 


âœ… æ­£ç¡®åšæ³•ï¼šæä¾›å®Œæ•´çš„ meta å¯¹è±¡
new_data = [
    {
        "id": 2, 
        "meta": {"category": "B", "score": 99} # å¿…é¡»åŒ…å« categoryï¼Œå¦åˆ™éœ€é‡æ–°è¯»å–
    }
]


æ‰§è¡Œæ›´æ–°ï¼ˆå¦‚æœ id å­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰
table.merge_insert("id") \
    .when_matched_update_all() \
    .when_not_matched_insert_all() \
    .execute(new_data)
```


æ€»ç»“å»ºè®®
- **é€‚åˆåœºæ™¯**ï¼šä½ çš„ JSON ç»“æ„ç›¸å¯¹å›ºå®šï¼ˆSchema ä¸ç»å¸¸å˜ï¼‰ï¼Œä¸»è¦ç”¨äºæ ¹æ®æŸäº›å­—æ®µè¿‡æ»¤å‘é‡æ£€ç´¢ç»“æœã€‚
- **ä¸é€‚åˆåœºæ™¯**ï¼šSchema æå…¶åŠ¨æ€ï¼ˆæ¯ä¸€è¡Œçš„ JSON é”®éƒ½ä¸ä¸€æ ·ï¼‰ï¼Œæˆ–è€…éœ€è¦é¢‘ç¹é«˜å¹¶å‘åœ°åªä¿®æ”¹ JSON ä¸­æŸä¸€ä¸ªæ·±å±‚å­—æ®µã€‚

å¦‚æœä½ çš„åº”ç”¨åœºæ™¯éœ€è¦é¢‘ç¹ä¿®æ”¹ JSON å†…éƒ¨çš„æ·±å±‚å­—æ®µï¼Œå»ºè®®åœ¨åº”ç”¨å±‚å…ˆè¯»å–å®Œæ•´å¯¹è±¡ï¼Œä¿®æ”¹åå†å†™å›ã€‚

````



```

import lancedb
import pandas as pd
import pyarrow as pa
import shutil
import os

# Clean up previous run
if os.path.exists("test_lancedb"):
    shutil.rmtree("test_lancedb")

# Connect
db = lancedb.connect("test_lancedb")

# 1. Create with Nested Data
print("--- 1. Create ---")
data = [
    {"id": 1, "vector": [1.0, 2.0], "meta": {"category": "A", "score": 10, "tags": ["x", "y"]}},
    {"id": 2, "vector": [3.0, 4.0], "meta": {"category": "B", "score": 20, "tags": ["y", "z"]}},
    {"id": 3, "vector": [5.0, 6.0], "meta": {"category": "A", "score": 5, "tags": ["x"]}},
]
table = db.create_table("items", data)
print(table.schema)
print(table.to_pandas())

# 2. Query with Nested Filter
print("\n--- 2. Query Nested ---")
try:
    # Try dot notation for nested field
    result = table.search().where("meta.category = 'A'").to_pandas()
    print("Filter meta.category = 'A':")
    print(result)
except Exception as e:
    print(f"Query failed: {e}")

# 3. Update Nested (Attempt)
print("\n--- 3. Update Nested ---")
try:
    # Attempt to update a specific nested field (likely to fail or require full struct replacement)
    # Let's try to update id=1, meta.score = 99
    # Standard SQL update usually requires replacing the whole column or using struct construction
    
    # Method A: Replace whole struct
    new_meta = {"category": "A", "score": 99, "tags": ["x", "y", "new"]}
    table.update(where="id = 1", values={"meta": new_meta})
    print("Updated id=1 with full struct replacement:")
    print(table.search().where("id = 1").to_pandas())
    
except Exception as e:
    print(f"Update failed: {e}")

# 4. Delete with Nested Filter
print("\n--- 4. Delete Nested ---")
try:
    table.delete("meta.score < 10")
    print("Deleted where meta.score < 10. Remaining:")
    print(table.to_pandas())
except Exception as e:
    print(f"Delete failed: {e}")


```



```

import lancedb
import pandas as pd
import shutil
import os

# Clean up previous run
if os.path.exists("test_lancedb_update_struct"):
    shutil.rmtree("test_lancedb_update_struct")

db = lancedb.connect("test_lancedb_update_struct")

# Create
data = [
    {"id": 1, "vector": [1.0, 2.0], "meta": {"category": "A", "score": 10}},
]
table = db.create_table("items", data)

print("--- Initial State ---")
print(table.to_pandas())

# Update with partial struct
print("\n--- Update with Partial Struct ---")
new_data = [
    {"id": 1, "meta": {"score": 99}} # Missing 'category' inside 'meta'
]
try:
    table.merge_insert("id").when_matched_update_all().execute(new_data)
    print("Merge partial struct successful. Result:")
    print(table.to_pandas().to_dict(orient="records"))
except Exception as e:
    print(f"Merge partial struct failed: {e}")


```



## C++

https://github.com/lancedb/lancedb-c





## GPU åŠ é€Ÿç´¢å¼•

```
pip install lancedb numpy pyarrow torch
```



````


ä»¥ä¸‹æ˜¯å…³äº LanceDB GPU åŠ é€Ÿçš„è¯¦ç»†è¯´æ˜åŠå¼€å¯æ–¹æ³•ï¼š


1. GPU åŠ é€Ÿæ”¯æŒæƒ…å†µ
- æ”¯æŒé˜¶æ®µ ï¼š ç´¢å¼•æ„å»º (Index Creation) ã€‚
- ä½œç”¨ ï¼šåœ¨åƒä¸‡çº§æˆ–äº¿çº§å‘é‡æ•°æ®ä¸Šæ„å»º IVF_PQ ç´¢å¼•æ—¶ï¼ŒGPU å¯ä»¥å°†è®­ç»ƒé€Ÿåº¦æå‡æ•°å€è‡³æ•°åå€ã€‚
- æŸ¥è¯¢é˜¶æ®µ ï¼šæŸ¥è¯¢è¿‡ç¨‹ä¸»è¦è¿è¡Œåœ¨ CPU ä¸Šï¼Œåˆ©ç”¨ Lance æ ¼å¼çš„ç£ç›˜ I/O ä¼˜åŒ–å’Œ SIMD æŒ‡ä»¤å®ç°é«˜æ€§èƒ½æ£€ç´¢ã€‚

2. ç¯å¢ƒè¦æ±‚
è¦å¼€å¯ GPU åŠ é€Ÿï¼Œä½ éœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

- Python ç¯å¢ƒ ï¼šå®‰è£…äº† lancedb ã€‚
- PyTorch ï¼šå¿…é¡»å®‰è£… torch >= 2.0 ã€‚
- ç¡¬ä»¶ ï¼š
  - NVIDIA GPU (CUDA)
  - Apple Silicon (MPS, Metal Performance Shaders)

3. å¦‚ä½•å¼€å¯ GPU åŠ é€Ÿ (Python ç¤ºä¾‹)
åœ¨åˆ›å»ºç´¢å¼• create_index æ—¶ï¼Œé€šè¿‡ accelerator å‚æ•°æŒ‡å®šè®¾å¤‡å³å¯å¼€å¯ã€‚

```
importÂ lancedb
importÂ numpyÂ asÂ np


1.Â è¿æ¥æ•°æ®åº“
dbÂ =Â lancedb.connect("./data/sample-lancedb")
dataÂ =Â [{"vector":Â np.random.randn(128),Â "id":Â i}Â forÂ iÂ inÂ range(10000)]
tblÂ =Â db.create_table("my_vectors",Â data)


2.Â å¼€å¯Â GPUÂ åŠ é€Ÿæ„å»ºç´¢å¼•

acceleratorÂ å‚æ•°å¯é€‰:Â "cuda"Â (NVIDIA)Â æˆ–Â "mps"Â (AppleÂ Silicon)
tbl.create_index(
Â Â Â Â metric="cosine",
Â Â Â Â num_partitions=256,
Â Â Â Â num_sub_vectors=96,
Â Â Â Â accelerator="cuda"Â Â #Â <---Â è¿™é‡Œå¼€å¯Â GPUÂ åŠ é€Ÿ
)


3.Â æ‰§è¡Œå‘é‡æŸ¥è¯¢Â (æŸ¥è¯¢è¿‡ç¨‹è‡ªåŠ¨åˆ©ç”¨å·²æ„å»ºçš„ç´¢å¼•)
query_vectorÂ =Â np.random.randn(128)
resultsÂ =Â tbl.search(query_vector).limit(10).to_pandas()
print(results)
```

å…³é”®å‚æ•°è¯´æ˜
- accelerator="cuda" ï¼šä½¿ç”¨ NVIDIA GPUã€‚
- accelerator="mps" ï¼šä½¿ç”¨ Apple Silicon (M1/M2/M3) GPUã€‚
- å¦‚æœä¸æŒ‡å®š accelerator ï¼Œé»˜è®¤ä½¿ç”¨ CPUã€‚

æ³¨æ„äº‹é¡¹
- æ•°æ®é‡ ï¼šGPU åŠ é€Ÿåœ¨æ•°æ®é‡è¾ƒå¤§ï¼ˆå¦‚ 100 ä¸‡æ¡ä»¥ä¸Šï¼‰æ—¶æ•ˆæœæœ€æ˜æ˜¾ã€‚å°æ•°æ®é‡ä¸‹ï¼Œæ•°æ®ä¼ è¾“åˆ° GPU çš„å¼€é”€å¯èƒ½å¯¼è‡´é€Ÿåº¦ä¸å¦‚ CPUã€‚
- æ˜¾å­˜é™åˆ¶ ï¼šGPU æ˜¾å­˜éœ€è¦èƒ½å¤Ÿå®¹çº³ç´¢å¼•è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ•°æ®ã€‚å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ batch_size æˆ–å›é€€åˆ° CPUã€‚
- Enterprise ç‰ˆ ï¼šLanceDB Enterprise ç‰ˆæ”¯æŒè‡ªåŠ¨ GPU ç´¢å¼•ç®¡ç†ï¼Œè€Œå¼€æºç‰ˆï¼ˆOSSï¼‰ç›®å‰ä¸»è¦é€šè¿‡ Python SDK æ‰‹åŠ¨æŒ‡å®šã€‚
å¦‚æœæ‚¨æ˜¯åœ¨è¿›è¡Œå¤§è§„æ¨¡å‘é‡åº“çš„æ„å»ºï¼Œå¼ºçƒˆå»ºè®®å¼€å¯æ­¤åŠŸèƒ½ä»¥ç¼©çŸ­ç´¢å¼•æ—¶é—´ã€‚

````





````

          
LanceDB åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®å’Œå¢é‡ç´¢å¼•æ–¹é¢æœ‰éå¸¸æˆç†Ÿçš„è®¾è®¡ã€‚é’ˆå¯¹æ‚¨æåˆ°çš„â€œå†…å­˜å­˜ä¸ä¸‹å…¨éƒ¨æ•°æ®â€å’Œâ€œå¢é‡æ„å»ºâ€ä¸¤ä¸ªé—®é¢˜ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆï¼š


1. å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿï¼ˆåŸºäºç£ç›˜çš„ç´¢å¼•æ„å»ºï¼‰

LanceDB çš„æ ¸å¿ƒä¼˜åŠ¿ä¹‹ä¸€å°±æ˜¯**åŸºäºç£ç›˜ï¼ˆDisk-basedï¼‰çš„ç´¢å¼•æœºåˆ¶**ã€‚ä½ **ä¸éœ€è¦**æŠŠæ‰€æœ‰æ•°æ®éƒ½åŠ è½½åˆ°å†…å­˜ä¸­ã€‚

*   **è‡ªåŠ¨åˆ†æ‰¹å¤„ç†**ï¼šå½“ä½ è°ƒç”¨ `create_index` æ—¶ï¼ŒLanceDB ä¸ä¼šæŠŠæ‰€æœ‰å‘é‡è¯»å…¥ RAMã€‚å®ƒä¼šè‡ªåŠ¨åˆ©ç”¨ Lance æ–‡ä»¶æ ¼å¼çš„ç‰¹æ€§ï¼Œè¿›è¡Œå¤–å­˜ï¼ˆOut-of-Coreï¼‰è®¡ç®—ã€‚
*   **æ— éœ€ç‰¹æ®Šé…ç½®**ï¼šä½ åªéœ€è¦æ­£å¸¸é€šè¿‡ `db.create_table()` æŠŠæ•°æ®å†™å…¥ç£ç›˜ï¼ˆæ•°æ®ä¼šä»¥ Lance æ ¼å¼å­˜å‚¨åœ¨ç¡¬ç›˜ä¸Šï¼‰ï¼Œç„¶åè°ƒç”¨ `create_index()`ã€‚LanceDB ä¼šè‡ªåŠ¨åˆ†æ‰¹è¯»å–æ•°æ®ã€è®­ç»ƒèšç±»ä¸­å¿ƒï¼ˆIVF centroidsï¼‰ï¼Œå¹¶ç”Ÿæˆç´¢å¼•æ–‡ä»¶ã€‚
*   **æ˜¾å­˜ä¼˜åŒ–**ï¼šå¦‚æœå¼€å¯äº† GPU åŠ é€Ÿï¼ŒLanceDB ä¹Ÿä¼šè‡ªåŠ¨ç®¡ç†æ˜¾å­˜ï¼Œåˆ†æ‰¹å°†æ•°æ®é€å…¥ GPU è®¡ç®—ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚

**æ“ä½œå»ºè®®**ï¼š
1.  å…ˆå°†æ•°æ®åˆ†æ‰¹å†™å…¥ï¼ˆ`add`ï¼‰åˆ°è¡¨ä¸­ã€‚
2.  æ•°æ®å…¨éƒ¨å†™å…¥ç¡¬ç›˜åï¼Œå†ä¸€æ¬¡æ€§è°ƒç”¨ `create_index`ã€‚

```python
import lancedb
import pyarrow as pa


1. è¿æ¥æ•°æ®åº“
db = lancedb.connect("./data/large-dataset")


2. åˆ†æ‰¹å†™å…¥æ•°æ® (æ¨¡æ‹Ÿå†…å­˜æ”¾ä¸ä¸‹çš„æƒ…å†µ)

å‡è®¾æˆ‘ä»¬æœ‰ 100 ä¸ªæ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡ 10 ä¸‡æ¡æ•°æ®

ç¬¬ä¸€æ¬¡åˆ›å»ºè¡¨
batch1 = ... # ä½ çš„æ•°æ®ç”Ÿæˆé€»è¾‘
tbl = db.create_table("vectors", batch1) 


åç»­æ‰¹æ¬¡ä½¿ç”¨ add è¿½åŠ åˆ°ç£ç›˜
for i in range(99):
    batch = ... 
    tbl.add(batch)


æ­¤æ—¶æ•°æ®éƒ½åœ¨ç£ç›˜ä¸Šï¼Œå†…å­˜ä¸­å ç”¨å¾ˆå°


3. æ„å»ºç´¢å¼• (è‡ªåŠ¨åˆ©ç”¨ç£ç›˜ï¼Œä¸ä¼šæ’‘çˆ†å†…å­˜)

è¿™ä¸€æ­¥ä¼šæ‰«æç£ç›˜ä¸Šçš„æ•°æ®æ¥æ„å»ºç´¢å¼•
tbl.create_index(
    metric="cosine",
    num_partitions=2048, # é’ˆå¯¹å¤§æ•°æ®é‡é€‚å½“è°ƒå¤§åˆ†åŒºæ•°
    num_sub_vectors=96,
    accelerator="cuda"   # æ¨èå¼€å¯ GPU åŠ é€Ÿ
)
```

---


2. å¢é‡æ„å»ºç´¢å¼•ï¼ˆIncremental Indexingï¼‰

LanceDB æ”¯æŒå¯¹æ–°åŠ å…¥çš„æ•°æ®è¿›è¡Œå¢é‡ç´¢å¼•ï¼Œä½†æœºåˆ¶ä¸ä¼ ç»Ÿæ•°æ®åº“ç•¥æœ‰ä¸åŒã€‚

*   **å†™å…¥å³æŸ¥è¯¢ï¼ˆWrite-Availableï¼‰**ï¼šå½“ä½ å‘å·²æœ‰ç´¢å¼•çš„è¡¨ä¸­ `add` æ–°æ•°æ®æ—¶ï¼Œæ–°æ•°æ®**ä¸ä¼šç«‹å³**åˆå¹¶åˆ°ä¸»ç´¢å¼•ä¸­ã€‚
    *   æ­¤æ—¶æŸ¥è¯¢ = `ä¸»ç´¢å¼•æŸ¥è¯¢ï¼ˆæ—§æ•°æ®ï¼‰` + `æš´åŠ›æœç´¢ï¼ˆæ–°æ•°æ®ï¼‰`ã€‚
    *   è¿™ä¿è¯äº†ä½ ç«‹é©¬èƒ½æŸ¥åˆ°æ–°æ•°æ®ï¼Œä½†å¦‚æœæ–°æ•°æ®ç§¯ç´¯å¤ªå¤šï¼ŒæŸ¥è¯¢é€Ÿåº¦ä¼šå˜æ…¢ã€‚

*   **æ‰‹åŠ¨è§¦å‘åˆå¹¶ï¼ˆReindex / Optimizeï¼‰**ï¼š
    ä¸ºäº†ä¿æŒé«˜æ€§èƒ½ï¼Œä½ éœ€è¦å®šæœŸå°†æ–°æ•°æ®åˆå¹¶åˆ°ç´¢å¼•ä¸­ã€‚åœ¨ LanceDB OSSï¼ˆå¼€æºç‰ˆï¼‰ä¸­ï¼Œç›®å‰ä¸»è¦é€šè¿‡**é‡æ–°è®­ç»ƒæˆ–ä¼˜åŒ–**æ¥å®ç°ã€‚

    LanceDB æä¾›äº† `optimize()` æ¥å£ï¼Œä½†è¿™é€šå¸¸ç”¨äºæ•´ç†æ–‡ä»¶ç¢ç‰‡ã€‚å¯¹äºç´¢å¼•æ›´æ–°ï¼Œæœ€ç¨³å¦¥çš„æ–¹å¼æ˜¯**é‡æ–°è¿è¡Œ `create_index`** æˆ–è€…ä¾èµ–å…¶è‡ªåŠ¨çš„å¢é‡åˆå¹¶é€»è¾‘ï¼ˆè§†ç‰ˆæœ¬è€Œå®šï¼Œæ–°ç‰ˆæœ¬æ­£åœ¨ä¼˜åŒ–æ­¤æµç¨‹ï¼‰ã€‚

    *æ³¨æ„ï¼š* ç›®å‰ LanceDB OSS çš„ `create_index` åœ¨ v2 æ ¼å¼ä¸‹ä¼šå°è¯•å¢é‡ä¼˜åŒ–ï¼Œä½†åœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼Œä¸ºäº†ä¿è¯ç´¢å¼•è´¨é‡ï¼ˆç‰¹åˆ«æ˜¯ IVF çš„èšç±»ä¸­å¿ƒåˆ†å¸ƒï¼‰ï¼Œé‡æ–°æ„å»ºç´¢å¼•å¾€å¾€æ˜¯ä¿è¯å¬å›ç‡çš„æœ€ä½³æ‰‹æ®µã€‚ç”±äºå®ƒæ˜¯åŸºäºç£ç›˜çš„ä¸”æ”¯æŒ GPU åŠ é€Ÿï¼Œé‡æ„é€Ÿåº¦é€šå¸¸å¾ˆå¿«ã€‚

**æœ€ä½³å®è·µæµç¨‹**ï¼š
1.  **åˆå§‹æ„å»º**ï¼šå¯¼å…¥å†å²å­˜é‡æ•°æ®ï¼Œæ„å»ºç´¢å¼•ã€‚
2.  **æ—¥å¸¸å†™å…¥**ï¼šæŒç»­ `tbl.add()` æ–°æ•°æ®ã€‚æ­¤æ—¶æŸ¥è¯¢ä»ç„¶æœ‰æ•ˆï¼ˆæ··åˆæœç´¢ï¼‰ã€‚
3.  **å®šæœŸç»´æŠ¤**ï¼šå½“æ–°æ•°æ®ç§¯ç´¯åˆ°ä¸€å®šé‡ï¼ˆæ¯”å¦‚è¾¾åˆ°æ€»é‡çš„ 10-20%ï¼‰ï¼Œæˆ–è€…å‘ç°æŸ¥è¯¢å»¶è¿Ÿå˜é«˜æ—¶ï¼Œæ‰§è¡Œä¸€æ¬¡ç´¢å¼•æ›´æ–°ã€‚

```python

æ¯å¤©å‡Œæ™¨æˆ–ç´¯ç§¯ä¸€å®šæ•°æ®åæ‰§è¡Œ
tbl.create_index(
    metric="cosine",
    num_partitions=2048,
    num_sub_vectors=96,
    replace=True,      # æ›¿æ¢æ—§ç´¢å¼•
    accelerator="cuda"
)
```


æ€»ç»“
*   **å†…å­˜ä¸è¶³**ï¼šå®Œå…¨ä¸ç”¨æ‹…å¿ƒã€‚LanceDB å¤©ç”Ÿå°±æ˜¯ä¸ºâ€œæ•°æ®é‡ > å†…å­˜â€è®¾è®¡çš„ï¼Œç›´æ¥åˆ†æ‰¹å†™å…¥ç£ç›˜ï¼Œç„¶åå»ºç´¢å¼•å³å¯ã€‚
*   **å¢é‡ç´¢å¼•**ï¼šæ”¯æŒâ€œå†™å…¥å³æŸ¥â€ã€‚æ–°æ•°æ®ä¼šè‡ªåŠ¨å‚ä¸æœç´¢ï¼ˆæš´åŠ›æœï¼‰ï¼Œä½†ä¸ºäº†æ€§èƒ½ï¼Œå»ºè®®å®šæœŸé‡æ–°è¿è¡Œ `create_index` æˆ–ä½¿ç”¨ `optimize`ï¼ˆè§†å…·ä½“ç‰ˆæœ¬æ”¯æŒæƒ…å†µï¼‰æ¥åˆå¹¶ç´¢å¼•ã€‚

````



### optimize å¢é‡ç´¢å¼•


åœ¨ LanceDB ä¸­ï¼Œ`optimize()` æ˜¯ç»´æŠ¤è¡¨æ€§èƒ½çš„æ ¸å¿ƒæ“ä½œï¼Œå®ƒçš„ä½œç”¨ç±»ä¼¼äºæ•°æ®åº“çš„ `VACUUM`ï¼Œä¸»è¦è´Ÿè´£ **æ–‡ä»¶åˆå¹¶ï¼ˆCompactionï¼‰**ã€**æ¸…ç†æ—§ç‰ˆæœ¬ï¼ˆPruneï¼‰** ä»¥åŠ **å¢é‡æ›´æ–°ç´¢å¼•**ã€‚

ä½¿ç”¨ `optimize()` å¯ä»¥å°†æ–°å†™å…¥çš„æ•°æ®åˆå¹¶åˆ°ç°æœ‰çš„å‘é‡ç´¢å¼•ä¸­ï¼Œè€Œæ— éœ€å…¨é‡é‡å»ºç´¢å¼•ã€‚

1. åŸºæœ¬ç”¨æ³•

åœ¨å†™å…¥æ–°æ•°æ®åï¼Œç›´æ¥è°ƒç”¨ `table.optimize()` å³å¯è§¦å‘ç´¢å¼•çš„å¢é‡åˆå¹¶ã€‚

```python
import lancedb

# 1. è¿æ¥æ•°æ®åº“
db = lancedb.connect("./data/my-db")
tbl = db.open_table("my_vectors")

# 2. æ‰§è¡Œ Optimize
# è¿™ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
# - åˆå¹¶å°æ–‡ä»¶ (Compaction)
# - æ¸…ç†è¿‡æœŸçš„æ—§ç‰ˆæœ¬æ•°æ® (Prune)
# - å°†æ–°æ•°æ®æ·»åŠ åˆ°ç°æœ‰çš„å‘é‡ç´¢å¼•ä¸­ (Index Update)
tbl.optimize()
```

2. é«˜çº§å‚æ•°é…ç½®

`optimize` è¿˜æœ‰å‡ ä¸ªå…³é”®å‚æ•°å¯ä»¥æ§åˆ¶æ¸…ç†åŠ›åº¦ï¼š

```python
from datetime import timedelta

æ¸…ç† 1 å°æ—¶å‰çš„æ—§ç‰ˆæœ¬æ•°æ®ï¼ˆé»˜è®¤ä¸º 7 å¤©ï¼‰
æ³¨æ„ï¼šè®¾ç½®ä¸º 0 (timedelta(0)) ä¼šåˆ é™¤é™¤äº†æœ€æ–°ç‰ˆæœ¬å¤–çš„æ‰€æœ‰å†å²ç‰ˆæœ¬ï¼Œæ— æ³•è¿›è¡Œ Time Travel
tbl.optimize(cleanup_older_than=timedelta(hours=1))

å¦‚æœä¹‹å‰çš„å†™å…¥å¤±è´¥ç•™ä¸‹äº†æœªéªŒè¯çš„æ–‡ä»¶ç¢ç‰‡ï¼Œå¼ºåˆ¶æ¸…ç†å®ƒä»¬
tbl.optimize(delete_unverified=True)
```

3. ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ optimize vs é‡å»ºç´¢å¼•ï¼Ÿ

è™½ç„¶ `optimize()` æ”¯æŒå¢é‡æ›´æ–°ç´¢å¼•ï¼Œä½†å®ƒå’Œå…¨é‡é‡å»ºç´¢å¼•ï¼ˆ`create_index`ï¼‰çš„ä½¿ç”¨åœºæ™¯ä¸åŒï¼š

| æ“ä½œ         | `tbl.optimize()`                                             | `tbl.create_index(replace=True)`                             |
| :----------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **ä½œç”¨**     | **å¢é‡åˆå¹¶**ï¼šæŠŠæ–°æ•°æ®æŒ‚è½½åˆ°ç°æœ‰çš„ç´¢å¼•ç»“æ„ä¸Šã€‚               | **å…¨é‡é‡è®­**ï¼šæ ¹æ®å½“å‰æ‰€æœ‰æ•°æ®ï¼Œé‡æ–°è®­ç»ƒèšç±»ä¸­å¿ƒï¼ˆIVF centroidsï¼‰ã€‚ |
| **é€Ÿåº¦**     | å¿«ï¼ˆåªå¤„ç†å¢é‡æ•°æ®ï¼‰ã€‚                                       | æ…¢ï¼ˆå¤„ç†å…¨é‡æ•°æ®ï¼‰ã€‚                                         |
| **ç´¢å¼•è´¨é‡** | **ä¼šé€æ¸ä¸‹é™**ã€‚å› ä¸ºæ–°æ•°æ®æ˜¯è¢«å¼ºåˆ¶åˆ†é…åˆ°æ—§çš„èšç±»ä¸­å¿ƒä¸Šçš„ã€‚å¦‚æœæ–°æ•°æ®åˆ†å¸ƒå’Œæ—§æ•°æ®å·®å¼‚å¾ˆå¤§ï¼Œæ£€ç´¢ç²¾åº¦ï¼ˆRecallï¼‰ä¼šé™ä½ã€‚ | **æœ€ä½³**ã€‚èšç±»ä¸­å¿ƒæ˜¯æ ¹æ®æœ€æ–°å…¨é‡æ•°æ®é‡æ–°è®¡ç®—çš„ã€‚             |
| **é€‚ç”¨åœºæ™¯** | æ—¥å¸¸ç»´æŠ¤ï¼Œæ¯æ‰¹å†™å…¥åæ‰§è¡Œï¼Œä¿è¯æŸ¥è¯¢è¦†ç›–æ–°æ•°æ®ã€‚               | å®šæœŸç»´æŠ¤ï¼ˆå¦‚æ¯å‘¨/æ¯æœˆï¼‰ï¼Œæˆ–è€…å½“æ•°æ®åˆ†å¸ƒå‘ç”Ÿå‰§çƒˆå˜åŒ–æ—¶ã€‚      |

æœ€ä½³å®è·µå»ºè®®

1.  **æ—¥å¸¸å†™å…¥**ï¼šæ¯æ¬¡ `add` ä¸€æ‰¹æ•°æ®åï¼Œå¦‚æœå¸Œæœ›è¿™æ‰¹æ•°æ®èƒ½è¢«å¿«é€Ÿæ£€ç´¢ï¼ˆè€Œä¸æ˜¯èµ°æš´åŠ›æœç´¢ï¼‰ï¼Œå¯ä»¥è°ƒç”¨ä¸€æ¬¡ `optimize()`ã€‚
2.  **å®šæœŸé‡æ„**ï¼šæ¯å½“æ•°æ®é‡å¢é•¿æ˜¾è‘—ï¼ˆä¾‹å¦‚å¢åŠ äº† 20% ä»¥ä¸Šï¼‰æˆ–è€…å‘ç°å¬å›ç‡ä¸‹é™æ—¶ï¼Œæ‰§è¡Œä¸€æ¬¡å…¨é‡çš„ `create_index(replace=True)` æ¥é‡æ–°æ ¡å‡†ç´¢å¼•ã€‚



## GUI

```

import streamlit as st
import lancedb
import pandas as pd
import json
import os

st.set_page_config(page_title="LanceDB Manager", layout="wide")

st.title("ğŸ¹ LanceDB Manager")

# Sidebar: Connection
st.sidebar.header("Connection")
db_path = st.sidebar.text_input("Database Path", value="data")

if not os.path.exists(db_path):
    if st.sidebar.button("Create Database Folder"):
        os.makedirs(db_path, exist_ok=True)
        st.sidebar.success(f"Created {db_path}")

try:
    db = lancedb.connect(db_path)
    st.sidebar.success(f"Connected to {db_path}")
except Exception as e:
    st.sidebar.error(f"Failed to connect: {e}")
    st.stop()

# List Tables
try:
    tables = db.table_names()
except Exception as e:
    st.error(f"Error listing tables: {e}")
    tables = []

st.sidebar.header("Tables")
selected_table_name = st.sidebar.selectbox("Select Table", [""] + tables)

if st.sidebar.button("Refresh Tables"):
    st.rerun()

# Create Table
with st.sidebar.expander("Create New Table"):
    new_table_name = st.text_input("New Table Name")
    # Simple JSON input for initial data
    initial_data_str = st.text_area("Initial Data (JSON List)", value='[{"id": 1, "vector": [0.1, 0.2], "text": "hello"}]')
    if st.button("Create Table"):
        try:
            data = json.loads(initial_data_str)
            db.create_table(new_table_name, data=data)
            st.success(f"Table {new_table_name} created!")
            st.rerun()
        except Exception as e:
            st.error(f"Error creating table: {e}")

if selected_table_name:
    st.header(f"Table: {selected_table_name}")
    tbl = db.open_table(selected_table_name)
    
    # Tabs for operations
    tab_view, tab_query, tab_add, tab_delete, tab_schema = st.tabs(["View Data", "Search/Query", "Add Data", "Delete", "Schema"])
    
    with tab_view:
        st.subheader("Preview Data")
        limit = st.number_input("Limit rows", value=10, min_value=1)
        df = tbl.to_pandas()
        st.dataframe(df.head(limit), use_container_width=True)
        st.caption(f"Total rows: {len(df)}")

    with tab_query:
        st.subheader("Search & Filter")
        query_type = st.radio("Query Type", ["SQL Filter", "Vector Search"])
        
        if query_type == "SQL Filter":
            sql_filter = st.text_input("Where Clause (SQL)", placeholder="id > 1 AND meta.category = 'A'")
            if st.button("Run Filter"):
                try:
                    if sql_filter:
                        res = tbl.search().where(sql_filter).to_pandas()
                    else:
                        res = tbl.to_pandas()
                    st.dataframe(res, use_container_width=True)
                except Exception as e:
                    st.error(f"Query Error: {e}")
        
        elif query_type == "Vector Search":
            vector_str = st.text_input("Query Vector (comma separated)", placeholder="0.1, 0.2")
            k = st.number_input("Top K", value=5)
            if st.button("Run Vector Search"):
                try:
                    vec = [float(x.strip()) for x in vector_str.split(",")]
                    res = tbl.search(vec).limit(k).to_pandas()
                    st.dataframe(res, use_container_width=True)
                except Exception as e:
                    st.error(f"Vector Error: {e}")

    with tab_add:
        st.subheader("Add Records")
        add_mode = st.radio("Mode", ["JSON", "Form (Simple)"])
        
        if add_mode == "JSON":
            json_input = st.text_area("JSON Data (List of Dicts)", height=200, value='[{"id": 3, "vector": [0.5, 0.5], "text": "new"}]')
            if st.button("Insert JSON"):
                try:
                    data_to_add = json.loads(json_input)
                    tbl.add(data_to_add)
                    st.success(f"Added {len(data_to_add)} rows.")
                    st.rerun() # Refresh to show new data
                except Exception as e:
                    st.error(f"Insert Error: {e}")
        else:
            st.info("Form mode supports simple flat schemas. For nested data, use JSON mode.")
            # Dynamic form based on schema (simplified)
            # This is hard to do generically perfectly, but let's try for simple fields
            schema = tbl.schema
            form_data = {}
            with st.form("add_row_form"):
                for name in schema.names:
                    # Very basic type checking
                    field = schema.field(name)
                    st.write(f"**{name}** ({field.type})")
                    val = st.text_input(f"Value for {name}")
                    form_data[name] = val
                
                submitted = st.form_submit_button("Add Row")
                if submitted:
                    try:
                        # Need to parse values based on type, very rough
                        # For now, just try to parse JSON for vector/structs, else string/int
                        parsed_data = {}
                        for k, v in form_data.items():
                            try:
                                parsed_data[k] = json.loads(v)
                            except:
                                parsed_data[k] = v # Keep as string if not valid JSON
                                # Try to convert to int/float if possible
                                if v.isdigit(): parsed_data[k] = int(v)
                                else:
                                    try: parsed_data[k] = float(v)
                                    except: pass
                        
                        tbl.add([parsed_data])
                        st.success("Added row.")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error: {e}")

    with tab_delete:
        st.subheader("Delete Records")
        delete_filter = st.text_input("Delete Condition (SQL Where)", placeholder="id = 5")
        if st.button("Delete Rows", type="primary"):
            if not delete_filter:
                st.error("Please provide a condition.")
            else:
                try:
                    tbl.delete(delete_filter)
                    st.success(f"Deleted rows where {delete_filter}")
                    st.rerun()
                except Exception as e:
                    st.error(f"Delete Error: {e}")

    with tab_schema:
        st.text(str(tbl.schema))

else:
    st.info("Select a table from the sidebar to manage.")


```



